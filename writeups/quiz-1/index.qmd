---
title: "Quiz 1 Study Guide"
subtitle: "DSAN 5300: Statistical Learning"
sidebar: mainnav
weeknum: 2
assignment: "Quiz 1"
categories:
  - "Extra Writeups"
format:
  html:
    embed-resources: true
    echo: true
    code-fold: true
    html-math-method: mathjax
    df-print: kable
    toc: true
---

Hello DSAN 5300 Section 01 friends!

I apologize for the lack of info yall have going into the first Quiz for the course tomorrow. I should have sent out an announcement about it earlier, since we didn't have class this week, but I ended up getting bogged down trying to put together a full-on comprehensive study guide, which is still not ready, so instead I thought it'd be good to at least send out this more summary-level study guide.

The key topics that the Quiz will cover are:

## "Parametric" Modeling

What does it mean to have a **"parametric"** model? For example, in a model such as regression, which we write as

$$
Y = \beta_0 + \beta1_ X + \varepsilon,
$$

which of the things in that equation are parameters of the model and which are not parameters (for example, which are just variables that we plug data into)?

## Optimization in General

Once we've identified the parameters in a model, how do we evaluate how "good" or "bad" a certain setting for the parameters is? (The answer being, a loss function)

## Gradient Descent

Once we have a loss function, how does the gradient allow us to choose a random value for the parameter and then "make our way" towards the optimal value? The answer to this question is the main content in [this previous writeup](../lab-1/)

As a refresher, a **gradient** is just the vector equivalent of a derivative. For example, in calculus we learn how

$$
f(x) = x^2
$$

has a derivative

$$
\frac{\partial f}{\partial x} = 2x.
$$

So in this class, if $\mathbf{x}$ is now a vector like $\mathbf{x} = (x_1,x_2)$ instead of just a single number, the **gradient** or **vector-valued derivative** of $f$ with respect to $\mathbf{x}$, $\nabla_{\mathbf{x}} f$, is

$$
\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} \right)
$$

If it helps, try to notice/keep in mind how the $\mathbf{x}$ in $\frac{\partial f}{\partial \mathbf{x}}$ is a **vector**, whereas the $x_1$ in $\frac{\partial f}{\partial x_1}$ is a **scalar**. In other words, although the first and second terms in this expression may look scary, each entry within the parentheses on the RHS of that equality (the third term) is just the "regular" univariate derivative that you learn in calculus class!

## More Efficient Optimization Methods

Here the idea (or, the way I see these "fancier" methods, at least) is, they use additional information about the loss function above and beyond just $L(x)$ and its derivative $L'(x)$.

So, [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) for example uses the second derivative $L''(x)$ as an additional piece of information about the curvature of the loss function, whereas the [secant method](https://en.wikipedia.org/wiki/Secant_method) is slower than Newton's method but doesn't require us to know this second derivative $L''(x)$ (since it approximates it).

## A Full-On Lecture Replacement

To try and fully "fill in" the missing week here, I can just give you all the resource that is literally a recording of the class I learned this stuff from, and that I later TAed. That way if you have additional questions I'll be able to refer specifically to the examples/materials that Prof. Ng uses in the following video:

{{< video https://www.youtube.com/watch?v=4b4MUYve_U8 >}}

Sorry again for the lateness here, and, I will do my best to add more details to this guide tonight (Sunday night) and tomorrow (Monday morning), before the lecture time at 6:30pm!
