So, long story short, rather than memorizing a bunch of tables of derivatives/integrals, I just have the following principles in my mind (not through memorization, but through practice I guess), and I try to just use them to figure out derivative/integral problems as best as possible without memorization. If I forget these, I derive them from even-more-basic principles (like the limit definition of the integral). And if I hit a wall... I google it or WolframAlpha it.

---


We can visualize the two "pieces" of this optimization, to see (finally!) how the objective function and the constraints come together:

```{r}
#| label: constrained-opt
my_f <- function(x) 5*x^2 + 2*x - 3
f_min_x <- -1/5
f_min_df <- tibble(x=f_min_x, y=my_f(f_min_x))
c_min_x <- 0
c_min_df <- tibble(x=c_min_x, y=my_f(c_min_x))
x_df <- tibble(x=c(-0.5,0.5))
x_df |> ggplot(aes(x=x)) +
  geom_rect(xmin=0, xmax=1, ymin=-Inf, ymax=Inf, alpha=0.5, fill=cb_palette[1]) +
  geom_vline(xintercept=0, linewidth=0.75) +
  stat_function(fun=my_f) +
  geom_point(data=f_min_df, aes(y=y)) +
  geom_point(data=c_min_df, aes(y=y)) +
  theme_classic()
```

---


::: {.callout-note title="Example 7: Lagrange Multipliers with Non-Binding Constraints"}

Find the optimal value $x^*$ for the following optimization problem:

$$
\begin{alignat}{2}
x^* = &&\min_{x} \quad &f(x) = 5x^2 + 2x - 3 \\
&& \text{s.t.} \quad & x \geq -\frac{1}{3}
\end{alignat}
$$

:::

Notice how our **objective function** here is the same as it was in the previous problem, but the **constraint** has been changed so that it **no longer binds** the optimal value. Let's see what happens when we construct the Lagrangian for this problem and then optimize as usual.

For the constraint function $g(x)$, we can use $g(x) = x + \frac{1}{3}$, since this satisfies the three conditions mentioned above for a constraint function. With this, our Lagrangian becomes

$$
\begin{align*}
\mathscr{L}(x, \lambda) &= 5x^2 + 2x - 3 + \lambda\left(x + \frac{1}{3}\right) \\
&= 5x^2 + 2x - 3 + \lambda x + \frac{\lambda}{3}
\end{align*}
$$

and we proceed as usual:

$$
\begin{align*}
\frac{\partial}{\partial x}\mathscr{L}(x, \lambda) &= 10x + 2 + \lambda \\
\frac{\partial}{\partial \lambda}\mathscr{L}(x, \lambda) &= x + \frac{1}{3}
\end{align*}
$$
