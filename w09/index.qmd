---
title: "Week 9: Generative vs. Discriminative Models"
subtitle: "*DSAN 5300: Statistical Learning*<br><span class='subsubtitle'>Spring 2025, Georgetown University</span>"
author: "Jeff Jacobs"
institute: "[`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu)"
bibliography: "../_DSAN5300.bib"
date: 2025-03-17
date-format: full
lecnum: 9
categories:
  - "Class Sessions"
format:
  revealjs:
    df-print: kable
    footer: "DSAN 5300-01 Week 9: {{< var w09.footer >}}"
    output-file: "slides.html"
    html-math-method: mathjax
    scrollable: true
    theme: [default, "../dsan-globals/jjquarto.scss"]
    slide-number: true
    echo: true
    code-fold: true
    link-external-icon: true
    link-external-newwindow: true
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'>"
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    revealjs-plugins:
      - simplemenu
  html:
    df-print: kable
    output-file: "index.html"
    html-math-method: mathjax
    echo: true
    code-fold: true
---

::: {.content-visible unless-format="revealjs"}

<center class='mb-3'>
<a class="h2" href="./slides.html" target="_blank">Open slides in new tab &rarr;</a>
</center>

:::

# Schedule {.smaller .small-title .crunch-title .crunch-callout data-stack-name="Schedule"}

Today's Planned Schedule:

| | Start | End | Topic |
|:- |:- |:- |:- |
| **Lecture** | 6:30pm | 7:00pm | [Separating Hyperplanes &rarr;](#learning-decision-boundaries) |
| | 7:00pm | 7:20pm | [Max-Margin Classifiers &rarr;](#max-margin-classifiers) | 
| | 7:20pm | 8:00pm | [Support Vector *Classifiers* &rarr;](#support-vector-classifiers) |
| **Break!** | 8:00pm | 8:10pm | |
| | 8:10pm | 9:00pm | [Quiz 2 &rarr;](#quiz-time) |

: {tbl-colwidths="[12,12,12,64]"}

::: {.hidden}

```{r}
#| label: r-source-globals
source("../dsan-globals/_globals.r")
set.seed(5300)
```

:::

{{< include ../dsan-globals/_globals-tex.qmd >}}

## Quick Roadmap {.crunch-title .crunch-ul .text-90 .crunch-li-first}

* Weeks 8-9: Shift from focus on **regression** to focus on **classification** (Though we use lessons from regression!)
* Last Week (W08): SVMs as *new method* with this focus
  * Emphasis on **boundary between classes** $\leadsto$ 2.5hrs on **separating hyperplanes**: in original feature space (Max-Margin, SVCs) or derived feature spaces (SVMs)
* Now: Wait, didn't we discuss a classification method before, though its name confusingly had "regression" in it? ðŸ¤”
  * Take **logistic regression** but use Bayes rule to "flip" from **regression[+thresholding]** task to **class-separation** task (think of SVM's max-width-of-"slab" objective!)

<!-- * Best way I can summarize: logistic regression is great if we **care about $\beta_j$ values** (effect of $\uparrow X$ on $\Pr(Y = 1)$)... 
  * Otherwise, if we care about best **separating** $Y = 0$ from $Y = 1$ (think of SVM's max-width-of-"slab" objective), we can optimize **separation boundary**, point where $\Pr(Y = 1) = \Pr(Y = 0)$ directly
-->

## Logistic Regression Refresher {.crunch-title .math-90 .crunch-math .crunch-li .title-09 .crunch-ul}

* We don't have time for full refresher, but just remember how it involves learning $\beta_j$ values to minimize **loss** w.r.t.

$$
\begin{align*}
&\log\left[ \frac{\Pr(Y = 1 \mid X)}{1 - \Pr(Y = 1 \mid X)} \right] = \beta_0 + \beta_1 X \\
&\iff \Pr(Y = 1 \mid X = x_i) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
\end{align*}
$$

* And then, if we want to **classify $x$** rather than just **predict** $\Pr(Y = 1 \mid X = x)$, we apply a threshold $t \in [0,1]$:

$$
\widehat{y} = \begin{cases}
1 &\text{if }\Pr(Y = 1 \mid X = x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} > t \\
0 &\text{otherwise}
\end{cases}
$$

## Intuition {.crunch-title .crunch-ul .title-09 .inline-90}

* Logistic regression is called a **discriminative model**, since we are learning parameters $\beta_j$ that best produce a predicted **class** $\widehat{y_i}$ from **features** $\mathbf{x}_i$...
* We're modeling $\Pr(Y = k \mid X)$ (for two classes, $k = 0$ and $k = 1$), hence the LHS of the Logistic Regression formula
* But there are cases where we can **do better**^[*(More normally-distributed $X$ $\implies$ more likely to "beat" Logistic Regression)*] by instead modeling (learning parameters for) $\Pr(X \mid Y = k)$, for each $k$, then using Bayes rule to "flip" back to $\Pr(Y = k \mid X)$!<br>$\leadsto$ **LDA**, **QDA**, and **NaÃ¯ve Bayes** classifiers

# Linear Discriminant Analysis (LDA) {data-stack-name="LDA"}

* *Not to be confused with the NLP model called "LDA"!*
* *In that case LDA = "Latent Dirichlet Allocation"*

## Bayes' Rule {.smaller .math-90 .crunch-ul}

* First things first, we generalize from $Y \in \{0, 1\}$ to $K$ possible classes (labels), since the notation for $K$ classes here is not much more complex than 2 classes!
* We label the pieces using ISLR's notation to make our lives easier:

$$
\underbrace{\Pr(Y = k \mid X = x)}_{p_k(x)} = \frac{
  \overbrace{\Pr(X = x \mid Y = k)}^{f_k(x)} \overbrace{\Pr(Y = k)}^{\pi_k}
}{
  \sum_{\ell = 1}^{K} \underbrace{\Pr(X = x \mid Y = \ell)}_{f_{\ell}(x)} \underbrace{\Pr(Y = \ell)}_{\pi_{\ell}}
} = \frac{f_k(x) \overbrace{\pi_k}^{\mathclap{\text{Prior}(k)}}}{\sum_{\ell = 1}^{K}f_{\ell}(x) \underbrace{\pi_\ell}_{\mathclap{\text{Prior}(\ell)}}}
$$

* So if we do have only two classes, $K = 2$ and $p_1(x) = \frac{f_1(x)\pi_1}{f_1(x)\pi_1 + f_0(x)\pi_0}$

* Priors can be estimated as $n_k / n$. The hard work is in modeling $f_k(x)$! With estimates of these two "pieces" for each $k$, we can derive a classifier $\widehat{y}(x) = \argmax_k p_k(x)$

## The LDA Assumption {.smaller .crunch-title .title-11 .math-90 .crunch-ul .crunch-li-8}

* Within each class $k$, values of $x$ are **normally distributed**:
  
$$
(X \mid Y = k) \sim \mathcal{N}(\param{\mu_k}, \param{\sigma^2}) \iff f_k(x) = \frac{1}{\sqrt{2 \pi}\sigma}\exp\left[-\frac{1}{2}\left( \frac{x - \mu_k}{\sigma} \right)^2\right]
$$

* Plugging back into (notationally-simplified) classifier, we get

$$
\widehat{y}(x) = \argmax_{k}\left[ \frac{
  \pi_k \frac{1}{\sqrt{2 \pi}\sigma}\exp\left[-\frac{1}{2}\left( \frac{x - \mu_k}{\sigma} \right)^2\right]
}{
  \sum_{\ell = 1}^{K}\pi_{\ell} \frac{1}{\sqrt{2 \pi}\sigma}\exp\left[-\frac{1}{2}\left( \frac{x - \mu_\ell}{\sigma} \right)^2\right]
}\right],
$$

* Gross, BUT $\argmax_k p_k(x) = \argmax_k \log(p_k(x)) \leadsto$ "linear" discriminant $\delta_k(x)$:

$$
\widehat{y}(x) = \argmax_k[\delta_k(x)] = \argmax_{k}\left[ \overbrace{\frac{\mu_k}{\sigma^2}}^{\smash{m}} x ~ \overbrace{- \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)}^{\smash{b}} \right]
$$

## Estimating From Data

![ISLR Figure 4.4](images/4_4.svg){fig-align="center"}

## Moving to Multiple Features

![ISLR Figure 4.6](images/4_6.svg){fig-align="center"}

## Qudratic Class Boundaries {.smaller .crunch-title .title-12}

* Here, dashed **purple** line is "true" boundary (Bayes decision boundary), dotted **black** line is LDA boundary, solid **green** line is QDA boundary 

![ISLR Figure 4.9](images/4_9.svg){fig-align="center"}

# Quiz Time! {data-stack-name="Quiz"}

## Appendix: Fuller Logistic Derivation {.smaller .crunch-title .title-09}

$$
\begin{align*}
&\log\left[ \frac{\Pr(Y = 1 \mid X)}{1 - \Pr(Y = 1 \mid X)} \right] = \beta_0 + \beta_1 X \\
&\iff \frac{\Pr(Y = 1 \mid X = x_i)}{1 - \Pr(Y = 1\ \mid X = x_i)} = e^{\beta_0 + \beta_1 X} \\
&\iff \Pr(Y = 1 \mid X) = e^{\beta_0 + \beta_1 X}(1 - \Pr(Y = 1 \mid X)) \\
&\iff \Pr(Y = 1 \mid X) = e^{\beta_0 + \beta_1 X} - e^{\beta_0 + \beta_1 X}\Pr(Y = 1 \mid X) \\
&\iff \Pr(Y = 1 \mid X) + e^{\beta_0 + \beta_1 X}\Pr(Y = 1 \mid X) = e^{\beta_0 + \beta_1 X} \\
&\iff \Pr(Y = 1 \mid X)(1 + e^{\beta_0 + \beta_1 X}) = e^{\beta_0 + \beta_1 X} \\
&\iff \Pr(Y = 1 \mid X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
\end{align*}
$$

## References

::: {#refs}
:::
