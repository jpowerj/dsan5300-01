[
  {
    "objectID": "w06/slides.html#reminder-w04-new-goal-generalizability",
    "href": "w06/slides.html#reminder-w04-new-goal-generalizability",
    "title": "Week 6: Regularization for Model Selection",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n Goal 2.0: Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w06/slides.html#clarification-target-diagrams",
    "href": "w06/slides.html#clarification-target-diagrams",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Clarification: Target Diagrams",
    "text": "Clarification: Target Diagrams\n\n\n\n\n\n\n\n\n\n\n\n\nLow Variance\nHigh Variance\n\n\n\n\nLow Bias\n\n\n\n\nHigh Bias\n\n\n\n\n\n\n\nFigure¬†1: Adapted from Fortmann-Roe (2012), ‚ÄúUnderstanding the Bias-Variance Tradeoff‚Äù"
  },
  {
    "objectID": "w06/slides.html#why-was-this-helpful-for-5100",
    "href": "w06/slides.html#why-was-this-helpful-for-5100",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Why Was This Helpful for 5100?",
    "text": "Why Was This Helpful for 5100?\n\nLaw of Large Numbers:\n\nAvg(many sample means \\(s\\)) \\(\\leadsto\\) true mean \\(\\mu\\)\n\n\\(\\widehat{\\theta}\\) unbiased estimator for \\(\\theta\\):\n\nAvg(Estimates \\(\\widehat{\\theta}\\)) \\(\\leadsto\\) true \\(\\theta\\)\n\n\n\nThe Low Bias, High Variance case"
  },
  {
    "objectID": "w06/slides.html#relevance-for-cv-error",
    "href": "w06/slides.html#relevance-for-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Relevance for CV Error",
    "text": "Relevance for CV Error\n\nIn Goal 2.0 world, we choose models on the basis of estimated test error (before, with Goal 1.0, we only used e.g.¬†MSE, RSS, \\(R^2\\), which was fine for linear regression)\nData \\(\\mathbf{D}\\) = single realization of DGP (for 5300, only relevance is why we don‚Äôt look at test set)\n\\(\\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right]\\) = random permutation of \\(\\mathbf{D}\\)\nBullseye on target = true test error(We could compute this, but then we‚Äôd have to end the study, collect more data‚Ä¶ better alternative on next slide!)\nDarts thrown around bullseye = estimated test errors (CV fold errors!)\n\nThey don‚Äôt hit bullseye because we‚Äôre inferring DGP from from sample\n\nTrue test error = \\(f(\\mathbf{D}) = f\\left( \\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right] \\right)\\)\nValidation error = \\(f(\\mathbf{D}_{\\text{Train}}) = f\\left( \\left[ \\mathbf{D}_{\\text{SubTr}} \\middle| \\mathbf{D}_{\\text{Val}} \\right] \\right)\\),\n\n\\(\\implies\\) Validation error is an estimate, using a smaller sample \\(\\mathbf{D}_{\\text{Train}}\\) drawn from the same distribution (DGP) as true test error!"
  },
  {
    "objectID": "w06/slides.html#true-test-error-vs.-cv-error",
    "href": "w06/slides.html#true-test-error-vs.-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "True Test Error vs.¬†CV Error",
    "text": "True Test Error vs.¬†CV Error\nNote the icons! Test set = Lake monster: pulling out of water to evaluate kills it üòµ\n\n\n\n\n\n\n True Test Error \\(\\varepsilon_{\\text{Test}} = \\text{Err}_{\\text{Test}}\\)\n\n\n Data \\(\\mathbf{D}\\) ‚Äúarises‚Äù out of (unobservable) DGP\n Randomly chop \\(\\mathbf{D}\\) into \\(\\left[ \\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}} \\right]\\)\n \\(\\underbrace{\\text{Err}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{no cap}}} = f(\\mathbf{D}_{\\text{Train}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Test}}}_{\\text{This kills monster üò¢}})\\)\nIssue: can only be evaluated once, ever üò±\n\n\n\n\n\n\n\n\n\n Validation Set Error \\(\\varepsilon_{\\text{Val}} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{Err}}_{\\text{Test}}\\)\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\)\n Leave \\(\\mathbf{D}_{\\text{Test}}\\) alone until end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\([\\mathbf{D}_{\\text{SubTr}} \\mid \\mathbf{D}_{\\text{Val}}]\\)\n \\(\\underbrace{\\widehat{\\text{Err}}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{capping a bit}}} = f(\\mathbf{D}_{\\text{SubTr}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Val}}}_{\\text{Monster still alive!}})\\)\n\n\n\n\n\n\n\n\n\n \\(K\\)-Fold Cross-Validation Error \\(\\varepsilon_{(K)} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{E}}\\text{rr}_{\\text{Test}}\\)\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\);  Leave \\(\\mathbf{D}_{\\text{Test}}\\) for end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\(\\left[ \\mathbf{D}_{\\text{TrFold}}^{(1)} \\middle| \\mathbf{D}_{\\text{TrFold}}^{(2)} \\middle| \\cdots \\middle| \\mathbf{D}_{\\text{TrFold}}^{(K)} \\right]\\)\n For \\(i \\in \\{1, \\ldots, K\\}\\):\n¬†¬†¬†¬† \\(\\varepsilon_{\\text{ValFold}}^{(i)} = f\\left( \\mathbf{D}_{\\text{TrFold}}^{(-i)} \\overset{\\text{fit}}{\\longrightarrow} \\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{TrFold}}^{(i)} \\right)\\)\n \\(\\underbrace{\\widehat{\\text{E}}\\text{rr}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{less cap!}}} = \\boxed{\\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{(i)}_{\\text{ValFold}}}~\\) (‚Ä¶monster still alive, even after all that!)"
  },
  {
    "objectID": "w06/slides.html#general-issue-with-cv-its-halfway-there",
    "href": "w06/slides.html#general-issue-with-cv-its-halfway-there",
    "title": "Week 6: Regularization for Model Selection",
    "section": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There",
    "text": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There\nCV plots will often look like (complexity on \\(x\\)-axis and CV error on \\(y\\)-axis):\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ncpl_label &lt;- TeX(\"$M_0$\")\nsim1k_delta_df &lt;- tibble(\n    complexity=1:7,\n    cv_err=c(8, 2, 1, 1, 1, 1, 2),\n    label=c(\"\",\"\",TeX(\"$M_3$\"),\"\",\"\",TeX(\"$M_6$\"),\"\")\n)\nsim1k_delta_df |&gt; ggplot(aes(x=complexity, y=cv_err, label=label)) +\n  geom_line(linewidth=1) +\n  geom_point(size=(2/3)*g_pointsize) +\n  geom_text(vjust=-0.7, size=10, parse=TRUE) +\n  scale_x_continuous(\n    breaks=seq(from=1,to=7,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(\n    title=\"Generic CV Error Plot\",\n    x = \"Complexity\",\n    y = \"CV Error\"\n  )\n\n\n\n\nWe ‚Äúknow‚Äù \\(\\mathcal{M}_3\\) preferable to \\(\\mathcal{M}_6\\) (same error yet, less overfitting) \\(\\implies\\) ‚Äú1SE rule‚Äù\nBut‚Ä¶ heuristic \\(\\;\\nimplies\\) optimal! What are we gaining/losing as we move \\(\\mathcal{M}_6 \\rightarrow \\mathcal{M}_3\\)?\nEnter REGULARIZATION!"
  },
  {
    "objectID": "w06/slides.html#cv-now-goes-into-your-toolbox",
    "href": "w06/slides.html#cv-now-goes-into-your-toolbox",
    "title": "Week 6: Regularization for Model Selection",
    "section": "CV Now Goes Into Your Toolbox",
    "text": "CV Now Goes Into Your Toolbox\n\n(We will take it back out later, I promise!)"
  },
  {
    "objectID": "w06/slides.html#contractual-obligation-stepwise-model-selection",
    "href": "w06/slides.html#contractual-obligation-stepwise-model-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Contractual Obligation: ‚ÄúStepwise‚Äù Model Selection",
    "text": "Contractual Obligation: ‚ÄúStepwise‚Äù Model Selection"
  },
  {
    "objectID": "w06/slides.html#way-cooler-and-more-useful-automatic-model-selection",
    "href": "w06/slides.html#way-cooler-and-more-useful-automatic-model-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Way Cooler and More Useful: Automatic Model Selection",
    "text": "Way Cooler and More Useful: Automatic Model Selection\n\nRidge Regression\nLasso\nElastic Net"
  },
  {
    "objectID": "w06/slides.html#regularized-regression-finally",
    "href": "w06/slides.html#regularized-regression-finally",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Regularized Regression (Finally!)",
    "text": "Regularized Regression (Finally!)\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{2} \\right]\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda_2 \\| \\boldsymbol\\beta \\|_{2} + \\lambda_1 \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]"
  },
  {
    "objectID": "w06/slides.html#the-key-plot",
    "href": "w06/slides.html#the-key-plot",
    "title": "Week 6: Regularization for Model Selection",
    "section": "The Key Plot",
    "text": "The Key Plot\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(ggforce) |&gt; suppressPackageStartupMessages()\nlibrary(patchwork) |&gt; suppressPackageStartupMessages()\n# Bounding the space\nxbound &lt;- c(-1, 1)\nybound &lt;- c(0, 1.65)\nstepsize &lt;- 0.05\ndx &lt;- 0.605\ndy &lt;- 1.6\n# The actual function we're plotting contours for\nb_inter &lt;- 1.5\nmy_f &lt;- function(x,y) 8^(b_inter*(x-dx)*(y-dy) - (x-dx)^2 - (y-dy)^2)\nx_vals &lt;- seq(from=xbound[1], to=xbound[2], by=stepsize)\ny_vals &lt;- seq(from=ybound[1], to=ybound[2], by=stepsize)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\n# Optimal beta df\nbeta_opt_df &lt;- tibble(\n  x=121/200, y=8/5, label=c(TeX(\"$\\\\beta^*_{OLS}$\"))\n)\n# Ridge optimal beta\nridge_opt_df &lt;- tibble(\n  x=0.111, y=0.998, label=c(TeX(\"$\\\\beta^*_{ridge}$\"))\n)\n# Lasso diamond\nlasso_df &lt;- tibble(x=c(1,0,-1,0,1), y=c(0,1,0,-1,0), z=c(1,1,1,1,1))\nlasso_opt_df &lt;- tibble(x=0, y=1, label=c(TeX(\"$\\\\beta^*_{lasso}$\")))\n\n# And plot\nbase_plot &lt;- ggplot() +\n  geom_contour_filled(\n    data=data_df, aes(x=x, y=y, z=z),\n    alpha=0.8, binwidth = 0.04, color='black', linewidth=0.65\n  ) +\n  # y-axis\n  geom_segment(aes(x=0, xend=0, y=-Inf, yend=Inf), color='white', linewidth=0.5, linetype=\"solid\") +\n  # Unconstrained optimal beta\n  geom_point(data=beta_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=beta_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.45, vjust=0.65, parse=TRUE, alpha=0.9\n  ) +\n  scale_fill_viridis_d(option=\"C\") +\n  #coord_equal() +\n  labs(\n    #title = \"Model Selection: Ridge vs. Lasso Constraints\",\n    x = TeX(\"$\\\\beta_1$\"),\n    y = TeX(\"$\\\\beta_2$\")\n  )\nridge_plot &lt;- base_plot +\n  geom_circle(\n    aes(x0=0, y0=0, r=1, alpha=I(0.1), linetype=\"circ\", color='circ'), fill=NA, linewidth=0.5\n  )\n  # geom_point(\n  #   data=data.frame(x=0, y=0), aes(x=x, y=y),\n  #   shape=21, size=135.8, color='white', stroke=1.2, linestyle=\"dashed\"\n  # )\nlasso_plot &lt;- ridge_plot +\n  geom_polygon(\n    data=lasso_df, aes(x=x, y=y, linetype=\"diamond\", color=\"diamond\"),\n    fill='white',\n    alpha=0.5,\n    linewidth=1\n  ) +\n  # Ridge beta\n  geom_point(data=ridge_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=ridge_opt_df, aes(x=x, y=y, label=label),\n    hjust=2, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  # Lasso beta\n  geom_point(data=lasso_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=lasso_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.75, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  ylim(ybound[1], ybound[2]) +\n  # xlim(xbound[1], xbound[2]) +\n  scale_linetype_manual(\"Line\", values=c(\"diamond\"=\"solid\", \"circ\"=\"dashed\"), labels=c(\"a\",\"b\")) +\n  scale_color_manual(\"Color\", values=c(\"diamond\"=\"white\", \"circ\"=\"white\"), labels=c(\"c\",\"d\")) +\n  # scale_fill_manual(\"Test\") +\n  # x-axis\n  geom_segment(aes(x=-Inf, xend=Inf, y=0, yend=0), color='white') +\n  theme_dsan(base_size=16) +\n  coord_fixed() +\n  theme(\n    legend.position = \"none\",\n    axis.line = element_blank(),\n    axis.ticks = element_blank()\n  )\nlasso_plot"
  },
  {
    "objectID": "w06/slides.html#bayesian-interpretation",
    "href": "w06/slides.html#bayesian-interpretation",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\n\nBelief \\(A\\): Most/all of the features you included have important effect on \\(Y\\)\n\\(A \\implies\\) Gaussian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), pdf of \\(X\\) is\n\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right]\n\\]\n\n\nCode\nlibrary(tidyverse)\nlibrary(latex2exp)\nprior_labs &lt;- labs(\n  x = TeX(\"$\\\\beta_j$\"),\n  y = TeX(\"$f(\\\\beta_j)$\")\n)\nggplot() +\n  stat_function(fun=dnorm, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nGaussian prior \\(\\leadsto\\) Ridge Regression!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(\\sigma^2\\))\n\n\n\nBelief \\(B\\): Only a few of the features you included have important effect on \\(Y\\)\n\\(B \\implies\\) Laplacian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{L}(\\mu, b)\\), pdf of \\(X\\) is\n\n\\[\nf(x) = \\frac{1}{2b}\\exp\\left[ -\\left| \\frac{x - \\mu}{b} \\right| \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(extraDistr) |&gt; suppressPackageStartupMessages()\nggplot() +\n  stat_function(fun=dlaplace, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nLaplacian prior \\(\\leadsto\\) Lasso!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(b\\))"
  },
  {
    "objectID": "w06/slides.html#week-7-preview-basis-functions",
    "href": "w06/slides.html#week-7-preview-basis-functions",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Week 7 Preview: Basis Functions",
    "text": "Week 7 Preview: Basis Functions\n\nQ: What do polynomial regression and piecewise regression have in common?\nA: They can both be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b(x_1) + \\beta_2 b(x_2) + \\cdots + \\beta_J b(x_J)\n\\]"
  },
  {
    "objectID": "w06/slides.html#references",
    "href": "w06/slides.html#references",
    "title": "Week 6: Regularization for Model Selection",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Regularization for Model Selection",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#reminder-w04-new-goal-generalizability",
    "href": "w06/index.html#reminder-w04-new-goal-generalizability",
    "title": "Week 6: Regularization for Model Selection",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n\n\n Goal 2.0: Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#clarification-target-diagrams",
    "href": "w06/index.html#clarification-target-diagrams",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Clarification: Target Diagrams",
    "text": "Clarification: Target Diagrams\n\n\n\n\n\n\n\n\n\n\n\n\nLow Variance\nHigh Variance\n\n\n\n\nLow Bias\n\n\n\n\nHigh Bias\n\n\n\n\n\n\n\nFigure¬†1: Adapted from Fortmann-Roe (2012), ‚ÄúUnderstanding the Bias-Variance Tradeoff‚Äù",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#why-was-this-helpful-for-5100",
    "href": "w06/index.html#why-was-this-helpful-for-5100",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Why Was This Helpful for 5100?",
    "text": "Why Was This Helpful for 5100?\n\nLaw of Large Numbers:\n\nAvg(many sample means \\(s\\)) \\(\\leadsto\\) true mean \\(\\mu\\)\n\n\\(\\widehat{\\theta}\\) unbiased estimator for \\(\\theta\\):\n\nAvg(Estimates \\(\\widehat{\\theta}\\)) \\(\\leadsto\\) true \\(\\theta\\)\n\n\n\n\n\nThe Low Bias, High Variance case",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#relevance-for-cv-error",
    "href": "w06/index.html#relevance-for-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Relevance for CV Error",
    "text": "Relevance for CV Error\n\nIn Goal 2.0 world, we choose models on the basis of estimated test error (before, with Goal 1.0, we only used e.g.¬†MSE, RSS, \\(R^2\\), which was fine for linear regression)\nData \\(\\mathbf{D}\\) = single realization of DGP (for 5300, only relevance is why we don‚Äôt look at test set)\n\\(\\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right]\\) = random permutation of \\(\\mathbf{D}\\)\nBullseye on target = true test error(We could compute this, but then we‚Äôd have to end the study, collect more data‚Ä¶ better alternative on next slide!)\nDarts thrown around bullseye = estimated test errors (CV fold errors!)\n\nThey don‚Äôt hit bullseye because we‚Äôre inferring DGP from from sample\n\nTrue test error = \\(f(\\mathbf{D}) = f\\left( \\left[ \\mathbf{D}_{\\text{Train}} \\middle| \\mathbf{D}_{\\text{Test}} \\right] \\right)\\)\nValidation error = \\(f(\\mathbf{D}_{\\text{Train}}) = f\\left( \\left[ \\mathbf{D}_{\\text{SubTr}} \\middle| \\mathbf{D}_{\\text{Val}} \\right] \\right)\\),\n\n\\(\\implies\\) Validation error is an estimate, using a smaller sample \\(\\mathbf{D}_{\\text{Train}}\\) drawn from the same distribution (DGP) as true test error!",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#true-test-error-vs.-cv-error",
    "href": "w06/index.html#true-test-error-vs.-cv-error",
    "title": "Week 6: Regularization for Model Selection",
    "section": "True Test Error vs.¬†CV Error",
    "text": "True Test Error vs.¬†CV Error\nNote the icons! Test set = Lake monster: pulling out of water to evaluate kills it üòµ\n\n\n\n\n\n\n\n\n True Test Error \\(\\varepsilon_{\\text{Test}} = \\text{Err}_{\\text{Test}}\\)\n\n\n\n Data \\(\\mathbf{D}\\) ‚Äúarises‚Äù out of (unobservable) DGP\n Randomly chop \\(\\mathbf{D}\\) into \\(\\left[ \\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}} \\right]\\)\n \\(\\underbrace{\\text{Err}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{no cap}}} = f(\\mathbf{D}_{\\text{Train}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Test}}}_{\\text{This kills monster üò¢}})\\)\nIssue: can only be evaluated once, ever üò±\n\n\n\n\n\n\n\n\n\n Validation Set Error \\(\\varepsilon_{\\text{Val}} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{Err}}_{\\text{Test}}\\)\n\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\)\n Leave \\(\\mathbf{D}_{\\text{Test}}\\) alone until end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\([\\mathbf{D}_{\\text{SubTr}} \\mid \\mathbf{D}_{\\text{Val}}]\\)\n \\(\\underbrace{\\widehat{\\text{Err}}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{capping a bit}}} = f(\\mathbf{D}_{\\text{SubTr}} \\overset{\\text{fit}}{\\longrightarrow} \\underbrace{\\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{Val}}}_{\\text{Monster still alive!}})\\)\n\n\n\n\n\n\n\n\n\n\n \\(K\\)-Fold Cross-Validation Error \\(\\varepsilon_{(K)} = \\widehat{\\varepsilon}_{\\text{Test}} = \\widehat{\\text{E}}\\text{rr}_{\\text{Test}}\\)\n\n\n\n \\(\\text{DGP} \\rightarrow \\mathbf{D}\\);  Randomly chop into \\([\\mathbf{D}_{\\text{Train}} \\mid \\mathbf{D}_{\\text{Test}}]\\);  Leave \\(\\mathbf{D}_{\\text{Test}}\\) for end of study\n Randomly chop \\(\\mathbf{D}_{\\text{Train}}\\) into \\(\\left[ \\mathbf{D}_{\\text{TrFold}}^{(1)} \\middle| \\mathbf{D}_{\\text{TrFold}}^{(2)} \\middle| \\cdots \\middle| \\mathbf{D}_{\\text{TrFold}}^{(K)} \\right]\\)\n For \\(i \\in \\{1, \\ldots, K\\}\\):\n¬†¬†¬†¬† \\(\\varepsilon_{\\text{ValFold}}^{(i)} = f\\left( \\mathbf{D}_{\\text{TrFold}}^{(-i)} \\overset{\\text{fit}}{\\longrightarrow} \\mathcal{M}_{\\theta} \\overset{\\text{eval}}{\\longrightarrow} \\mathbf{D}_{\\text{TrFold}}^{(i)} \\right)\\)\n \\(\\underbrace{\\widehat{\\text{E}}\\text{rr}_{\\text{Test}}}_{\\substack{\\text{Test error,} \\\\ \\text{less cap!}}} = \\boxed{\\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{(i)}_{\\text{ValFold}}}~\\) (‚Ä¶monster still alive, even after all that!)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#general-issue-with-cv-its-halfway-there",
    "href": "w06/index.html#general-issue-with-cv-its-halfway-there",
    "title": "Week 6: Regularization for Model Selection",
    "section": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There",
    "text": "General Issue with CV: It‚Äôs‚Ä¶ Halfway There\nCV plots will often look like (complexity on \\(x\\)-axis and CV error on \\(y\\)-axis):\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\ncpl_label &lt;- TeX(\"$M_0$\")\nsim1k_delta_df &lt;- tibble(\n    complexity=1:7,\n    cv_err=c(8, 2, 1, 1, 1, 1, 2),\n    label=c(\"\",\"\",TeX(\"$M_3$\"),\"\",\"\",TeX(\"$M_6$\"),\"\")\n)\nsim1k_delta_df |&gt; ggplot(aes(x=complexity, y=cv_err, label=label)) +\n  geom_line(linewidth=1) +\n  geom_point(size=(2/3)*g_pointsize) +\n  geom_text(vjust=-0.7, size=10, parse=TRUE) +\n  scale_x_continuous(\n    breaks=seq(from=1,to=7,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(\n    title=\"Generic CV Error Plot\",\n    x = \"Complexity\",\n    y = \"CV Error\"\n  )\n\n\n\n\n\n\n\n\n\n\nWe ‚Äúknow‚Äù \\(\\mathcal{M}_3\\) preferable to \\(\\mathcal{M}_6\\) (same error yet, less overfitting) \\(\\implies\\) ‚Äú1SE rule‚Äù\nBut‚Ä¶ heuristic \\(\\;\\nimplies\\) optimal! What are we gaining/losing as we move \\(\\mathcal{M}_6 \\rightarrow \\mathcal{M}_3\\)?\nEnter REGULARIZATION!",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#cv-now-goes-into-your-toolbox",
    "href": "w06/index.html#cv-now-goes-into-your-toolbox",
    "title": "Week 6: Regularization for Model Selection",
    "section": "CV Now Goes Into Your Toolbox",
    "text": "CV Now Goes Into Your Toolbox\n\n\n\n\n\n(We will take it back out later, I promise!)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#contractual-obligation-stepwise-model-selection",
    "href": "w06/index.html#contractual-obligation-stepwise-model-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Contractual Obligation: ‚ÄúStepwise‚Äù Model Selection",
    "text": "Contractual Obligation: ‚ÄúStepwise‚Äù Model Selection",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#way-cooler-and-more-useful-automatic-model-selection",
    "href": "w06/index.html#way-cooler-and-more-useful-automatic-model-selection",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Way Cooler and More Useful: Automatic Model Selection",
    "text": "Way Cooler and More Useful: Automatic Model Selection\n\nRidge Regression\nLasso\nElastic Net",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#regularized-regression-finally",
    "href": "w06/index.html#regularized-regression-finally",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Regularized Regression (Finally!)",
    "text": "Regularized Regression (Finally!)\nRidge Regression:\n\\[\n\\boldsymbol\\beta^*_{\\text{ridge}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{2} \\right]\n\\]\nLASSO:\n\\[\n\\boldsymbol\\beta^*_{\\text{lasso}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]\nElastic Net:\n\\[\n\\boldsymbol\\beta^*_{\\text{EN}} = \\argmin_{\\beta}\\left[ \\frac{1}{N}\\sum_{i=1}^{N}(\\widehat{y}_i(\\boldsymbol\\beta) - y_i)^2 + \\lambda_2 \\| \\boldsymbol\\beta \\|_{2} + \\lambda_1 \\| \\boldsymbol\\beta \\|_{1} \\right]\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#the-key-plot",
    "href": "w06/index.html#the-key-plot",
    "title": "Week 6: Regularization for Model Selection",
    "section": "The Key Plot",
    "text": "The Key Plot\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(ggforce) |&gt; suppressPackageStartupMessages()\nlibrary(patchwork) |&gt; suppressPackageStartupMessages()\n# Bounding the space\nxbound &lt;- c(-1, 1)\nybound &lt;- c(0, 1.65)\nstepsize &lt;- 0.05\ndx &lt;- 0.605\ndy &lt;- 1.6\n# The actual function we're plotting contours for\nb_inter &lt;- 1.5\nmy_f &lt;- function(x,y) 8^(b_inter*(x-dx)*(y-dy) - (x-dx)^2 - (y-dy)^2)\nx_vals &lt;- seq(from=xbound[1], to=xbound[2], by=stepsize)\ny_vals &lt;- seq(from=ybound[1], to=ybound[2], by=stepsize)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\n# Optimal beta df\nbeta_opt_df &lt;- tibble(\n  x=121/200, y=8/5, label=c(TeX(\"$\\\\beta^*_{OLS}$\"))\n)\n# Ridge optimal beta\nridge_opt_df &lt;- tibble(\n  x=0.111, y=0.998, label=c(TeX(\"$\\\\beta^*_{ridge}$\"))\n)\n# Lasso diamond\nlasso_df &lt;- tibble(x=c(1,0,-1,0,1), y=c(0,1,0,-1,0), z=c(1,1,1,1,1))\nlasso_opt_df &lt;- tibble(x=0, y=1, label=c(TeX(\"$\\\\beta^*_{lasso}$\")))\n\n# And plot\nbase_plot &lt;- ggplot() +\n  geom_contour_filled(\n    data=data_df, aes(x=x, y=y, z=z),\n    alpha=0.8, binwidth = 0.04, color='black', linewidth=0.65\n  ) +\n  # y-axis\n  geom_segment(aes(x=0, xend=0, y=-Inf, yend=Inf), color='white', linewidth=0.5, linetype=\"solid\") +\n  # Unconstrained optimal beta\n  geom_point(data=beta_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=beta_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.45, vjust=0.65, parse=TRUE, alpha=0.9\n  ) +\n  scale_fill_viridis_d(option=\"C\") +\n  #coord_equal() +\n  labs(\n    #title = \"Model Selection: Ridge vs. Lasso Constraints\",\n    x = TeX(\"$\\\\beta_1$\"),\n    y = TeX(\"$\\\\beta_2$\")\n  )\nridge_plot &lt;- base_plot +\n  geom_circle(\n    aes(x0=0, y0=0, r=1, alpha=I(0.1), linetype=\"circ\", color='circ'), fill=NA, linewidth=0.5\n  )\n  # geom_point(\n  #   data=data.frame(x=0, y=0), aes(x=x, y=y),\n  #   shape=21, size=135.8, color='white', stroke=1.2, linestyle=\"dashed\"\n  # )\nlasso_plot &lt;- ridge_plot +\n  geom_polygon(\n    data=lasso_df, aes(x=x, y=y, linetype=\"diamond\", color=\"diamond\"),\n    fill='white',\n    alpha=0.5,\n    linewidth=1\n  ) +\n  # Ridge beta\n  geom_point(data=ridge_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=ridge_opt_df, aes(x=x, y=y, label=label),\n    hjust=2, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  # Lasso beta\n  geom_point(data=lasso_opt_df, aes(x=x, y=y), size=2) +\n  geom_label(\n    data=lasso_opt_df, aes(x=x, y=y, label=label),\n    hjust=-0.75, vjust=-0.15, parse=TRUE, alpha=0.9\n  ) +\n  ylim(ybound[1], ybound[2]) +\n  # xlim(xbound[1], xbound[2]) +\n  scale_linetype_manual(\"Line\", values=c(\"diamond\"=\"solid\", \"circ\"=\"dashed\"), labels=c(\"a\",\"b\")) +\n  scale_color_manual(\"Color\", values=c(\"diamond\"=\"white\", \"circ\"=\"white\"), labels=c(\"c\",\"d\")) +\n  # scale_fill_manual(\"Test\") +\n  # x-axis\n  geom_segment(aes(x=-Inf, xend=Inf, y=0, yend=0), color='white') +\n  theme_dsan(base_size=16) +\n  coord_fixed() +\n  theme(\n    legend.position = \"none\",\n    axis.line = element_blank(),\n    axis.ticks = element_blank()\n  )\nlasso_plot",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#bayesian-interpretation",
    "href": "w06/index.html#bayesian-interpretation",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\n\nBelief \\(A\\): Most/all of the features you included have important effect on \\(Y\\)\n\\(A \\implies\\) Gaussian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), pdf of \\(X\\) is\n\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[ -\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right]\n\\]\n\n\nCode\nlibrary(tidyverse)\nlibrary(latex2exp)\nprior_labs &lt;- labs(\n  x = TeX(\"$\\\\beta_j$\"),\n  y = TeX(\"$f(\\\\beta_j)$\")\n)\nggplot() +\n  stat_function(fun=dnorm, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nGaussian prior \\(\\leadsto\\) Ridge Regression!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(\\sigma^2\\))\n\n\n\nBelief \\(B\\): Only a few of the features you included have important effect on \\(Y\\)\n\\(B \\implies\\) Laplacian prior on \\(\\beta_j\\), \\(\\mu = 0\\)\nIf \\(X \\sim \\mathcal{L}(\\mu, b)\\), pdf of \\(X\\) is\n\n\\[\nf(x) = \\frac{1}{2b}\\exp\\left[ -\\left| \\frac{x - \\mu}{b} \\right| \\right]\n\\]\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(latex2exp) |&gt; suppressPackageStartupMessages()\nlibrary(extraDistr) |&gt; suppressPackageStartupMessages()\nggplot() +\n  stat_function(fun=dlaplace, linewidth=1) +\n  xlim(-3, 3) +\n  theme_dsan(base_size=28) +\n  prior_labs\n\n\n\n\n\n\n\n\n\n\nLaplacian prior \\(\\leadsto\\) Lasso!(High complexity penalty \\(\\lambda\\) \\(\\leftrightarrow\\) low \\(b\\))",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#week-7-preview-basis-functions",
    "href": "w06/index.html#week-7-preview-basis-functions",
    "title": "Week 6: Regularization for Model Selection",
    "section": "Week 7 Preview: Basis Functions",
    "text": "Week 7 Preview: Basis Functions\n\nQ: What do polynomial regression and piecewise regression have in common?\nA: They can both be written in the form\n\\[\nY = \\beta_0 + \\beta_1 b(x_1) + \\beta_2 b(x_2) + \\cdots + \\beta_J b(x_J)\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#references",
    "href": "w06/index.html#references",
    "title": "Week 6: Regularization for Model Selection",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5300-01: Statistical Learning",
    "section": "",
    "text": "Welcome to the homepage for Section 01 (Mondays 6:30-9pm in Car Barn 203) of DSAN 5300: Statistical Learning at Georgetown University, for the Spring 2025 semester!\nIf you‚Äôre looking to book an office hour with Jeff, you can use the link in the sidebar, or this direct link: the office hour blocks for Spring 2025 are held from 3:30-6:30pm every Tuesday.\nUse the following links to view notes and lecture slides for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Introduction to the Course\n\n\nJanuary 8\n\n\n\n\nWeek 2: Linear Regression\n\n\nJanuary 13\n\n\n\n\nWeek 3: Getting Fancy with Regression\n\n\nJanuary 27\n\n\n\n\nWeek 4: The Scourge of Overfitting\n\n\nFebruary 3\n\n\n\n\nWeek 5: Cross-Validation for Model Assessment\n\n\nFebruary 10\n\n\n\n\nWeek 6: Regularization for Model Selection\n\n\nFebruary 18\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  },
  {
    "objectID": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\n\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don‚Äôt like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to ‚Äúfind a pattern‚Äù in this data, by clustering similar cars together\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures",
    "href": "writeups/machine-learning/slides.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (‚Äúfeatures‚Äù) and response variables (‚Äúlabels‚Äù)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\nrating\n\n\n\n\n0\n1000\n1\nDisliked\n\n\n1\n2000\n2\nLiked\n\n\n2\n2500\n1\nLiked\n\n\n3\n1500\n2\nDisliked\n\n\n4\n2200\n1\nLiked\n\n\n\n\n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (‚Äúfeatures‚Äù)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\n\n\n\n\n0\n1000\n1\n\n\n1\n2000\n2\n\n\n2\n2500\n1\n\n\n3\n1500\n2\n\n\n4\n2200\n1"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#different-goals",
    "href": "writeups/machine-learning/slides.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The ‚ÄúLearning‚Äù in Machine Learning",
    "text": "The ‚ÄúLearning‚Äù in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNa√Øve idea: Try random lines (each forming a decision boundary), pick ‚Äúbest‚Äù one\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?",
    "text": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?\n\nWhat‚Äôs your intuition? How about accuracy‚Ä¶ ü§î\n\n\nSo‚Ä¶ what‚Äôs wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What‚Äôs Wrong with Accuracy?",
    "text": "What‚Äôs Wrong with Accuracy?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?1 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)2\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\n\"Nature\"\n\n\ncluster_02\n\n\"Science\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of nameThanks to Bayes‚Äô Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just ‚Äútrue prediction good, false prediction bad‚Äù\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you‚Äôve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots üíÄ\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we‚Äôve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model‚Äôs parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "href": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = ‚Äúdirection of sharpest decrease‚Äù\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the ‚Äúloss crater‚Äù\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#good-and-bad-news",
    "href": "writeups/machine-learning/slides.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) ‚àí p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?\n\n\n\nFigure from @schmidinger_exploring_2019"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "href": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What‚Äôs the Issue?",
    "text": "So What‚Äôs the Issue?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5443776\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5164584\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#generalization",
    "href": "writeups/machine-learning/slides.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8327716\n\n\nCode\nlin_rss_test\n\n\n[1] 0.1733079\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.3972511\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6246616"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))\n\nLeast Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "href": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#cross-validation",
    "href": "writeups/machine-learning/slides.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameters",
    "href": "writeups/machine-learning/slides.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several we‚Äôve already seen ‚Äì can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "href": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater ‚Äúgaps‚Äù between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html",
    "href": "writeups/machine-learning/index.html",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "",
    "text": "Open slides in new window ‚Üí"
  },
  {
    "objectID": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\n\n\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don‚Äôt like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to ‚Äúfind a pattern‚Äù in this data, by clustering similar cars together"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures",
    "href": "writeups/machine-learning/index.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (‚Äúfeatures‚Äù) and response variables (‚Äúlabels‚Äù)\n\n\nsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms, ~rating,\n  0, 1000, 1, \"Disliked\",\n  1, 2000, 2, \"Liked\",\n  2, 2500, 1, \"Liked\",\n  3, 1500, 2, \"Disliked\",\n  4, 2200, 1, \"Liked\"\n)\nsup_data\n\n# A tibble: 5 √ó 4\n  home_id  sqft bedrooms rating  \n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1       0  1000        1 Disliked\n2       1  2000        2 Liked   \n3       2  2500        1 Liked   \n4       3  1500        2 Disliked\n5       4  2200        1 Liked   \n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (‚Äúfeatures‚Äù)\n\n\nunsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms,\n  0, 1000, 1,\n  1, 2000, 2,\n  2, 2500, 1,\n  3, 1500, 2,\n  4, 2200, 1\n)\nunsup_data\n\n# A tibble: 5 √ó 3\n  home_id  sqft bedrooms\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1       0  1000        1\n2       1  2000        2\n3       2  2500        1\n4       3  1500        2\n5       4  2200        1"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  dsan_theme(\"half\")\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# To force a legend\nunsup_grouped &lt;- unsup_data |&gt; mutate(big=bedrooms &gt; 1)\nunsup_grouped[['big']] &lt;- factor(unsup_grouped[['big']], labels=c(\"?1\",\"?2\"))\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms, fill=big)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  theme(legend.background = element_rect(fill=\"white\", color=\"white\"), legend.box.background = element_rect(fill=\"white\"), legend.text = element_text(color=\"white\"), legend.title = element_text(color=\"white\"), legend.position = \"right\") +\n  scale_fill_discrete(labels=c(\"?\",\"?\")) +\n  #scale_color_discrete(values=c(\"white\",\"white\"))\n  scale_color_manual(name=NULL, values=c(\"white\",\"white\")) +\n  #scale_color_manual(values=c(\"?1\"=\"white\",\"?2\"=\"white\"))\n  guides(fill = guide_legend(override.aes = list(shape = NA)))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#different-goals",
    "href": "writeups/machine-learning/index.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_vline(xintercept = 1750, linetype=\"dashed\", color = \"black\", size=1) +\n  annotate('rect', xmin=-Inf, xmax=1750, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[1]) +\n  annotate('rect', xmin=1750, xmax=Inf, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[2])\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n  #geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf, alpha=.2, fill='red'))\n\n\n\nlibrary(ggforce)\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms)) +\n  #scale_color_brewer(palette = \"PuOr\") +\n  geom_mark_ellipse(expand=0.1, aes(fill=big), size = 1) +\n  geom_point(size=g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  #theme(legend.position = \"none\") +\n  #theme(legend.title = text_element(\"?\"))\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(cbPalette[3],cbPalette[4]), labels=c(\"?\",\"?\"))\n\n\n\n\n\n\n\n  #scale_fill_manual(labels=c(\"?\",\"?\"))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The ‚ÄúLearning‚Äù in Machine Learning",
    "text": "The ‚ÄúLearning‚Äù in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNa√Øve idea: Try random lines (each forming a decision boundary), pick ‚Äúbest‚Äù one\n\n\nx_min &lt;- 0\nx_max &lt;- 3000\ny_min &lt;- -1\ny_max &lt;- 3\nrand_y0 &lt;- runif(50, min=y_min, max=y_max)\nrand_y1 &lt;- runif(50, min=y_min, max=y_max)\nrand_slope &lt;- (rand_y1 - rand_y0)/(x_max - x_min)\nrand_intercept &lt;- rand_y0\nrand_lines &lt;- tibble::tibble(id=1:50, slope=rand_slope, intercept=rand_intercept)\n#ggplot() +\n#  geom_abline(data=rand_lines, aes(slope=slope, #intercept=intercept)) +\n#  xlim(0,3000) +\n#  ylim(0,2) +\n#  dsan_theme()\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size=g_pointsize) +\n  labs(\n    title = \"The Like vs. Dislike Boundary: 50 Guesses\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_abline(data=rand_lines, aes(slope=slope, intercept=intercept), linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?",
    "text": "What Makes a ‚ÄúGood‚Äù/‚ÄúBest‚Äù Guess?\n\nWhat‚Äôs your intuition? How about accuracy‚Ä¶ ü§î\n\n\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3100)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nlibrary(patchwork)\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(710),f2(data_range),f2(Inf)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot\n\n\n\n\n\n\n\n\nSo‚Ä¶ what‚Äôs wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What‚Äôs Wrong with Accuracy?",
    "text": "What‚Äôs Wrong with Accuracy?\n\ngen_homes &lt;- function(n) {\n  rand_sqft &lt;- runif(n, min=2000, max=3000)\n  rand_bedrooms &lt;- sample(c(1,2), size=n, prob=c(0.5,0.5), replace=TRUE)\n  rand_ids &lt;- 1:n\n  rand_rating &lt;- \"Liked\"\n  rand_tibble &lt;- tibble::tibble(home_id=rand_ids, sqft=rand_sqft, bedrooms=rand_bedrooms, rating=rand_rating)\n  return(rand_tibble)\n}\nfake_homes &lt;- gen_homes(18)\nfake_sup_data &lt;- dplyr::bind_rows(sup_data, fake_homes)\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\n# And check accuracy\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary1=f1(sqft)) %&gt;% mutate(guessDislike1 = bedrooms &lt; boundary1) %&gt;% mutate(correct1 = ((rating==\"Disliked\") & (guessDislike1)) | (rating==\"Liked\") & (!guessDislike1))\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary2=f2(sqft)) %&gt;% mutate(guessDislike2 = bedrooms &gt; boundary2) %&gt;% mutate(correct2 = ((rating==\"Disliked\") & (guessDislike2)) | (rating==\"Liked\") & (!guessDislike2))\n\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\n\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3200)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 91.3% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct1, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(700),f2(data_range),f2(3100)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 73.9% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct2, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?2 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)3\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\n\"Nature\"\n\n\ncluster_02\n\n\"Science\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just ‚Äútrue prediction good, false prediction bad‚Äù\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you‚Äôve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\nbase &lt;-\n  ggplot() +\n  xlim(-5, 5) +\n  ylim(0, 25) +\n  labs(\n    x = \"Y[obs] - Y[pred]\",\n    y = \"Prediction Badness (Loss)\"\n  ) +\n  dsan_theme()\n\nmy_fn &lt;- function(x) { return(x^2) }\nmy_deriv2 &lt;- function(x) { return(4*x - 4) }\nmy_derivN4 &lt;- function(x) { return(-8*x - 16) }\nbase + geom_function(fun = my_fn, color=cbPalette[1], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_deriv2, color=cbPalette[2], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-4,y=16)), aes(x=x,y=y), color=cbPalette[3], size=g_pointsize/2) + \n  geom_function(fun = my_derivN4, color=cbPalette[3], linewidth=1)\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 70 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\nmy_fake_deriv &lt;- function(x) { return(-x) }\nmy_fake_deriv2 &lt;- function(x) { return(-(1/2)*x + 1/2) }\nmy_fake_deriv3 &lt;- function(x) { return(-(1/4)*x + 3/4) }\n\nd=data.frame(x=c(-2,-1,0,1,2), y=c(1,0,0,1,1))\nbase &lt;- ggplot() +\n  xlim(-5,5) +\n  ylim(0,2) +\n  labs(\n    x=\"Y[obs] - Y[pred]\",\n    y=\"Prediction Badness (Loss)\"\n  ) +\n  geom_step(data=d, mapping=aes(x=x, y=y), linewidth=1) +\n  dsan_theme()\n\nbase + geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_fake_deriv, color=cbPalette[2], linewidth=1) +\n  geom_function(fun = my_fake_deriv2, color=cbPalette[3], linewidth=1) +\n  geom_function(fun = my_fake_deriv3, color=cbPalette[4], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-1,y=1)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) +\n  annotate(\"text\", x = -0.7, y = 1.1, label = \"?\", size=6)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 80 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots üíÄ\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]\n\ny_pred &lt;- seq(from = 0, to = 1, by = 0.001)\ncompute_ce &lt;- function(y_p, y_o) { return(-(y_o * log(y_p) + (1-y_o)*log(1-y_p))) }\nce0 &lt;- compute_ce(y_pred, 0)\nce1 &lt;- compute_ce(y_pred, 1)\nce0_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=0, ce=ce0)\nce1_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=1, ce=ce1)\nce_data &lt;- dplyr::bind_rows(ce0_data, ce1_data)\nggplot(ce_data, aes(x=y_pred, y=ce, color=factor(y_obs))) +\n  geom_line(linewidth=1) +\n  labs(\n    title=\"Binary Cross-Entropy Loss\",\n    x = \"Predicted Value\",\n    y = \"Loss\",\n    color = \"Actual Value\"\n  ) +\n  dsan_theme() +\n  ylim(0,6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we‚Äôve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model‚Äôs parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-strikes-again",
    "href": "writeups/machine-learning/index.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = ‚Äúdirection of sharpest decrease‚Äù\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the ‚Äúloss crater‚Äù\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#good-and-bad-news",
    "href": "writeups/machine-learning/index.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\nFigure from @schmidinger_exploring_2019\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) ‚àí p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#so-whats-the-issue",
    "href": "writeups/machine-learning/index.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What‚Äôs the Issue?",
    "text": "So What‚Äôs the Issue?\n\n\n\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\nsummary(lin_model)$r.squared\n\n[1] 0.5210572\n\nget_rss(lin_model)\n\n[1] 0.51618\n\n\n\nPolynomial Model:\n\n\nsummary(poly_model)$r.squared\n\n[1] 1\n\nget_rss(poly_model)\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/index.html#generalization",
    "href": "writeups/machine-learning/index.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  dsan_theme() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Performance on Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\nlin_r2_test\n\n[1] 0.8755704\n\nlin_rss_test\n\n[1] 0.1378964\n\n\n\nPolynomial Model:\n\n\npoly_r2_test\n\n[1] 0.4251704\n\npoly_rss_test\n\n[1] 0.6370419"
  },
  {
    "objectID": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO4 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#training-vs.-test-data",
    "href": "writeups/machine-learning/index.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#cross-validation",
    "href": "writeups/machine-learning/index.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_04\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameters",
    "href": "writeups/machine-learning/index.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several we‚Äôve already seen ‚Äì can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameter-selection",
    "href": "writeups/machine-learning/index.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater ‚Äúgaps‚Äù between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#footnotes",
    "href": "writeups/machine-learning/index.html#footnotes",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)‚Ü©Ô∏é\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of name‚Ü©Ô∏é\nThanks to Bayes‚Äô Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)‚Ü©Ô∏é\nLeast Absolute Shrinkage and Selection Operator‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/hw2-guide/index.html",
    "href": "writeups/hw2-guide/index.html",
    "title": "Getting Started with HW 2",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nOriginal version posted 10 Feb 2025, 10:00pm"
  },
  {
    "objectID": "writeups/hw2-guide/index.html#full-text-for-hw-2.5-islr-6.8-8a-d",
    "href": "writeups/hw2-guide/index.html#full-text-for-hw-2.5-islr-6.8-8a-d",
    "title": "Getting Started with HW 2",
    "section": "Full Text for HW-2.5: ISLR-6.8 #8(a-d)",
    "text": "Full Text for HW-2.5: ISLR-6.8 #8(a-d)\nHere, for ease of access (since the problem in this case is from the previous edition of ISLR, pdf here‚Äîthank you Prof.¬†James for the PDF link!), is the full text of Section 6.8 Problem 8. Remember that you only need to do (a) through (d)! The full problem is here just for completeness (you can think about how you‚Äôd approach parts (e) and (f)).\n\n\n\n\n\n\nISLR Section 6.8, Exercise #8\n\n\n\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n\nUse the rnorm() function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\epsilon\\) of length \\(n = 100\\).\nGenerate a response vector \\(Y\\) of length \\(n = 100\\) according to the model\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon,\n\\]\nwhere \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) are constants of your choice.\nUse the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X, X^2, \\ldots, X^{10}\\). What is the best model obtained according to \\(C_p\\), BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\nRepeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\nNow fit a lasso model to the simulated data, again using \\(X, X^2, \\ldots, X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\nNow generate a response vector \\(Y\\) according to the model\n\\[\nY = \\beta_0 + \\beta_7 X^7 + \\epsilon,\n\\]\nand perform best subset selection and the lasso. Discuss the results obtained."
  },
  {
    "objectID": "writeups/optimization/index.html",
    "href": "writeups/optimization/index.html",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of ‚Äúshifted into high gear‚Äù.\nSo, although it won‚Äôt help in terms of the Quizzes and Labs you‚Äôve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you‚Äôll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the ‚Äúnext step‚Äù once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample ‚Äúversion‚Äù of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means ‚Äúnothing‚Äù: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for ‚Äúsubject to‚Äù, or sometimes ‚Äúsuch that‚Äù. This means that the full expression in general can be read like ‚Äú\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____‚Äù"
  },
  {
    "objectID": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "href": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of ‚Äúshifted into high gear‚Äù.\nSo, although it won‚Äôt help in terms of the Quizzes and Labs you‚Äôve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you‚Äôll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the ‚Äúnext step‚Äù once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample ‚Äúversion‚Äù of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means ‚Äúnothing‚Äù: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for ‚Äúsubject to‚Äù, or sometimes ‚Äúsuch that‚Äù. This means that the full expression in general can be read like ‚Äú\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____‚Äù"
  },
  {
    "objectID": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "href": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Visualizing Unconstrained and Constrained Optimization",
    "text": "Visualizing Unconstrained and Constrained Optimization\nThe unconstrained optimizations we will carry out here can be visualized as ‚Äúhill climbing‚Äù: the optimization approach you probably learned in calculus class‚Äîcomputing the derivative and setting it equal to zero‚Äîworks precisely because the top of the ‚Äúhill‚Äù is the point at which the derivative becomes zero. For example, if we started climbing from the left side of the following plot, the derivative would get lower and lower as we moved right, hitting zero when we reach the top of the ‚Äúhill‚Äù:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nmy_hill &lt;- function(x) exp(-(1/2)*x^2)\nmy_slope &lt;- function(x) -x*exp(-(1/2)*x^2)\nx0_vals &lt;- c(-1.75, -0.333, 0)\ntangent_at_x0 &lt;- function(x,x0) my_slope(x0)*(x - x0) + my_hill(x0)\ntan_x1 &lt;- function(x) tangent_at_x0(x, x0_vals[1])\ntan_x2 &lt;- function(x) tangent_at_x0(x, x0_vals[2])\ntan_x3 &lt;- function(x) tangent_at_x0(x, x0_vals[3])\neval_df &lt;- tibble(x=x0_vals) |&gt; mutate(y=my_hill(x))\ntan_ext &lt;- 0.5\nslopes &lt;- round(c(\n  my_slope(x0_vals[1]),\n  my_slope(x0_vals[2]),\n  my_slope(x0_vals[3])\n), 3)\nopt_df &lt;- tibble(x=0, y=my_hill(0))\nx_df &lt;- tibble(x=c(-3, 3))\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_function(\n    fun=tan_x1, aes(color=\"x1\"), linewidth=1,\n    xlim=c(\n      x0_vals[1]-tan_ext,\n      x0_vals[1]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x2, aes(color=\"x2\"), linewidth=1,\n    xlim=c(\n      x0_vals[2]-tan_ext,\n      x0_vals[2]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x3, aes(color=\"x3\"), linewidth=1,\n    xlim=c(\n      -1,1\n    )\n  ) +\n  geom_point(\n    data=eval_df,\n    aes(x=x, y=y, color=c(\"x1\",\"x2\",\"x3\")),\n    size=2\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"mle\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at x\",\n    values=c(cb_palette[1], cb_palette[2], cb_palette[3]),\n    labels=slopes\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.15))\n\n\n\n\n\nThe derivatives ‚Äúpoint‚Äù in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nUsing this same metaphor of the top of a hill being the ‚Äúbest‚Äù point, constrained optimization is a bit harder to visualize, but you can think of it like:\n\nGetting (almost) crunched between two walls coming closer together, then\nGetting (almost) crunched between a floor and a ceiling coming closer together,\n\nwhere that coming-closer-together is set up so as to crunch the space, making it smaller and smaller until the only point(s) left are the point(s) representing the optimal solution:\n\n\nCode\nxl &lt;- -0.06\nxu &lt;- 0.06\nyl &lt;- 0.985\nyu &lt;- 1.015\nanno_x &lt;- -1.8\nanno_y &lt;- 0.2\nx_df &lt;- tibble(x=c(-3, 3))\nrib_df &lt;- x_df |&gt; mutate(\n  ymin = -0.1,\n  ymax = 0.1\n)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_rect(\n    aes(fill=\"x\"), xmin=xl, xmax=xu, ymin=-Inf, ymax=Inf, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=xl, xend=xl, y=-Inf, yend=Inf) +\n  geom_segment(x=xu, xend=xu, y=-Inf, yend=Inf) +\n  geom_rect(\n    aes(fill=\"y\"), xmin=-Inf, xmax=Inf, ymin=yl, ymax=yu, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=-Inf, xend=Inf, y=yl, yend=yl) +\n  geom_segment(x=-Inf, xend=Inf, y=yu, yend=yu) +\n  # Anno xmin\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xl-0.5, xend = xl-0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xl-1.2, label = \"x &gt; a\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno xmax\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xu+0.5, xend = xu+0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xu+0.6, label = \"x &lt; b\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno ymin\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yl-0.2, yend = yl-0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yl-0.25, label = \"y &gt; c\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=0.5, size = 5\n  ) +\n  # Anno ymax\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yu+0.2, yend = yu+0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yu+0.2, label = \"y &lt; d\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=-0.5, size = 5\n  ) +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(\"a &lt; x &lt; b\", \"c &lt; y &lt; d\")\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"gmm\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"GMM Estimate\"\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.5))\n\n\n\n\n\nThe constraints ‚Äúcrunch‚Äù the admissible values of \\(x\\) until only one point (the Method of Moments estimate) is left. The metaphor is a bit more contrived in this case, however, since we usually don‚Äôt manually compute these constraints in exactly this form (they are implicit in the GMM‚Äôs system of equations), though numerically (i.e., if we use R to compute the GMM estimate), this is exactly how the constraints would be applied!\n\n\n\n\nLet‚Äôs look at how we can work through both MLE and GMM estimation (which we previously learned separately!) as optimization problems, and how that will give us a unified optimization framework for estimating parameters in the fancier ML models you‚Äôll see in DSAN 5300!"
  },
  {
    "objectID": "writeups/optimization/index.html#general-unconstrained-optimization",
    "href": "writeups/optimization/index.html#general-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Unconstrained Optimization",
    "text": "General Unconstrained Optimization\nAs the name implies, Maximum likelihood estimation involves maximizing something:\n\nIn MLE, that thing is the Likelihood Function \\(\\mathcal{L}(X \\mid \\theta)\\).\nIn optimization, more generally, that thing is called the objective function \\(f(x, \\theta)\\).\n\nThis is the reasoning behind something I might have mentioned in class in earlier weeks, that ‚Äúthe objective function in MLE is the likelihood function‚Äù.\nIn other words, when you see the term ‚Äúobjective function‚Äù, just replace it in your brain with ‚Äúthing I‚Äôm trying to optimize (minimize or maximize) here‚Äù.1\nBefore we look at MLE specifically, therefore, let‚Äôs look at a general unconstrained optimization problem, something you almost surely have already solved tons of times (even if you didn‚Äôt have the vocabulary of optimization, objective functions, constraints, and so on):\n\n\n\n\n\n\nExample 1: General Unconstrained Optimization\n\n\n\nFind \\(x^*\\), the solution to\n\\[\n\\begin{align}\n    \\min_{x} ~ & f(x) = 3x^2 - x \\\\\n    \\text{s.t. } ~ & \\varnothing\n\\end{align}\n\\]\n\n\nFrom calculus, we know that finding the optimum value for a function like this (whether minimum or maximum) boils down to:\n\nComputing the derivative \\(f'(x) = \\frac{\\partial}{\\partial x}f(x)\\),\nSetting it equal to zero: \\(f'(x) = 0\\), and\nSolving this equal for \\(x\\), i.e., finding values \\(x^*\\) which satisfy \\(f'(x^*) = 0\\)\n\nHere, without any constraints, we can follow this exact procedure to find the minimum value. We start by computing the derivative:\n\\[\nf'(x) = \\frac{\\partial}{\\partial x}f(x) = \\frac{\\partial}{\\partial x}\\left[3x^2 - x\\right] = 6x - 1,\n\\]\nthen solve for \\(x^*\\) as the value(s) satisfying \\(\\frac{\\partial}{\\partial x}f'(x^*) = 0\\) for the just-derived \\(f'(x)\\):\n\\[\nf'(x^*) = 0 \\iff 6x^* - 1 = 0 \\iff x^* = \\frac{1}{6}.\n\\]\n\n\n\n\n\n\nDerivative Cheatsheet (Click to Collapse / Expand)\n\n\n\n\n\n(These green boxes are where I get to pop off a tiny bit, but in a way that I hope is helpful! üòú)\nPersonally, I absolutely hate, despise memorizing things. So much of school growing up felt like memorizing a ton of things for no reason, since they were things we could just Google 90% of the time‚Ä¶\nSo, even though people usually associate math with memorization of formulas, for whatever reason I have the opposite association: math was the one class where I didn‚Äôt have to memorize things, because (unlike‚Ä¶ ‚Äúthe mitochondria is the powerhouse of the cell‚Äù) I had really good teachers who always walked us through how to derive things from more basic principles.\nSo, I‚Äôm providing this here as a small set of ‚Äúshortcuts‚Äù, but long story short each of these can be derived from even simpler procedures (I don‚Äôt want to clutter this writeup even more by writing those out, but I‚Äôm happy to walk you through how you could derive these, in office hours for example!)\n\n\n\n\n\n\n\n\nType of Thing\nThing\nChange in Thing when \\(x\\) Changes by Tiny Amount\n\n\n\n\nPolynomial\n\\(f(x) = x^n\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}f(x) = nx^{n-1}\\)\n\n\nFraction\n\\(f(x) = \\frac{1}{x}\\)\nUse Polynomial rule (since \\(\\frac{1}{x} = x^{-1}\\)) to get \\(f'(x) = -\\frac{1}{x^2}\\)\n\n\nLogarithm\n\\(f(x) = \\ln(x)\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x} = \\frac{1}{x}\\)\n\n\nExponential\n\\(f(x) = e^x\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}e^x = e^x\\) (üßê‚ùóÔ∏è)\n\n\nMultiplication\n\\(f(x) = g(x)h(x)\\)\n\\(f'(x) = g'(x)h(x) + g(x)h'(x)\\)\n\n\nDivision\n\\(f(x) = \\frac{g(x)}{h(x)}\\)\nToo hard to memorize‚Ä¶ turn it into Multiplication, as \\(f(x) = g(x)(h(x))^{-1}\\)\n\n\nComposition (Chain Rule)\n\\(f(x) = g(h(x))\\)\n\\(f'(x) = g'(h(x))h'(x)\\)\n\n\nFancy Logarithm\n\\(f(x) = \\ln(g(x))\\)\n\\(f'(x) = \\frac{g'(x)}{g(x)}\\) by Chain Rule\n\n\nFancy Exponential\n\\(f(x) = e^{g(x)}\\)\n\\(f'(x) = g'(x)e^{g(x)}\\) by Chain Rule\n\n\n\n\n\n\nThe reason why derivatives are so important for optimization is that we‚Äôre trying to climb a hill (for maximization; if we‚Äôre minimizing then we‚Äôre trying to reach the bottom of a lake)"
  },
  {
    "objectID": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "href": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Maximum Likelihood Estimation as Unconstrained Optimization",
    "text": "Maximum Likelihood Estimation as Unconstrained Optimization\nNow that we have this general procedure for non-constrained optimization in general, let‚Äôs use it to obtain a maximum likelihood estimate for some probabilistic model.\n\n‚ÄúStandard‚Äù Example: Poisson Distribution\nWe‚Äôll start with an example more similar to the ones you did in DSAN 5100: estimating the rate parameter \\(\\param{\\lambda}\\) for a random variable \\(X\\) with a Poisson distribution.\n\n\n\n\n\n\nExample 2: MLE for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Maximum Likelihood Estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nLike with other distributions we looked at in class, the way to approach problems like this is to write out the details step-by-step until you have enough information to start deriving the MLE. So, the first piece of information we have is:\n\nA random variable \\(X \\sim \\text{Pois}(\\param{\\lambda})\\)\n\nGiven how the Poisson distribution works, this means that2\n\\[\n\\Pr(X = k; \\param{\\lambda}) = \\frac{\\param{\\lambda}^ke^{-\\param{\\lambda}}}{k!},\n\\tag{1}\\]\nwhere we can use probability mass \\(\\Pr(X = k)\\) rather than probability density \\(f_X(k)\\) since the Poisson distribution is a discrete distribution (modeling integer counts rather than continuous values like the normal distribution).\nThe next piece of information we have, in a Maximum Likelihood setup, is an observed dataset containing \\(N\\) i.i.d. datapoints,\n\n\\(\\mathbf{x} = (x_1, \\ldots, x_n)\\),\n\nwhere each point is assumed to be drawn from this Poisson distribution with parameter \\(\\param{\\lambda}\\). This assumption means that we treat each of these observed points \\(x_i\\) as the realization of a Poisson-distributed Random Variable \\(X_i\\), so that (by Equation¬†1 above):\n\\[\n\\Pr(X_i = x_i; \\param{\\lambda}) =  \\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!},\n\\]\nOur goal in Maximum Likelihood Estimation is to take this observed dataset and figure out what value of the parameter \\(\\param{\\lambda}\\) is most likely to have produced \\(\\mathbf{x}\\).\nIn other words, we are trying to find the value of \\(\\param{\\lambda}\\) which maximizes the joint probability that \\(X_1\\) is realized as \\(x_1\\), \\(X_2\\), is realized as \\(x_2\\), and so on up to \\(X_n\\) being realized as \\(x_n\\), which we call the likelihood \\(\\mathcal{L}(\\mathbf{x}; \\param{\\lambda})\\) of the dataset:\n\\[\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}).\n\\]\nThe key for being able to compute this giant joint probability is the independence assumption: since the values in \\(\\mathbf{x}\\) are assumed to be independent and identically distributed (i.i.d.), by the definition of independence, we can factor this full joint probability into the product of individual-variable probabilities:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}) \\\\\n&= \\Pr(X_1 = x_1; \\param{\\lambda}) \\times \\cdots \\times \\Pr(X_n = x_n; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda})\n\\end{align*}\n\\]\nThis factoring into individual terms is the key to solving this problem! Now that we‚Äôve done this, the rest of the problem boils down to a calculus problem. Let‚Äôs write out this product (which will look messy at first, but we‚Äôll make it simpler using \\(\\log()\\) below!):\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!}\n\\end{align*}\n\\]\nBecause \\(\\log()\\) is a monotonic function, the value of \\(x\\) which maximizes \\(\\log(f(x))\\) will be the same as the value which maximizes \\(f(x)\\). So, to make our lives easier since \\(\\log()\\) turns multiplications into additions, we compute the log-likelihood (the log of the likelihood function above), which we denote \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\):\n\\[\n\\ell(\\mathbf{x}; \\param{\\lambda}) = \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right]\n\\]\nI recommend working through this application of \\(\\log()\\) yourself, if you can, then you can click the following button to show the worked-out solution:\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{\\param{\\lambda}^{x_i}e^{-\\lambda}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log (\\param{\\lambda}^{x_i}e^{-\\lambda}) - \\sum_{i=1}^N\\log(x_i!) \\\\\n&= \\sum_{i=1}^{N}x_i\\log(\\param{\\lambda}) + \\sum_{i=1}^{N}\\log(e^{-\\param{\\lambda}}) - \\sum_{i=1}^{N}\\log(x_i!) \\\\\n&= \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!).\n\\end{align*}\n\\]\n\nThis might look scary at first, for example, because of the term with the \\(x_i!\\). However, keep in mind that we won‚Äôt need to worry about this term, since it does not involve \\(\\param{\\lambda}\\), the parameter we are maximizing over!\nSo, following the same procedure as our previous example, we maximize this log-likelihood function with respect to \\(\\param{\\lambda}\\). We start by computing the derivative of \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\) with respect to \\(\\param{\\lambda}\\):\n\\[\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) = \\frac{\\partial}{\\partial \\param{\\lambda}}\\left[ \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!) \\right]\n\\]\nLike before, I recommend working through this yourself on paper, and then you can click the following to show the worked-out solution:\n\n\nClick to Show Solution\n\nSince both \\(\\sum_{i=1}^{N}x_i\\) and \\(\\sum_{i=1}^{N}\\log(x_i!)\\) are constants with respect to \\(\\param{\\lambda}\\), and since the derivative operator \\(\\frac{\\partial}{\\partial\\param{\\lambda}}\\) is linear, this reduces to:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\left( \\sum_{i=1}^{N}x_i\\right) \\frac{\\partial}{\\partial \\param{\\lambda}}[\\log(\\param{\\lambda})] - N\\frac{\\partial}{\\partial\\param{\\lambda}}[\\param{\\lambda}] \\\\\n&= \\frac{\\sum_{i=1}^{N}x_i}{\\param{\\lambda}} - N.\n\\end{align*}\n\\]\nAnd now we can set this equal to zero and solve to obtain the maximum-likelihood estimator \\(\\lambda^*\\):\n\\[\n\\begin{align*}\n&\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} - N = 0 \\\\\n\\iff &\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} = N \\\\\n\\iff &\\sum_{i=1}^{N}x_i = N\\lambda^* \\\\\n\\iff &\\lambda^* = \\frac{1}{N}\\sum_{i=1}^{N}x_i,\n\\end{align*}\n\\]\n\nmeaning that, after all this work, the maximum likelihood estimator for a dataset containing realizations of i.i.d. Poisson RVs is the sample mean of those points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\nIn other words, if you are given a dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\), and you think that the entries in this dataset were generated via the Poisson distribution, then the ‚Äúbest guess‚Äù (if we define ‚Äúbest guess‚Äù as ‚Äúguess with maximum likelihood‚Äù) for the parameter \\(\\param{\\lambda}\\) of this Poisson distribution is the sample mean of the observed points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\n\n\nTrickier Example: Linear Regression\nSince I think linear regression is a really important model to have in the back of your mind as you move towards fancier Machine Learning models, but is a bit more complex than the Poisson case we just looked at, a good starting point is an over-simplified version of linear regression, where we don‚Äôt even have an intercept.\n\n\n\n\n\n\nExample 3: Zero-Intercept Linear Regression\n\n\n\nAssume we have a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_N,y_N))\\) containing noisy observations from some underlying linear relationship \\(y = \\param{\\beta} x\\), so that we model what we observe in \\(\\mathbf{d}\\) as realizations of random variables \\(X\\), \\(Y\\), and \\(\\varepsilon_i\\):\n\\[\nY_i = \\param{\\beta} X_i + \\varepsilon_i,\n\\]\nwhere the variables \\(\\varepsilon_i\\) are i.i.d. normally-distributed variables with mean zero and a given variance \\(\\sigma^2\\): \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).\nFind the Maximum Likelihood Estimator for the single parameter in this case, \\(\\param{\\beta}\\).\n\n\nNote that in general, if a random variable \\(X\\) is distributed normally, then adding things to \\(X\\) just shifts the mean parameter \\(\\mu\\) (meaning, for example, if \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\), then \\(X + 3 \\sim \\mathcal{N}(\\mu + 3, \\sigma)\\)).\nHere, since \\(\\varepsilon_i\\) is a normally-distributed random variable with \\(\\mu = 0\\), the left-hand side of the above equation means that \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nJust as we used the Poisson PMF in the previous example, here you will use the Normal pdf, the probability density function for \\(Y_i\\) in this case, which is typically denoted using the Greek letter \\(\\varphi\\) (‚ÄúPhi‚Äù):\n\\[\n\\varphi(v; \\param{\\mu}, \\param{\\sigma}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{v - \\mu}{\\sigma}\\right)^2\\right].\n\\]\nAlthough in this ‚Äústandard form‚Äù for the pdf of the normal distribution the two parameters are \\(\\mu\\) and \\(\\sigma\\), in our case note that we are not estimating the parameters \\(\\mu\\) and \\(\\sigma\\) itself. Instead, we are estimating a single parameter \\(\\param{\\beta}\\), which is not the mean or standard deviation itself, though it ends up affecting the mean since \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nSo, the way we obtain the likelihood which we can then use to estimate \\(\\param{\\beta}\\) is by plugging \\(\\param{\\beta} X_i\\) into the pdf above, to obtain:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{d}; \\param{\\beta}) &= \\prod_{i=1}^{N}\\varphi(y_i; \\param{\\beta}x_i, \\sigma) \\\\\n&= \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right]\n\\end{align*}\n\\]\nLike in the Poisson case, this looks scary until you transform it into the log-likelihood function, at which point lots of things simplify and you can compute a closed-form solution!\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{d}; \\param{\\beta}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}-\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2}\\sum_{i=1}^{N}\\left(\\frac{y_i - \\param{\\beta} x_i}{\\sigma}\\right)^2 \\\\\n&= -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2.\n\\end{align*}\n\\]\nAnd we now have the log-likelihood in a form where we can compute a derivative straightforwardly (using the derivative ‚Äúrules‚Äù in the table presented earlier):\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\param{\\beta}}\\ell(\\mathbf{d}; \\param{\\beta}) &= \\frac{\\partial}{\\partial\\param{\\beta}}\\left[ -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2 \\right] \\\\\n&= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\param{\\beta}}\\left[ (y_i - \\param{\\beta}x_i)^2 \\right] \\\\\n&= -\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i-\\param{\\beta}x_i)x_i \\\\\n&= -\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right].\n\\end{align*}\n\\]\nBy setting this equal to zero, we can now solve for the MLE estimator \\(\\beta^*\\):\n\\[\n\\begin{align*}\n&-\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right] = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2 = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i = \\param{\\beta}\\sum_{i=1}^{N}x_i^2,\n\\end{align*}\n\\]\nAnd thus the MLE for \\(\\param{\\beta}\\) in this case is\n\\[\n\\widehat{\\beta}_{MLE} = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}.\n\\]\n\nThis means that, if we want the line of ‚Äúbest‚Äù fit for a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_n,y_n))\\), where ‚Äúbest fit‚Äù is defined to be ‚Äúslope with maximum likelihood, with intercept 0‚Äù, this line is\n\\[\ny = \\widehat{\\beta}_{MLE}x = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}x.\n\\]"
  },
  {
    "objectID": "writeups/optimization/index.html#general-constraints-only-optimization",
    "href": "writeups/optimization/index.html#general-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constraints-Only Optimization",
    "text": "General Constraints-Only Optimization\nSeeing an optimization problem written out with only constraints looks a bit weird at first, but will be helpful to consider before we look at full-on constrained optimization.\nLike how we looked at a ‚Äúbasic‚Äù calculus problem before applying the same methodology to MLE before, here let‚Äôs look at a ‚Äúbasic‚Äù algebra problem with inequalities:\n\n\n\n\n\n\nExample 4: Inequality Constraints\n\n\n\nFind the value \\(x^*\\) which satisfies the system of inequalities\n\\[\n\\begin{alignat}{2}\nx^* = &&\\max_{x} \\quad &f(x,y) = 0 \\\\\n&&\\text{s.t.} \\quad & y \\geq x^2 + 1 \\\\\n&& \\quad & y \\leq \\sqrt{1 - x^2}\n\\end{alignat}\n\\]\n\n\nIt looks weird at first because, the \\(\\max_x\\) portion doesn‚Äôt give us anything to work with: we‚Äôre ‚Äúmaximizing‚Äù \\(f(x)\\) which is always just the number \\(0\\)! So, instead, we focus on just the constraints:\n\n\\(y \\geq x^2 + 1\\)\n\\(y \\leq \\sqrt{1 - x^2}\\)\n\nPlotting these two functions, we can see that they meet at exactly one point:\n\n\nCode\nlibrary(latex2exp)\nx_df &lt;- tibble(x=c(-2,2))\nf1 &lt;- function(x) x^2 + 1\nf1_lab &lt;- TeX(\"$y \\\\geq x^2 + 1$\")\nf2 &lt;- function(x) sqrt(1 - x^2)\nf2_lab &lt;- TeX(\"$y \\\\leq \\\\sqrt{1 - x^2}$\")\nsoln_df &lt;- tibble(x=0, y=1)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=f1, color='black') +\n  stat_function(fun=f1, geom=\"ribbon\", mapping=aes(ymin=after_stat(y),ymax=4, fill=\"f1\"), alpha=0.5) +\n  stat_function(fun=f2, color='black') +\n  stat_function(fun=f2, geom=\"area\", aes(fill=\"f2\"), alpha=0.5) +\n  geom_point(data=soln_df, aes(x=x, y=y)) +\n  theme_classic() +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(f1_lab, f2_lab)\n  ) +\n  ylim(0,4)\n\n\n\n\n\n\n\n\n\nwhich means that we can solve for where they‚Äôre equal to derive the unique optimal solution \\(x^*\\)!\n\\[\n\\begin{align*}\n&x^2 + 1 = \\sqrt{1 - x^2} \\\\\n\\iff &(x^2 + 1)^2 = 1 - x^2 \\\\\n\\iff & x^4 + 2x^2 + 1 = 1 - x^2 \\\\\n\\iff & x^4 + 3x^2 = 0 \\\\\n\\iff & x^2(x^2 + 3) = 0,\n\\end{align*}\n\\]\nwhich means that the only possible solutions are the solutions to \\(x^2 = 0\\) and \\(x^2 + 3 = 0\\). The only \\(x\\) which satisfies \\(x^2 = 0\\) is \\(x^* = 0\\). The only \\(x\\) which satisfies \\(x^2 + 3 = 0\\) is \\(x^* = \\sqrt{-3}= \\pm 3i\\), meaning that our only real solution is \\(x^* = 0\\), forming the unique (real) solution to our optimization problem."
  },
  {
    "objectID": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "href": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "GMM Estimation as Constraints-Only Optimization",
    "text": "GMM Estimation as Constraints-Only Optimization\nThe Generalized Method of Moments approach basically takes advantage of the type of ‚Äúcrunching‚Äù we saw in the previous example, setting up a system of equations which ‚Äúcrunches‚Äù the set of possible values for the desired parameter \\(\\param{\\theta}\\) down into just a single value.\nIt is able to accomplish this, basically, by saying:\nIf the entries in our dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) are realizations of RVs \\(X_1, \\ldots, X_n\\) drawn i.i.d. from some distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) with parameters \\(\\param{\\boldsymbol\\theta}\\), then the moments of the theoretical distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) should match their observed counterparts.‚Äù\nSpecifically, \\(\\param{\\boldsymbol\\theta}\\) should be the value which makes the:\n\n\n\n\n\n\n\n\nExpected mean \\(\\mu = \\mathbb{E}[X_i]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved mean of the dataset, \\(\\widehat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nExpected variance \\(\\sigma^2 = \\mathbb{E}[(X_i - \\mu)^2]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved variance of the dataset, \\(\\widehat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^2\\)\n\n\nExpected skewness \\(\\gamma = \\mathbb{E}[(X_i - \\mu)^3]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\gamma} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^3\\)\n\n\nExpected kurtosis \\(\\kappa = \\mathbb{E}[(X_i - \\mu)^4]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\kappa} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^4\\)\n\n\n\nand so on‚Äîas many equations as we need to estimate the parameters \\(\\param{\\boldsymbol\\theta}\\) of the distribution \\(\\mathcal{D}\\)!\nSo, let‚Äôs repeat the earlier problem with the Poisson distribution, using GMM instead of MLE, to compute an estimator for the rate parameter \\(\\param{\\lambda}\\).\n\n\n\n\n\n\nExample 5: GMM Estimate for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Generalized Method of Moments estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nHere, since our distribution only has one parameter \\(\\param{\\lambda}\\), we only need one equation in our system of equations, which will ‚Äúmatch‚Äù the expected value of a Poisson-distributed RV with the mean of our dataset \\(\\mathbf{x}\\):\n\\[\n\\begin{align*}\n&\\mu = \\widehat{\\mu} \\\\\n\\iff &\\mathbb{E}[X_i] = \\frac{1}{N}\\sum_{i=1}^{N}x_i \\\\\n\\iff &\\param{\\lambda} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n\\end{align*}\n\\]\nAnd‚Ä¶ yup, that‚Äôs it! Since the expected value of a poisson-distributed Random Variable \\(X_i\\) is just exactly the rate parameter \\(\\param{\\lambda}\\)3, we‚Äôve obtained the GMM estimate of \\(\\param{\\lambda}\\) as desired here, and we see that in fact the GMM estimator is the same as the MLE estimator in this case."
  },
  {
    "objectID": "writeups/optimization/index.html#general-constrained-optimization",
    "href": "writeups/optimization/index.html#general-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constrained Optimization",
    "text": "General Constrained Optimization\nThough we introduced objective functions and constraints separately here, in reality most problems will require you to optimize with respect to both of these to find the solution to your problem.\nTo use the example I used throughout the semester one more time: if you are trying to estimate the population mean height from a sample of heights, you may want to have:\n\nAn objective function quantifying how ‚Äúgood‚Äù a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) is in terms of fitting the data (this would be precisely the likelihood function), but also\nA constraint on \\(\\mu\\) to ensure that you don‚Äôt get negative values (since heights in reality can‚Äôt be negative), and perhaps another constraint to ensure that you don‚Äôt get absurdly high numbers in the millions and billions as well.\n\nWhile you‚Äôve seen likelihood functions, constraints in optimization world are written out as:\n\nEqualities like \\(\\sum_{i=1}^{N}\\theta_i = 1\\) or\nInequalities like \\(\\theta_i &lt; 0.5 ~ \\forall i\\).\n\nSo, for the normal distribution example just mentioned, you could introduce inequality constraints to re-write the problem in the following form (where we just include the non-negative constraint, for simplicity):\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\mu}, \\sigma} \\quad &\\mathcal{L}(X = v_X \\mid \\param{\\mu}, \\sigma) \\\\\n&& \\text{s.t.} \\quad & \\param{\\mu} &gt; 0\n\\end{alignat}\n\\]\nThese two working together (the objective function and the constraints), it turns out, will ‚Äúsupercharge‚Äù your estimation! Now, instead of just guessing a random number from \\(-\\infty\\) to \\(\\infty\\) as your initial guess for \\(\\widehat{\\mu}\\), you have the power to ‚Äúguide‚Äù the optimization by (say) starting at the lower bound on \\(\\widehat{\\mu}\\), in this case, 0.\nFor solving by hand, though, we‚Äôll need to use a technique called the Lagrange Multiplier approach, to turn this constrained optimization problem back into an unconstrained optimization, since this is the case where we know how to use calculus to solve!\n\n\n\n\n\n\nExample 6: Constrained Optimization via Lagrange Multipliers\n\n\n\nFind the optimal value \\(x^*\\) for the following optimization problem:\n\\[\n\\begin{alignat}{2}\nx^* = &&\\min_{x} \\quad &f(x, y) = 2 - x^2 - 2y^2 \\\\\n&& \\text{s.t.} \\quad & x^2 + y^2 = 1\n\\end{alignat}\n\\]\n\n\nWe can visualize the two ‚Äúpieces‚Äù of this optimization, to see (finally!) how the objective function and the constraints come together:\n\n\nCode\nlibrary(ggforce)\nmy_f &lt;- function(x,y) 2 - x^2 - 2*y^2\nx_vals &lt;- seq(from=-2, to=2, by=0.1)\ny_vals &lt;- seq(from=-2, to=2, by=0.1)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\ndata_df |&gt; ggplot(aes(x=x, y=y, z=z)) +\n  # geom_rect(xmin=0, xmax=1, ymin=-Inf, ymax=Inf, alpha=0.5, fill=cb_palette[1]) +\n  # geom_vline(xintercept=0, linewidth=0.75) +\n  geom_contour_filled(alpha=0.9, binwidth = 0.5, color='black', linewidth=0.2) +\n  geom_point(aes(x=0, y=0)) +\n  geom_circle(aes(x0=0, y0=0, r=1)) +\n  geom_segment(aes(x=0, y=0, xend=1, yend=0), linetype=\"dashed\") +\n  scale_fill_viridis_d(option=\"C\") +\n  theme_classic() +\n  # theme(legend.position=\"none\") +\n  coord_equal()\n\n\n\n\n\n\n\n\nFigure¬†1: The goal of our constrained optimization problem is to find the optimal value(s) \\((x^*,y^*)\\) which maximize \\(f(x,y)\\) (higher values are brighter yellow here), subject to the constraint that the point \\((x^*, y^*)\\) lies on the unit circle.\n\n\n\n\n\nSadly, our earlier approach of finding the derivative of the objective function, setting it equal to zero, and solving, won‚Äôt work here. Let‚Äôs see why. First compute the partial derivatives:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = -2x \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = -4y\n\\end{align*}\n\\]\nThen set them equal to zero and solve the system of equations:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = 0 \\iff -2x^* = 0 \\iff &\\boxed{x^* = 0} \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = 0 \\iff -4y^* = 0 \\iff &\\boxed{y^* = 0}.\n\\end{align*}\n\\]\nWe can see now that our computed optimal point, \\((0, 0)\\), violates the desired constraint, since it does not lie on the unit circle \\(x^2 + y^2 = 1\\):\n\\[\n(x^*)^2 + (y^*)^2 = 0^2 + 0^2 = 0 \\neq 1.\n\\]\nThe Lagrange Multiplier approach comes to the rescue here, because it allows us to use this same derivative-based method by incorporating the constraints into the function we take the derivative of and set equal to zero to solve.\nThe new approach you can use to solve for \\(x^*\\) in situations like this is based on constructing a new function \\(\\mathscr{L}(x, y, \\lambda)\\), called the Lagrangian, where the new parameter \\(\\lambda\\) is just the coefficient on a constraint function \\(g(x, y)\\).\nThis \\(g(x, y)\\) may seem complicated at first, but I think of it like a ‚Äúhelper function‚Äù which you derive from the constraint(s), finding a \\(g(x, y)\\) such that:\n\n\\(g(x) = 0\\) when the constraint is satisfied, and\n\\(g(x) \\neq 0\\) when the constraint is violated.\n\nIn our case, therefore, we can rewrite the constraint like:\n\\[\nx^2 + y^2 = 1 \\iff x^2 + y^2 - 1 = 0,\n\\]\nand choose our \\(g(x,y)\\) to be the left side of this equation: \\(g(x,y) = x^2 + y^2 - 1\\).\nWith our constraint function now chosen, we construct the Lagrangian:\n\\[\n\\begin{align*}\n\\mathscr{L}(x, \\lambda) &= f(x) + \\lambda g(x) = 2 - x^2 - 2y^2 + \\lambda(x^2 + y^2 - 1) \\\\\n&= 2 - x^2 - 2y^2 + \\lambda x^2 + \\lambda y^2 - \\lambda\n\\end{align*}\n\\]\nand optimize in the same way we‚Äôve always optimized via calculus, making sure to compute all three of the partial derivatives:\n\nWith respect to \\(x\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial x} = -2x + 2\\lambda x\n\\]\nWith respect to \\(y\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial y} = -4y + 2\\lambda y\n\\]\nAnd with respect to \\(\\lambda\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial \\lambda} = x^2 + y^2 - 1\n\\]\n\nwe can now set all of these derivatives equal to zero, to obtain a system of equations:\n\\[\n\\begin{align*}\n-2x + 2\\lambda x &= 0 \\\\\n-4y + 2\\lambda y &= 0 \\\\\nx^2 + y^2 - 1 &= 0\n\\end{align*}\n\\]\nThere are a few ways to solve this system (including using matrices!), but here‚Äôs how I solved it, by solving for \\(\\lambda^*\\) first:\n\\[\n\\begin{align*}\n-2x + 2\\lambda^* x = 0 \\iff 2\\lambda^* x = 2x \\iff \\boxed{\\lambda^* = 1},\n\\end{align*}\n\\]\nthen deriving the value of \\(y\\):\n\\[\n\\begin{align*}\n&-4y^* + 2\\lambda^* y^* = 0 \\iff -4y^* + 2(1)y^* = 0 \\\\\n&\\iff -4y^* + 2y^* = 0 \\iff \\boxed{y^* = 0},\n\\end{align*}\n\\]\nand \\(x\\):\n\\[\n\\begin{align*}\n&(x^*)^2 + (y^*)^2 - 1 = 0 \\iff (x^*)^2 + (0)^2 - 1 = 0 \\\\\n&\\iff (x^*)^2 = 1 \\iff \\boxed{x^* = \\pm 1}\n\\end{align*}\n\\]\nAnd indeed, by looking at the plot in Figure¬†1 above, we see that \\((-1,0)\\) and \\((1,0)\\) are the two optimal values here!"
  },
  {
    "objectID": "writeups/optimization/index.html#references",
    "href": "writeups/optimization/index.html#references",
    "title": "Mathematical Optimization",
    "section": "References",
    "text": "References\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf."
  },
  {
    "objectID": "writeups/optimization/index.html#footnotes",
    "href": "writeups/optimization/index.html#footnotes",
    "title": "Mathematical Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA somewhat important point, but I thought I‚Äôd make it a footnote in case it‚Äôs already clear to some folks: when you enter the ‚Äúworld‚Äù of optimization problems, you stop caring so much about whether the problem is a maximization or minimization problem. Instead, you literally just use whichever is easier to compute: the max of \\(f(x)\\) or the min of \\(-f(x)\\). To be a little more specific: in convex optimization, the most general type of optimization we have efficient algorithms for, you always minimize, and then just transform \\(f(x)\\) into \\(-f(x)\\) if your original intent was to maximize (this is‚Ä¶ by convention essentially. See Boyd and Vandenberghe (2004) for more!).‚Ü©Ô∏é\nNote how we use ‚Äú;‚Äù to separate random variables like \\(X\\) in this case from non-random variables like \\(\\lambda\\) in this case: sometimes people use the conditional operator ‚Äú|‚Äù for this, but that can sometimes lead to confusion since conditioning on the value of a Random Variable is different from having the value of a non-random variable as a parameter to the function.‚Ü©Ô∏é\nYou can find a proof of this fact here.‚Ü©Ô∏é"
  },
  {
    "objectID": "writeups/lab-1/index.html",
    "href": "writeups/lab-1/index.html",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let‚Äôs visualize the objective function one more time, then we‚Äôll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we‚Äôre given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)‚Äîthe differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here‚Äîwhile in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "href": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let‚Äôs visualize the objective function one more time, then we‚Äôll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we‚Äôre given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)‚Äîthe differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here‚Äîwhile in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "href": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "title": "Getting Started with Lab 1",
    "section": "How the Derivative Helps Us",
    "text": "How the Derivative Helps Us\nIn this case, our loss function actually has an easily-computed closed-form derivative (though, as mentioned in the info box above, this is usually not the case as we move to more complex models like neural networks). We can use the chain rule \\(\\frac{\\partial}{\\partial x}f(g(w)) = f'(g(w))g'(w)\\) to make our lives easier, letting \\(f(x) = x^2 + 5\\) and \\(g(x) = x - 10\\) and recalling that \\(\\frac{\\partial}{\\partial x}x^2 = 2x\\):\n\\[\nL'(w) = \\frac{\\partial L(w)}{\\partial w} = 2(w - 10)\n\\]\nThe reason this matters / the reason it helps us is as follows. Recall how, in calculus class, we were able to use the derivative as a tool for finding minima and maxima of functions since the minima and maxima of these functions are precisely the values at which the function‚Äôs derivative is zero!\nBut what happens if we‚Äôre not exactly at a minimum, in this case? Calculus classes usually gloss over this question, since the answer would usually be ‚ÄúWhy do we care about points besides these optimal points? We can just compute the minimum and then we‚Äôre done! No need to worry about non-optimal points‚Äù\nHowever, when we work with more complicated models like neural networks, we don‚Äôt necessarily have an exact closed-form solution allowing us to take a derivative, set to zero, and solve. We therefore need to utilize numerical optimization approaches, which use the derivative as information telling us which direction we should move in and (approximately) how much we should move if we want to move from a non-optimal point towards the optimal point.\nLet‚Äôs pick three values of \\(w\\):\n\nA value below the minimizing value, \\(w_&lt; = 5\\),\nThe minimizing value itself, \\(w_0 = 10\\), and\nA value above the minimizing value, \\(w_&gt; = 19\\)\n\n\n\nCode\nlibrary(latex2exp)\nloss_deriv &lt;- function(w) {\n    return(2 * (w - 10))\n}\nw_vals &lt;- c(5, 10, 19)\nw_labels &lt;- factor(c(\"wlt\",\"w0\",\"wgt\"), levels=c(\"wlt\",\"w0\",\"wgt\"))\ndata_df &lt;- tibble(w=w_vals, label=w_labels)\ndata_df &lt;- data_df |&gt;\n  mutate(\n    loss = loss_fn(w),\n    deriv = loss_deriv(w),\n    second_deriv = 2\n  )\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=factor(label)),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_color_manual(\n    \"Our Three Values\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$w_&lt; = 5$\"),\"w0\"=TeX(\"$w_0 = 10$\"),\"wgt\"=TeX(\"$w_&gt; = 19$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\n\n\n\n\nAnd let‚Äôs evaluate both the loss function itself (loss) as well as the derivative of the loss function (deriv) at each point, looking closely at what these values tell us:\n\n\nCode\ndata_df\n\n\n\n\n\n\nw\nlabel\nloss\nderiv\nsecond_deriv\n\n\n\n\n5\nwlt\n30\n-10\n2\n\n\n10\nw0\n5\n0\n2\n\n\n19\nwgt\n86\n18\n2\n\n\n\n\n\n\nHere we can notice that, when we are at a value below the optimal value like \\(w_&lt;\\), the derivative has a negative sign, whereas at a value above the optimal value like \\(w_&gt;\\) the derivative has a positive sign. This relates to one of the natural interpretations of the derivative, one that your calculus class hopefully talked about, as the slope of the line tangent to the curve at that point. Adding to the previous plot of just the points, we can see this ‚Äúslope interpretation‚Äù in action: while the loss value tells us how high or low we are on the \\(y\\)-axis here, the deriv value tells us how steep the loss function is at this point. If we adopt the convention of drawing the tangent lines (for nonzero slopes) as vectors, we get a picture that looks like:\n\n\nCode\ntangent_at_x0 &lt;- function(x,x0) loss_deriv(x0)*(x - x0) + loss_fn(x0)\ntan_wlt &lt;- function(x) tangent_at_x0(x, data_df$w[1])\ntan_w0 &lt;- function(x) tangent_at_x0(x, data_df$w[2])\ntan_wgt &lt;- function(x) tangent_at_x0(x, data_df$w[3])\nslopes &lt;- round(c(\n  data_df$deriv[1],\n  data_df$deriv[2],\n  data_df$deriv[3]\n), 3)\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_function(\n    fun=tan_wlt, aes(color=data_df$label[1]), linewidth=1,\n    xlim=c(0,7.5), arrow = arrow(length=unit(0.30,\"cm\"))\n  ) +\n  geom_function(\n    fun=tan_w0, aes(color=data_df$label[2]), linewidth=1,\n    xlim=c(8,12)\n  ) +\n  geom_function(\n    fun=tan_wgt, aes(color=data_df$label[3]), linewidth=1,\n    xlim=c(14.5,20), arrow = arrow(length=unit(0.30,\"cm\"), ends=\"first\")\n  ) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=label),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at w\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$L'(w_&lt;) = -10$\"),\"w0\"=TeX(\"$L'(w_0) = 0$\"),\"wgt\"=TeX(\"$L'(w_&gt;) = 18$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\nThe derivatives ‚Äúpoint‚Äù in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nAnd this shows us exactly why computing the derivative at a point away from the optimal value still helps us when it comes to numerical optimization, in two ways:\n\nFirst, the derivative at these points (literally) points us in the direction we should go in if we want to move towards the optimal value.\nThen (though I‚Äôll stop after this and let you see the effect of this point by going through the assignment‚Äôs different parts, since it relates to the second derivative rather than the first), notice also how the monotonicity properties of the quadratic function \\(f(x) = x^2\\) also tells us ‚Äúhow wrong‚Äù we are, in a sense:\n\n\\(w_&gt; = 19\\) is further away from the optimal point than \\(w_&lt; = 5\\), therefore\nThe magnitude of the derivative at \\(w_&gt;\\) (18) is greater than the magnitude of the derivative at \\(w_&lt;\\) (10).\n\n\nLike I mentioned, I‚Äôm stopping here since the later portions of the assignment dive into the information that the second derivative at a point can provide for our numerical optimizer, but to test your understanding you can imagine writing a third section here titled ‚ÄúHow The (Second) Derivative Helps Us‚Äù.\nFor example, the Huber Loss is often used as an alternative to both ‚Äúpure‚Äù quadratic loss and ‚Äúpure‚Äù absolute loss functions because it penalizes outliers less harshly than \\(f(x) = x^2\\) but more harshly than \\(f(x) = |x|\\). The following plot illustrates a ‚ÄúHuberized‚Äù version of Lab 1‚Äôs loss function, where values within \\(\\delta = 4\\) units of the optimal value \\(w^* = 10\\) are penalized quadratically, but values more than 4 units away from the optimal value are penalized linearly. Think through what is happening to the second derivative as we move from left to right here (relative to quadratic loss with \\(L''(w) = 2\\) and absolute loss with \\(L''(w) = 1\\)):\n\n\nCode\nabs_loss &lt;- function(w) {\n  return(abs(w - 10) + 5)\n}\ndelta &lt;- 4\nhuberized_loss &lt;- function(w) {\n  cases_result &lt;- ifelse(\n    abs(w - 10) &lt;= delta,\n    (1/2)*(w - 10)^2,\n    delta * (abs(w-10) - (1/2)*delta)\n  )\n  return(cases_result + 5)\n}\ntext_df &lt;- tibble::tribble(\n  ~x, ~y, ~label,\n  4, 100, \"‚Üê Linear\",\n  10, 100, \"Quadratic\",\n  16, 100, \"Linear ‚Üí\"\n)\nggplot() +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=abs_loss,\n    aes(color='Absolute'),\n    linewidth=0.5\n  ) +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=huberized_loss,\n    aes(color='Huber'),\n    linewidth=1\n  ) +\n  stat_function(\n    data=tibble(x=c(0,10)),\n    fun=loss_fn,\n    aes(color='Quadratic'),\n    linewidth=0.5\n  ) +\n  geom_vline(\n    xintercept=10 - delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_vline(\n    xintercept=10+delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_text(\n    data=text_df,\n    aes(x=x, y=y, label=label),\n    color=cb_palette[2]\n  ) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  scale_color_manual(\n    \"Loss Functions\",\n    values=c('Absolute'=cb_palette[1], 'Huber'=cb_palette[2], 'Quadratic'=cb_palette[3])\n    #labels=c('Absolute', 'Huber', 'Quadratic')\n  ) +\n  labs(\n    title = \"Loss Functions: Huber vs. 'Pure' Squared or Absolute\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "w02/slides.html#how-do-we-define-best",
    "href": "w02/slides.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "w02/slides.html#predictive-example-advertising-effects",
    "href": "w02/slides.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#explanatory-example-industrialization-effects",
    "href": "w02/slides.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot"
  },
  {
    "objectID": "w02/slides.html#simple-linear-regression",
    "href": "w02/slides.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we‚Äôll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe‚Äôve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we‚Äôd like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the ‚Äúleast wrong‚Äù predictions: motivates focus on residuals‚Ä¶"
  },
  {
    "objectID": "w02/slides.html#least-squares-minimizing-residuals",
    "href": "w02/slides.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217"
  },
  {
    "objectID": "w02/slides.html#why-not-absolute-value",
    "href": "w02/slides.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we‚Äôre aiming to minimize these residuals‚Ä¶\nGhost of calculus past üò±: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#outliers-penalized-quadratically",
    "href": "w02/slides.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#key-features-of-regression-line",
    "href": "w02/slides.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]"
  },
  {
    "objectID": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But‚Ä¶ What About All the Other Types of Vars?",
    "text": "But‚Ä¶ What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs.¬†continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like ‚ÄúI want to do [regression / classification], but my data is [not cardinal+continuous]‚Äù"
  },
  {
    "objectID": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )"
  },
  {
    "objectID": "w02/slides.html#evaluating-with-residuals",
    "href": "w02/slides.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\nrc2_resid_plot"
  },
  {
    "objectID": "w02/slides.html#now-the-math",
    "href": "w02/slides.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]"
  },
  {
    "objectID": "w02/slides.html#regression-r-vs.-statsmodels",
    "href": "w02/slides.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs.¬†statsmodels",
    "text": "Regression: R vs.¬†statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)"
  },
  {
    "objectID": "w02/slides.html#interpreting-output",
    "href": "w02/slides.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "w02/slides.html#zooming-in-coefficients",
    "href": "w02/slides.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "w02/slides.html#zooming-in-significance",
    "href": "w02/slides.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "w02/slides.html#the-residual-plot",
    "href": "w02/slides.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "w02/slides.html#q-q-plot",
    "href": "w02/slides.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "w02/slides.html#multiple-linear-regression",
    "href": "w02/slides.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "w02/slides.html#visualizing-multiple-linear-regression",
    "href": "w02/slides.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided."
  },
  {
    "objectID": "w02/slides.html#interpreting-mlr",
    "href": "w02/slides.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w02/slides.html#but-wait",
    "href": "w02/slides.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!"
  },
  {
    "objectID": "w02/slides.html#correlations-among-features",
    "href": "w02/slides.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales"
  },
  {
    "objectID": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Linear Regression",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#how-do-we-define-best",
    "href": "w02/index.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.3     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#predictive-example-advertising-effects",
    "href": "w02/index.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\n\n\nNew names:\nRows: 200 Columns: 5\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\n\nCode\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#explanatory-example-industrialization-effects",
    "href": "w02/index.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\n\nRows: 89 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): country_code, country_name\ndbl (12): .rownames, services, agriculture, industrial, manufacturing, resou...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#simple-linear-regression",
    "href": "w02/index.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we‚Äôll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe‚Äôve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we‚Äôd like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the ‚Äúleast wrong‚Äù predictions: motivates focus on residuals‚Ä¶",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#least-squares-minimizing-residuals",
    "href": "w02/index.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#why-not-absolute-value",
    "href": "w02/index.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we‚Äôre aiming to minimize these residuals‚Ä¶\nGhost of calculus past üò±: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\n\n\nWarning in latex2exp(\"$f(x) = x^2$\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#outliers-penalized-quadratically",
    "href": "w02/index.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#key-features-of-regression-line",
    "href": "w02/index.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But‚Ä¶ What About All the Other Types of Vars?",
    "text": "But‚Ä¶ What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs.¬†continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like ‚ÄúI want to do [regression / classification], but my data is [not cardinal+continuous]‚Äù",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n\n\nWarning in latex2exp(\"Parameter Space ($\\\\beta_1$)\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#evaluating-with-residuals",
    "href": "w02/index.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \", round(rc1_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \", round(rc2_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_resid_plot",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#now-the-math",
    "href": "w02/index.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#regression-r-vs.-statsmodels",
    "href": "w02/index.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs.¬†statsmodels",
    "text": "Regression: R vs.¬†statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-output",
    "href": "w02/index.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-coefficients",
    "href": "w02/index.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-significance",
    "href": "w02/index.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\n‚Ñπ Results may be unexpected or may change in future versions of ggplot2.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-residual-plot",
    "href": "w02/index.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#q-q-plot",
    "href": "w02/index.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#multiple-linear-regression",
    "href": "w02/index.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-multiple-linear-regression",
    "href": "w02/index.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-mlr",
    "href": "w02/index.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-wait",
    "href": "w02/index.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#correlations-among-features",
    "href": "w02/index.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\n\n\nRows: 400 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): Own, Student, Married, Region\ndbl (7): Income, Limit, Rating, Cards, Age, Education, Balance\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w04/slides.html#what-we-have-thus-far",
    "href": "w04/slides.html#what-we-have-thus-far",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "What We Have Thus Far",
    "text": "What We Have Thus Far\n\nWe have a core model, regression, that we can build up into p much anything we want!\n\n\n\n\n\n\n\n\nClass Topic\nThis Video\n\n\n\n\nLinear regression\nPachelbel‚Äôs Canon in D (1m26s-1m46s)\n\n\nLogistic regression\nAdd swing:  (1m46s)\n\n\nNeural networks\n(triads \\(\\mapsto\\) 7th/9th chords) (5m24s-5m53s)"
  },
  {
    "objectID": "w04/slides.html#how-can-we-attain-hiromis-aura",
    "href": "w04/slides.html#how-can-we-attain-hiromis-aura",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "How Can We Attain Hiromi‚Äôs Aura?",
    "text": "How Can We Attain Hiromi‚Äôs Aura?\n\nIngredient 1: Lots of examples: find mysteries/questions you care about in the world and think of how regression could help us understand them!\nBut, Ingredient 2 is Generalized Linear Models (GLM), which I‚Äôll give an intro to on the board üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è"
  },
  {
    "objectID": "w04/slides.html#where-are-we-going-what-problems-are-we-solving",
    "href": "w04/slides.html#where-are-we-going-what-problems-are-we-solving",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Where Are We Going / What Problems Are We Solving?",
    "text": "Where Are We Going / What Problems Are We Solving?\n\nAnother nice property we have: OLS estimator is BLUE (Best Linear Unbiased Estimator) of conditional mean \\(\\mathbb{E}[Y \\mid X]\\)\nThe first problem we‚Äôll tackle is: as we move from linear models with these kinds of guarantees to fancier models with more uncertainties / ‚Äúpotholes‚Äù‚Ä¶ how do we ensure they still achieve want we want them to achieve?\nTldr: we can study more complex relationships between \\(X\\) and \\(Y\\) than linear ones, but we lose guarantees like ‚ÄúIf it‚Äôs linear, then it is [this]‚Äù: in other words, we lose this automatic generalizability\n(With great[er] power comes great[er] responsibility!)"
  },
  {
    "objectID": "w04/slides.html#the-level-2-goal-generalizability",
    "href": "w04/slides.html#the-level-2-goal-generalizability",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "The Level 2 Goal: Generalizability",
    "text": "The Level 2 Goal: Generalizability\n\n\n\n\n The Goal of Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w04/slides.html#can-we-just-like-not",
    "href": "w04/slides.html#can-we-just-like-not",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Can We Just, Like, Not?",
    "text": "‚Ä¶Can We Just, Like, Not?\n\n\n\nWhat happens if we ‚Äúunleash‚Äù fancier non-linear models on data the same way we‚Äôve been using linear models?\nThe evil scourge of‚Ä¶ OVERFITTING (‚ö°Ô∏è a single overly-dramatic lightning bolt strikes the whiteboard behind me right at this exact moment what are the odds ‚ö°Ô∏è)\n\n\n\n\n\nYour computer is Yes Man\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nset.seed(5300)\nN &lt;- 30\nx_vals &lt;- runif(N, min=0, max=1)\ny_vals_raw &lt;- 3 * x_vals\ny_noise &lt;- rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_vals_raw + y_noise\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2) +\n  stat_smooth(\n    method=\"lm\",\n    formula=\"y ~ x\",\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Linear Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶ but you‚Äôre only allowed to be linear!‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2.5) +\n  stat_smooth(\n    method=\"lm\",\n    formula=y ~ poly(x, N, raw=TRUE),\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Polynomial Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n(Image Source)"
  },
  {
    "objectID": "w04/slides.html#memorizing-data-vs.-learning-the-relationship",
    "href": "w04/slides.html#memorizing-data-vs.-learning-the-relationship",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Memorizing Data vs.¬†Learning the Relationship",
    "text": "Memorizing Data vs.¬†Learning the Relationship\n\n\n\n\nCode\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHow have we measured ‚Äúgood‚Äù fit? High \\(R^2\\)? Low \\(RSS\\)?\n\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5679903\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5638522\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0"
  },
  {
    "objectID": "w04/slides.html#accuracy-leadsto-5300-generalization",
    "href": "w04/slides.html#accuracy-leadsto-5300-generalization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization",
    "text": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\nCode\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  theme_classic() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Evaluation: Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8906269\n\n\nCode\nlin_rss_test\n\n\n[1] 0.139159\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.5237016\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6060099"
  },
  {
    "objectID": "w04/slides.html#in-other-words",
    "href": "w04/slides.html#in-other-words",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "In Other Words‚Ä¶",
    "text": "In Other Words‚Ä¶\n\nImage source: circulated as secret shitposting among PhD students in seminars"
  },
  {
    "objectID": "w04/slides.html#ok-so-how-do-we-avoid-overfitting",
    "href": "w04/slides.html#ok-so-how-do-we-avoid-overfitting",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Ok So, How Do We Avoid Overfitting?",
    "text": "Ok So, How Do We Avoid Overfitting?\n\nThe gist: penalize model complexity\nOriginal optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y, \\widehat{y})\n\\]\nNew optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\lambda \\mathcal{L}(y, \\widehat{y}) + (1-\\lambda) \\mathsf{Complexity}(\\theta) \\right]\n\\]\nBut how do we measure, and penalize, ‚Äúcomplexity‚Äù?"
  },
  {
    "objectID": "w04/slides.html#regularization-measuring-and-penalizing-complexity",
    "href": "w04/slides.html#regularization-measuring-and-penalizing-complexity",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "w04/slides.html#lasso-and-elastic-net-regularization",
    "href": "w04/slides.html#lasso-and-elastic-net-regularization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 (Tibshirani 1996) is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\nEnsures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\)\n\nLeast Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "w04/slides.html#training-vs.-test-data",
    "href": "w04/slides.html#training-vs.-test-data",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"TB\"\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n    N1[label=\"20%\"] N2[label=\"20%\"] N3[label=\"20%\"] N4[label=\"20%\"]\n    }\n    subgraph cluster_02 {\n        label=\"Test Set (20%)\"\n    N5[label=\"20%\",fillcolor=orange]\n    }\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "w04/slides.html#cross-validation",
    "href": "w04/slides.html#cross-validation",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "w04/slides.html#hyperparameters",
    "href": "w04/slides.html#hyperparameters",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several you‚Äôve already seen in e.g.¬†5000 ‚Äì can you name them?"
  },
  {
    "objectID": "w04/slides.html#hyperparameters-youve-already-seen",
    "href": "w04/slides.html#hyperparameters-youve-already-seen",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters You‚Äôve Already Seen",
    "text": "Hyperparameters You‚Äôve Already Seen\n\nUnsupervised Clustering: The number of clusters we want \\(K\\)\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "w04/slides.html#hyperparameter-selection",
    "href": "w04/slides.html#hyperparameter-selection",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, nodes per layer\nDecision Trees: Max tree depth, max features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM (Kingma and Ba 2017) for Neural Network learning rates)"
  },
  {
    "objectID": "w04/slides.html#now-what",
    "href": "w04/slides.html#now-what",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Now What?",
    "text": "‚Ä¶Now What?\n\nSo we‚Äôve got a trained model‚Ä¶\n\nData collected ‚úÖ\nLoss function chosen ‚úÖ\nGradient descent complete ‚úÖ\nHyperparameters tuned ‚úÖ\nGood \\(F_1\\) score on test data ‚úÖ\n\nWhat‚Äôs our next step?\n\nThis is where engineers and social scientists diverge‚Ä¶\nStay tuned!"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "References",
    "text": "References\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù arXiv. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nTibshirani, Robert. 1996. ‚ÄúRegression Shrinkage and Selection via the Lasso.‚Äù Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267‚Äì88. https://www.jstor.org/stable/2346178."
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#what-we-have-thus-far",
    "href": "w04/index.html#what-we-have-thus-far",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "What We Have Thus Far",
    "text": "What We Have Thus Far\n\nWe have a core model, regression, that we can build up into p much anything we want!\n\n\n\n\n\n\n\n\nClass Topic\nThis Video\n\n\n\n\nLinear regression\nPachelbel‚Äôs Canon in D (1m26s-1m46s)\n\n\nLogistic regression\nAdd swing:  (1m46s)\n\n\nNeural networks\n(triads \\(\\mapsto\\) 7th/9th chords) (5m24s-5m53s)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#how-can-we-attain-hiromis-aura",
    "href": "w04/index.html#how-can-we-attain-hiromis-aura",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "How Can We Attain Hiromi‚Äôs Aura?",
    "text": "How Can We Attain Hiromi‚Äôs Aura?\n\nIngredient 1: Lots of examples: find mysteries/questions you care about in the world and think of how regression could help us understand them!\nBut, Ingredient 2 is Generalized Linear Models (GLM), which I‚Äôll give an intro to on the board üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#where-are-we-going-what-problems-are-we-solving",
    "href": "w04/index.html#where-are-we-going-what-problems-are-we-solving",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Where Are We Going / What Problems Are We Solving?",
    "text": "Where Are We Going / What Problems Are We Solving?\n\nAnother nice property we have: OLS estimator is BLUE (Best Linear Unbiased Estimator) of conditional mean \\(\\mathbb{E}[Y \\mid X]\\)\nThe first problem we‚Äôll tackle is: as we move from linear models with these kinds of guarantees to fancier models with more uncertainties / ‚Äúpotholes‚Äù‚Ä¶ how do we ensure they still achieve want we want them to achieve?\nTldr: we can study more complex relationships between \\(X\\) and \\(Y\\) than linear ones, but we lose guarantees like ‚ÄúIf it‚Äôs linear, then it is [this]‚Äù: in other words, we lose this automatic generalizability\n(With great[er] power comes great[er] responsibility!)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-level-2-goal-generalizability",
    "href": "w04/index.html#the-level-2-goal-generalizability",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "The Level 2 Goal: Generalizability",
    "text": "The Level 2 Goal: Generalizability\n\n\n\n\n\n\n The Goal of Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#can-we-just-like-not",
    "href": "w04/index.html#can-we-just-like-not",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Can We Just, Like, Not?",
    "text": "‚Ä¶Can We Just, Like, Not?\n\n\n\nWhat happens if we ‚Äúunleash‚Äù fancier non-linear models on data the same way we‚Äôve been using linear models?\nThe evil scourge of‚Ä¶ OVERFITTING (‚ö°Ô∏è a single overly-dramatic lightning bolt strikes the whiteboard behind me right at this exact moment what are the odds ‚ö°Ô∏è)\n\n\n\n\n\nYour computer is Yes Man\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.3     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nset.seed(5300)\nN &lt;- 30\nx_vals &lt;- runif(N, min=0, max=1)\ny_vals_raw &lt;- 3 * x_vals\ny_noise &lt;- rnorm(N, mean=0, sd=0.5)\ny_vals &lt;- y_vals_raw + y_noise\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2) +\n  stat_smooth(\n    method=\"lm\",\n    formula=\"y ~ x\",\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Linear Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶ but you‚Äôre only allowed to be linear!‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\nCode\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=2.5) +\n  stat_smooth(\n    method=\"lm\",\n    formula=y ~ poly(x, N, raw=TRUE),\n    se=FALSE,\n    linewidth=1\n  ) +\n  labs(\n    title = paste0(\"Polynomial Regression, N = \",N)\n  ) +\n  theme_dsan(base_size=28)\n\n\n\n\n\nYou: ‚ÄúFit the data‚Ä¶‚Äù Computer: ‚ÄúYou got it boss!‚Äù\n\n\n\n\n\n\n\n(Image Source)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#memorizing-data-vs.-learning-the-relationship",
    "href": "w04/index.html#memorizing-data-vs.-learning-the-relationship",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Memorizing Data vs.¬†Learning the Relationship",
    "text": "Memorizing Data vs.¬†Learning the Relationship\n\n\n\n\nCode\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHow have we measured ‚Äúgood‚Äù fit? High \\(R^2\\)? Low \\(RSS\\)?\n\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5679903\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5638522\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#accuracy-leadsto-5300-generalization",
    "href": "w04/index.html#accuracy-leadsto-5300-generalization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization",
    "text": "5000: Accuracy \\(\\leadsto\\) 5300: Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\nCode\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  theme_classic() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Evaluation: Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  theme_dsan(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8906269\n\n\nCode\nlin_rss_test\n\n\n[1] 0.139159\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.5237016\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6060099",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#in-other-words",
    "href": "w04/index.html#in-other-words",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "In Other Words‚Ä¶",
    "text": "In Other Words‚Ä¶\n\n\n\nImage source: circulated as secret shitposting among PhD students in seminars",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#ok-so-how-do-we-avoid-overfitting",
    "href": "w04/index.html#ok-so-how-do-we-avoid-overfitting",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Ok So, How Do We Avoid Overfitting?",
    "text": "Ok So, How Do We Avoid Overfitting?\n\nThe gist: penalize model complexity\nOriginal optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y, \\widehat{y})\n\\]\nNew optimization:\n\\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\lambda \\mathcal{L}(y, \\widehat{y}) + (1-\\lambda) \\mathsf{Complexity}(\\theta) \\right]\n\\]\nBut how do we measure, and penalize, ‚Äúcomplexity‚Äù?",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#regularization-measuring-and-penalizing-complexity",
    "href": "w04/index.html#regularization-measuring-and-penalizing-complexity",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(\\widehat{y} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to ‚Äújust figure it out‚Äù\nThe gist, in the general case, is thus: try to ‚Äúamplify‚Äù the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#lasso-and-elastic-net-regularization",
    "href": "w04/index.html#lasso-and-elastic-net-regularization",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 (Tibshirani 1996) is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\nEnsures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#training-vs.-test-data",
    "href": "w04/index.html#training-vs.-test-data",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"TB\"\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n    N1[label=\"20%\"] N2[label=\"20%\"] N3[label=\"20%\"] N4[label=\"20%\"]\n    }\n    subgraph cluster_02 {\n        label=\"Test Set (20%)\"\n    N5[label=\"20%\",fillcolor=orange]\n    }\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#cross-validation",
    "href": "w04/index.html#cross-validation",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_04\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameters",
    "href": "w04/index.html#hyperparameters",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) ‚Äúsettings‚Äù for our learning procedure (that we haven‚Äôt optimized via gradient descent)\nThere are several you‚Äôve already seen in e.g.¬†5000 ‚Äì can you name them?",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameters-youve-already-seen",
    "href": "w04/index.html#hyperparameters-youve-already-seen",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameters You‚Äôve Already Seen",
    "text": "Hyperparameters You‚Äôve Already Seen\n\nUnsupervised Clustering: The number of clusters we want \\(K\\)\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#hyperparameter-selection",
    "href": "w04/index.html#hyperparameter-selection",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, nodes per layer\nDecision Trees: Max tree depth, max features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM (Kingma and Ba 2017) for Neural Network learning rates)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#now-what",
    "href": "w04/index.html#now-what",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "‚Ä¶Now What?",
    "text": "‚Ä¶Now What?\n\nSo we‚Äôve got a trained model‚Ä¶\n\nData collected ‚úÖ\nLoss function chosen ‚úÖ\nGradient descent complete ‚úÖ\nHyperparameters tuned ‚úÖ\nGood \\(F_1\\) score on test data ‚úÖ\n\nWhat‚Äôs our next step?\n\nThis is where engineers and social scientists diverge‚Ä¶\nStay tuned!",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "References",
    "text": "References\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù arXiv. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nTibshirani, Robert. 1996. ‚ÄúRegression Shrinkage and Selection via the Lasso.‚Äù Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267‚Äì88. https://www.jstor.org/stable/2346178.",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: The Scourge of Overfitting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLeast Absolute Shrinkage and Selection Operator‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-regression-is-not",
    "href": "w03/index.html#what-regression-is-not",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "What Regression is Not",
    "text": "What Regression is Not\n\nFinal reminder that Regression, PCA have different goals!\nIf your goal was to, e.g., generate realistic \\((x,y)\\) pairs, then (mathematically) you want PCA! Roughly:\n\\[\n\\widehat{f}_{\\text{PCA}} = \\min_{\\mathbf{c}}\\left[ \\sum_{i=1}^{n} (\\widehat{x}_i(\\mathbf{c}) - x_i)^2 + (\\widehat{y}_i(\\mathbf{c}) - y_i)^2 \\right]\n\\]\nOur goal is a good predictor of \\(Y\\):\n\\[\n\\widehat{f}_{\\text{Reg}} = \\min_{\\beta_0, \\beta_1}\\left[ \\sum_{i=1}^{n} (\\widehat{y}_i(\\beta) - y_i)^2 \\right]\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#how-do-we-define-best",
    "href": "w03/index.html#how-do-we-define-best",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.3     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#multiple-linear-regression-mlr-model",
    "href": "w03/index.html#multiple-linear-regression-mlr-model",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Multiple Linear Regression (MLR) Model",
    "text": "Multiple Linear Regression (MLR) Model\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#visualizing-multiple-linear-regression",
    "href": "w03/index.html#visualizing-multiple-linear-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#interpreting-mlr",
    "href": "w03/index.html#interpreting-mlr",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\nCode\nmlr_model = smf.ols(\n  formula=\"sales ~ TV + radio + newspaper\",\n  data=ad_df\n)\nmlr_result = mlr_model.fit()\nprint(mlr_result.summary().tables[1])\n\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\n\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\n\n\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#but-wait",
    "href": "w03/index.html#but-wait",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\n# print(mlr_result.summary2(float_format='%.3f'))\nprint(mlr_result.summary2())\n\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.896   \nDependent Variable: sales            AIC:                780.3622\nDate:               2025-02-17 00:52 BIC:                793.5555\nNo. Observations:   200              Log-Likelihood:     -386.18 \nDf Model:           3                F-statistic:        570.3   \nDf Residuals:       196              Prob (F-statistic): 1.58e-96\nR-squared:          0.897            Scale:              2.8409  \n------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025  0.975]\n------------------------------------------------------------------\nIntercept       2.9389    0.3119   9.4223  0.0000   2.3238  3.5540\nTV              0.0458    0.0014  32.8086  0.0000   0.0430  0.0485\nradio           0.1885    0.0086  21.8935  0.0000   0.1715  0.2055\nnewspaper      -0.0010    0.0059  -0.1767  0.8599  -0.0126  0.0105\n-----------------------------------------------------------------\nOmnibus:             60.414       Durbin-Watson:          2.084  \nProb(Omnibus):       0.000        Jarque-Bera (JB):       151.241\nSkew:                -1.327       Prob(JB):               0.000  \nKurtosis:            6.332        Condition No.:          454    \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nCode\nslr_model = smf.ols(\n  formula=\"sales ~ newspaper\",\n  data=ad_df\n)\nslr_result = slr_model.fit()\nprint(slr_result.summary2())\n\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.047    \nDependent Variable: sales            AIC:                1220.6714\nDate:               2025-02-17 00:52 BIC:                1227.2680\nNo. Observations:   200              Log-Likelihood:     -608.34  \nDf Model:           1                F-statistic:        10.89    \nDf Residuals:       198              Prob (F-statistic): 0.00115  \nR-squared:          0.052            Scale:              25.933   \n-------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nIntercept      12.3514    0.6214  19.8761  0.0000  11.1260  13.5769\nnewspaper       0.0547    0.0166   3.2996  0.0011   0.0220   0.0874\n------------------------------------------------------------------\nOmnibus:               6.231        Durbin-Watson:           1.983\nProb(Omnibus):         0.044        Jarque-Bera (JB):        5.483\nSkew:                  0.330        Prob(JB):                0.064\nKurtosis:              2.527        Condition No.:           65   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#correlations-among-features",
    "href": "w03/index.html#correlations-among-features",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\nad_df.drop(columns=\"id\").corr()\n\n\n                 TV     radio  newspaper     sales\nTV         1.000000  0.054809   0.056648  0.782224\nradio      0.054809  1.000000   0.354104  0.576223\nnewspaper  0.056648  0.354104   1.000000  0.228299\nsales      0.782224  0.576223   0.228299  1.000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w03/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\times \\texttt{income} \\\\\n&\\phantom{Y}\n\\end{align*}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\n\n\nRows: 400 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): Own, Student, Married, Region\ndbl (7): Income, Limit, Rating, Cards, Age, Education, Balance\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé\n(Dear future Jeff, let‚Äôs go through this on the board! Sincerely, past Jeff)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#from-mlr-to-logistic-regression",
    "href": "w03/index.html#from-mlr-to-logistic-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "From MLR to Logistic Regression",
    "text": "From MLR to Logistic Regression\n\nAs DSAN students, you know that we‚Äôre still sweeping classification under the rug!\nWe saw how to include binary/multiclass covariates, but what if the actual thing we‚Äôre trying to predict is binary?\nThe wrong approach is the ‚ÄúLinear Probability Model‚Äù:\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#credit-default",
    "href": "w03/index.html#credit-default",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Credit Default",
    "text": "Credit Default\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ndefault_df &lt;- read_csv(\"assets/Default.csv\") |&gt;\n  mutate(default_num = ifelse(default==\"Yes\",1,0))\n\n\nRows: 10000 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): default, student\ndbl (2): balance, income\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndefault_plot &lt;- default_df |&gt; ggplot(aes(x=balance, y=income, color=default, shape=default)) +\n  geom_point(alpha=0.6) +\n  theme_classic(base_size=16) +\n  labs(\n    title=\"Credit Defaults by Income and Account Balance\",\n    x = \"Account Balance\",\n    y = \"Income\"\n  )\ndefault_mplot &lt;- default_plot |&gt; ggMarginal(type=\"boxplot\", groupColour=FALSE, groupFill=TRUE)\ndefault_mplot",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#lines-vs.-sigmoids",
    "href": "w03/index.html#lines-vs.-sigmoids",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Lines vs.¬†Sigmoids(!)",
    "text": "Lines vs.¬†Sigmoids(!)\n\n\nHere‚Äôs what lines look like for this dataset:\n\n\nCode\n#lpm_model &lt;- lm(default ~ balance, data=default_df)\ndefault_df |&gt; ggplot(\n    aes(\n      x=balance, y=default_num\n    )\n  ) +\n  geom_point(aes(color=default)) +\n  stat_smooth(method=\"lm\", formula=y~x, color='black') +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs what sigmoids look like:\n\n\nCode\nlibrary(tidyverse)\nlogistic_model &lt;- glm(default_num ~ balance, family=binomial(link='logit'),data=default_df)\ndefault_df$predictions &lt;- predict(logistic_model, newdata = default_df, type = \"response\")\nmy_sigmoid &lt;- function(x) 1 / (1+exp(-x))\ndefault_df |&gt; ggplot(aes(x=balance, y=default_num)) +\n  #stat_function(fun=my_sigmoid) +\n  geom_point(aes(color=default)) +\n  geom_line(\n    data=default_df,\n    aes(x=balance, y=predictions),\n    linewidth=1\n  ) +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\n\\[\n\\log\\underbrace{\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\mathclap{\\smash{\\text{Odds Ratio}}}} = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#but-lets-derive-this",
    "href": "w03/index.html#but-lets-derive-this",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Let‚Äôs Derive This!",
    "text": "But Let‚Äôs Derive This!\n\\[\n\\begin{align*}\n\\Pr(Y \\mid X) &= \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\\\\n\\iff \\underbrace{\\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)}}_{\\text{Odds Ratio}} &= e^{\\beta_0 + \\beta_1X} \\\\\n\\iff \\underbrace{\\log\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\text{Log-Odds Ratio}} &= \\beta_0 + \\beta_1X\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#now-how-will-we-ever-find-good-parameter-values",
    "href": "w03/index.html#now-how-will-we-ever-find-good-parameter-values",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?",
    "text": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?\n\nIf only we had some sort of estimator‚Ä¶ One that would choose \\(\\beta_0\\) and \\(\\beta_1\\) so as to maximize the likelihood of seeing some data‚Ä¶\n\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{\\{i \\mid y_i = 1\\}}\\Pr(Y = 1 \\mid X) \\prod_{\\{i \\mid y_i = 0\\}}(1-\\Pr(Y = 1 \\mid X))\n\\]\n\n(Much more on this later üò∏)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-interpretation-problem",
    "href": "w03/index.html#the-interpretation-problem",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "The Interpretation Problem",
    "text": "The Interpretation Problem\n\n\nCode\noptions(scipen = 999)\nprint(summary(logistic_model))\n\n\n\nCall:\nglm(formula = default_num ~ balance, family = binomial(link = \"logit\"), \n    data = default_df)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nSlope is no longer the same everywhere! It varies across different values of \\(x\\)‚Ä¶\nLet‚Äôs brainstorm some possible ways to make our lives easier when interpreting these coefficients!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#appendix-mlr-in-r",
    "href": "w03/index.html#appendix-mlr-in-r",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Appendix: MLR in R",
    "text": "Appendix: MLR in R\n\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\n\n\nNew names:\nRows: 200 Columns: 5\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\n\nCode\nmlr_model &lt;- lm(\n  sales ~ TV + radio + newspaper,\n  data=ad_df\n)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422 &lt;0.0000000000000002 ***\nTV           0.045765   0.001395  32.809 &lt;0.0000000000000002 ***\nradio        0.188530   0.008611  21.893 &lt;0.0000000000000002 ***\nnewspaper   -0.001037   0.005871  -0.177                0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/slides.html#what-regression-is-not",
    "href": "w03/slides.html#what-regression-is-not",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "What Regression is Not",
    "text": "What Regression is Not\n\nFinal reminder that Regression, PCA have different goals!\nIf your goal was to, e.g., generate realistic \\((x,y)\\) pairs, then (mathematically) you want PCA! Roughly:\n\\[\n\\widehat{f}_{\\text{PCA}} = \\min_{\\mathbf{c}}\\left[ \\sum_{i=1}^{n} (\\widehat{x}_i(\\mathbf{c}) - x_i)^2 + (\\widehat{y}_i(\\mathbf{c}) - y_i)^2 \\right]\n\\]\nOur goal is a good predictor of \\(Y\\):\n\\[\n\\widehat{f}_{\\text{Reg}} = \\min_{\\beta_0, \\beta_1}\\left[ \\sum_{i=1}^{n} (\\widehat{y}_i(\\beta) - y_i)^2 \\right]\n\\]"
  },
  {
    "objectID": "w03/slides.html#how-do-we-define-best",
    "href": "w03/slides.html#how-do-we-define-best",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "w03/slides.html#multiple-linear-regression-mlr-model",
    "href": "w03/slides.html#multiple-linear-regression-mlr-model",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Multiple Linear Regression (MLR) Model",
    "text": "Multiple Linear Regression (MLR) Model\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "w03/slides.html#visualizing-multiple-linear-regression",
    "href": "w03/slides.html#visualizing-multiple-linear-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided."
  },
  {
    "objectID": "w03/slides.html#interpreting-mlr",
    "href": "w03/slides.html#interpreting-mlr",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\nCode\nmlr_model = smf.ols(\n  formula=\"sales ~ TV + radio + newspaper\",\n  data=ad_df\n)\nmlr_result = mlr_model.fit()\nprint(mlr_result.summary().tables[1])\n\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\n\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\n\n\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w03/slides.html#but-wait",
    "href": "w03/slides.html#but-wait",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Wait‚Ä¶",
    "text": "But Wait‚Ä¶\n\n\n\n\nCode\n# print(mlr_result.summary2(float_format='%.3f'))\nprint(mlr_result.summary2())\n\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.896   \nDependent Variable: sales            AIC:                780.3622\nDate:               2025-02-17 00:51 BIC:                793.5555\nNo. Observations:   200              Log-Likelihood:     -386.18 \nDf Model:           3                F-statistic:        570.3   \nDf Residuals:       196              Prob (F-statistic): 1.58e-96\nR-squared:          0.897            Scale:              2.8409  \n------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025  0.975]\n------------------------------------------------------------------\nIntercept       2.9389    0.3119   9.4223  0.0000   2.3238  3.5540\nTV              0.0458    0.0014  32.8086  0.0000   0.0430  0.0485\nradio           0.1885    0.0086  21.8935  0.0000   0.1715  0.2055\nnewspaper      -0.0010    0.0059  -0.1767  0.8599  -0.0126  0.0105\n-----------------------------------------------------------------\nOmnibus:             60.414       Durbin-Watson:          2.084  \nProb(Omnibus):       0.000        Jarque-Bera (JB):       151.241\nSkew:                -1.327       Prob(JB):               0.000  \nKurtosis:            6.332        Condition No.:          454    \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\nCode\nslr_model = smf.ols(\n  formula=\"sales ~ newspaper\",\n  data=ad_df\n)\nslr_result = slr_model.fit()\nprint(slr_result.summary2())\n\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.047    \nDependent Variable: sales            AIC:                1220.6714\nDate:               2025-02-17 00:51 BIC:                1227.2680\nNo. Observations:   200              Log-Likelihood:     -608.34  \nDf Model:           1                F-statistic:        10.89    \nDf Residuals:       198              Prob (F-statistic): 0.00115  \nR-squared:          0.052            Scale:              25.933   \n-------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nIntercept      12.3514    0.6214  19.8761  0.0000  11.1260  13.5769\nnewspaper       0.0547    0.0166   3.2996  0.0011   0.0220   0.0874\n------------------------------------------------------------------\nOmnibus:               6.231        Durbin-Watson:           1.983\nProb(Omnibus):         0.044        Jarque-Bera (JB):        5.483\nSkew:                  0.330        Prob(JB):                0.064\nKurtosis:              2.527        Condition No.:           65   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It‚Äôs how we‚Äôre able to control for confounding variables!"
  },
  {
    "objectID": "w03/slides.html#correlations-among-features",
    "href": "w03/slides.html#correlations-among-features",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\nad_df.drop(columns=\"id\").corr()\n\n\n                 TV     radio  newspaper     sales\nTV         1.000000  0.054809   0.056648  0.782224\nradio      0.054809  1.000000   0.354104  0.576223\nnewspaper  0.056648  0.354104   1.000000  0.228299\nsales      0.782224  0.576223   0.228299  1.000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher‚Ä¶\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets‚Ä¶\nIn SLR which only examines sales vs.¬†newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales‚Ä¶\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper ‚Äúgets credit‚Äù for the association between radio and sales"
  },
  {
    "objectID": "w03/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w03/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n\n\n\\[\n\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\times \\texttt{income} \\\\\n&\\phantom{Y}\n\\end{align*}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression üòé\n(Dear future Jeff, let‚Äôs go through this on the board! Sincerely, past Jeff)"
  },
  {
    "objectID": "w03/slides.html#from-mlr-to-logistic-regression",
    "href": "w03/slides.html#from-mlr-to-logistic-regression",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "From MLR to Logistic Regression",
    "text": "From MLR to Logistic Regression\n\nAs DSAN students, you know that we‚Äôre still sweeping classification under the rug!\nWe saw how to include binary/multiclass covariates, but what if the actual thing we‚Äôre trying to predict is binary?\nThe wrong approach is the ‚ÄúLinear Probability Model‚Äù:\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]"
  },
  {
    "objectID": "w03/slides.html#credit-default",
    "href": "w03/slides.html#credit-default",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Credit Default",
    "text": "Credit Default\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ndefault_df &lt;- read_csv(\"assets/Default.csv\") |&gt;\n  mutate(default_num = ifelse(default==\"Yes\",1,0))\ndefault_plot &lt;- default_df |&gt; ggplot(aes(x=balance, y=income, color=default, shape=default)) +\n  geom_point(alpha=0.6) +\n  theme_classic(base_size=16) +\n  labs(\n    title=\"Credit Defaults by Income and Account Balance\",\n    x = \"Account Balance\",\n    y = \"Income\"\n  )\ndefault_mplot &lt;- default_plot |&gt; ggMarginal(type=\"boxplot\", groupColour=FALSE, groupFill=TRUE)\ndefault_mplot"
  },
  {
    "objectID": "w03/slides.html#lines-vs.-sigmoids",
    "href": "w03/slides.html#lines-vs.-sigmoids",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Lines vs.¬†Sigmoids(!)",
    "text": "Lines vs.¬†Sigmoids(!)\n\n\nHere‚Äôs what lines look like for this dataset:\n\n\nCode\n#lpm_model &lt;- lm(default ~ balance, data=default_df)\ndefault_df |&gt; ggplot(\n    aes(\n      x=balance, y=default_num\n    )\n  ) +\n  geom_point(aes(color=default)) +\n  stat_smooth(method=\"lm\", formula=y~x, color='black') +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs what sigmoids look like:\n\n\nCode\nlibrary(tidyverse)\nlogistic_model &lt;- glm(default_num ~ balance, family=binomial(link='logit'),data=default_df)\ndefault_df$predictions &lt;- predict(logistic_model, newdata = default_df, type = \"response\")\nmy_sigmoid &lt;- function(x) 1 / (1+exp(-x))\ndefault_df |&gt; ggplot(aes(x=balance, y=default_num)) +\n  #stat_function(fun=my_sigmoid) +\n  geom_point(aes(color=default)) +\n  geom_line(\n    data=default_df,\n    aes(x=balance, y=predictions),\n    linewidth=1\n  ) +\n  theme_classic(base_size=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\Pr(Y \\mid X) = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\n\\[\n\\log\\underbrace{\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\mathclap{\\smash{\\text{Odds Ratio}}}} = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]"
  },
  {
    "objectID": "w03/slides.html#but-lets-derive-this",
    "href": "w03/slides.html#but-lets-derive-this",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "But Let‚Äôs Derive This!",
    "text": "But Let‚Äôs Derive This!\n\\[\n\\begin{align*}\n\\Pr(Y \\mid X) &= \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\\\\n\\iff \\underbrace{\\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)}}_{\\text{Odds Ratio}} &= e^{\\beta_0 + \\beta_1X} \\\\\n\\iff \\underbrace{\\log\\left[ \\frac{\\Pr(Y \\mid X)}{1 - \\Pr(Y \\mid X)} \\right]}_{\\text{Log-Odds Ratio}} &= \\beta_0 + \\beta_1X\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#now-how-will-we-ever-find-good-parameter-values",
    "href": "w03/slides.html#now-how-will-we-ever-find-good-parameter-values",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?",
    "text": "Now How Will We Ever Find ‚ÄúGood‚Äù Parameter Values?\n\nIf only we had some sort of estimator‚Ä¶ One that would choose \\(\\beta_0\\) and \\(\\beta_1\\) so as to maximize the likelihood of seeing some data‚Ä¶\n\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{\\{i \\mid y_i = 1\\}}\\Pr(Y = 1 \\mid X) \\prod_{\\{i \\mid y_i = 0\\}}(1-\\Pr(Y = 1 \\mid X))\n\\]\n\n(Much more on this later üò∏)"
  },
  {
    "objectID": "w03/slides.html#the-interpretation-problem",
    "href": "w03/slides.html#the-interpretation-problem",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "The Interpretation Problem",
    "text": "The Interpretation Problem\n\n\nCode\noptions(scipen = 999)\nprint(summary(logistic_model))\n\n\n\nCall:\nglm(formula = default_num ~ balance, family = binomial(link = \"logit\"), \n    data = default_df)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nSlope is no longer the same everywhere! It varies across different values of \\(x\\)‚Ä¶\nLet‚Äôs brainstorm some possible ways to make our lives easier when interpreting these coefficients!"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "w03/slides.html#appendix-mlr-in-r",
    "href": "w03/slides.html#appendix-mlr-in-r",
    "title": "Week 3: Getting Fancy with Regression",
    "section": "Appendix: MLR in R",
    "text": "Appendix: MLR in R\n\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\nmlr_model &lt;- lm(\n  sales ~ TV + radio + newspaper,\n  data=ad_df\n)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422 &lt;0.0000000000000002 ***\nTV           0.045765   0.001395  32.809 &lt;0.0000000000000002 ***\nradio        0.188530   0.008611  21.893 &lt;0.0000000000000002 ***\nnewspaper   -0.001037   0.005871  -0.177                0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nHolding radio and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant‚Ä¶\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#reminder-w04-new-goal-generalizability",
    "href": "w05/index.html#reminder-w04-new-goal-generalizability",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n\n\n The Goal of Statistical Learning\n\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#our-working-dgp",
    "href": "w05/index.html#our-working-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Our Working DGP",
    "text": "Our Working DGP\n\nEach country \\(i\\) has a certain \\(x_i = \\texttt{gdp\\_per\\_capita}_i\\)\nThey spend some portion of it on healthcare each year, which translates (based on the country‚Äôs healthcare system) into health outcomes \\(y_i\\)\nWe operationalize these health outcomes as \\(y_i = \\texttt{DALY}_i\\): Disability Adjusted Life Years, cross-nationally-standardized ‚Äúlost years of minimally-healthy life‚Äù\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(plotly) |&gt; suppressPackageStartupMessages()\ndaly_df &lt;- read_csv(\"assets/dalys_cleaned.csv\")\n\n\nRows: 162 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): name, gdp_cap\ndbl (4): dalys_pc, population, gdp_pc_clean, log_dalys_pc\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndaly_df &lt;- daly_df |&gt; mutate(\n  gdp_pc_1k=gdp_pc_clean/1000\n)\nmodel_labels &lt;- labs(\n  x=\"GDP per capita ($1K PPP, 2021)\",\n  y=\"Log(DALYs/n)\",\n  title=\"Decrease in DALYs as GDP/n Increases\"\n)\ndaly_plot &lt;- daly_df |&gt; ggplot(aes(x=gdp_pc_1k, y=log_dalys_pc, label=name)) +\n  geom_point() +\n  # geom_smooth(method=\"loess\", formula=y ~ x) +\n  geom_smooth(method=\"lm\", formula=y ~ poly(x,5), se=FALSE) +\n  theme_dsan(base_size=14) +\n  model_labels\nggplotly(daly_plot)\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\leadsto Y = &10.58 - 0.2346 X + 0.01396 X^2 \\\\\n&- 0.0004 X^3 + 0.000005 X^4 \\\\\n&- 0.00000002 X^5 + \\varepsilon\n\\end{align*}\n\\]\n\n\nCode\neval_fitted_poly &lt;- function(x) {\n  coefs &lt;- c(\n    10.58,  -0.2346, 0.01396,\n    -0.0004156, 0.0000053527, -0.0000000244\n  )\n  x_terms &lt;- c(x^0, x^1, x^2, x^3, x^4, x^5)\n  dot_prod &lt;- sum(coefs * x_terms)\n  return(dot_prod)\n}\nN &lt;- 500\nx_vals &lt;- runif(N, min=0, max=90)\ny_vals &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\nsim_df &lt;- tibble(gdpc=x_vals, ldalys=y_vals)\nggplot() +\n  geom_line(data=sim_df, aes(x=gdpc, y=ldalys)) +\n  geom_point(data=daly_df, aes(x=gdp_pc_1k, y=log_dalys_pc)) +\n  theme_dsan() +\n  model_labels",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-true-model",
    "href": "w05/index.html#the-true-model",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The ‚ÄúTrue‚Äù Model",
    "text": "The ‚ÄúTrue‚Äù Model\n\nFrom here onwards, we adopt this as our ‚Äútrue‚Äù model, for pedagogical purposes!\nMeaning: we use this model to get a sense for how‚Ä¶\n\nCV can ‚Äúforesee‚Äù test error \\(\\leadsto\\) confidence in CV\nRegularization can penalize overly-complex models \\(\\leadsto\\) confidence in LASSO\n\nIn the real world we don‚Äôt know the DGP!\n\n\\(\\implies\\) We build our confidence here, then take off the training wheels irl: use CV/Regularization in hopes they can help us ‚Äúuncover‚Äù the unknown DGP\n\n\n\n\nCode\nrun_dgp &lt;- function(world_label=\"Sim\", N=60, x_max=90) {\n  x_vals &lt;- runif(N, min=0, max=x_max)\n  y_raw &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\n  y_noise &lt;- rnorm(N, mean=0, sd=0.8)\n  y_vals &lt;- y_raw + y_noise\n  sim_df &lt;- tibble(\n    gdpc=x_vals,\n    ldalys=y_vals,\n    world=world_label\n  )\n  return(sim_df)\n}\ndf1 &lt;- run_dgp(\"World 1\")\ndf2 &lt;- run_dgp(\"World 2\")\ndf3 &lt;- run_dgp(\"World 3\")\ndgp_df &lt;- bind_rows(df1, df2, df3)\ndgp_df |&gt; ggplot(aes(x=gdpc, y=ldalys)) +\n  geom_point(aes(color=world)) +\n  facet_wrap(vars(world)) +\n  theme_dsan(base_size=22) +\n  remove_legend() +\n  model_labels +\n  labs(title=\"Three Possible Realizations of our DGP\")",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#training-vs.-test-data",
    "href": "w05/index.html#training-vs.-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\nWe introduced this as a first step towards tackling the scourge of overfitting!\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%\n\n\n\n\n\n\n\n\n\nTraining Set \\(\\leadsto\\) Training Error, Test Set \\(\\leadsto\\) Test Error\nSo‚Ä¶ what‚Äôs the issue? Why do we need to complicate this picture?",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-chilling-truth-behind-test-data",
    "href": "w05/index.html#the-chilling-truth-behind-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Chilling Truth Behind Test Data ü´£",
    "text": "The Chilling Truth Behind Test Data ü´£\n\nScience-wise, technically, once you use the test set, you should stop working\nFull gory details in fancy books (Hume (1760) \\(\\rightarrow\\) Popper (1934)), but the essence is captured by visualizing scientific inference (and statistical learning!) like:\n\n\n\n\n\n\n\nSo, what do we do? Use \\(\\mathbf{D}_{\\text{Tr}}\\) along with knowledge of issues like overfitting to estimate test error!\nFulfills our goal: find model which best predicts \\(Y\\) from \\(X\\) for unobserved data",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#the-validation-set-approach",
    "href": "w05/index.html#the-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Validation Set Approach",
    "text": "The Validation Set Approach\n\n‚ö†Ô∏è Remember: under our new goal, ‚Äúgood‚Äù models = models that generalize well!\nOptimizing over test set violates scientific inference axioms\nInstead, optimize over training data by ‚Äúholding out‚Äù some portion of it‚Ä¶ Enter the Validation Set:\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#evaluating-one-model-validation-set-approach",
    "href": "w05/index.html#evaluating-one-model-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Evaluating One Model: Validation Set Approach",
    "text": "Evaluating One Model: Validation Set Approach\n\n Randomly pick 20% of \\(\\mathbf{D}_{\\text{train}}\\) as sub-training set \\(\\mathbf{D}_{\\text{SubTr}}\\)\n The other 80% becomes validation set \\(\\mathbf{D}_{\\text{Val}}\\)\n Train model on \\(\\mathbf{D}_{\\text{SubTr}}\\), then evaluate using \\(\\mathbf{D}_{\\text{Val}}\\), to produce validation error \\(\\boxed{\\varepsilon_{\\text{Val}} = \\widehat{\\text{Err}}_{\\text{Test}}}\\)\n\\(\\varepsilon_{\\text{Val}}\\) gives us an estimate of the test error\n\n\\(\\implies\\) this is what we want to optimize, in place of training error, for our new goal üòé!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#how-does-it-do-for-our-dgp",
    "href": "w05/index.html#how-does-it-do-for-our-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "How Does It Do for Our DGP?",
    "text": "How Does It Do for Our DGP?\n\nRecall that the ‚Äútrue‚Äù degree is 5, but that you‚Äôre not supposed to know that!\n\n\n\n\n\nCode\nlibrary(boot)\nset.seed(5300)\nsim200_df &lt;- run_dgp(\n  world_label=\"N=200\", N=200, x_max=100\n)\nsim1k_df &lt;- run_dgp(\n  world_label=\"N=1000\", N=1000, x_max=100\n)\ncompute_deltas &lt;- function(df, min_deg=1, max_deg=12) {\n  cv_deltas &lt;- c()\n  for (i in min_deg:max_deg) {\n    cur_poly &lt;- glm(ldalys ~ poly(gdpc, i), data=df)\n    cur_poly_cv_result &lt;- cv.glm(data=df, glmfit=cur_poly, K=5)\n    cur_cv_adj &lt;- cur_poly_cv_result$delta[1]\n    cv_deltas &lt;- c(cv_deltas, cur_cv_adj)\n  }\n  return(cv_deltas)\n}\nsim200_deltas &lt;- compute_deltas(sim200_df)\nsim200_delta_df &lt;- tibble(degree=1:12, delta=sim200_deltas)\nsim200_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 200\")\n\n\n\n\n\nHere, validation error fails to capture ‚Äòtrue‚Äô model (likely culprits: low \\(N\\), high noise‚Ä¶)\n\n\n\n\n\nPossible resolution: [See coming slides!]\n\n\n\n\nCode\nsim1k_deltas &lt;- compute_deltas(sim1k_df)\nsim1k_delta_df &lt;- tibble(degree=1:12, delta=sim1k_deltas)\nsim1k_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 1000\")\n\n\n\n\n\nHere, validation error fails to sharply distinguish \\(d \\in \\{5, 6, 7, 8\\}\\)\n\n\n\n\n\nPossible resolution: ‚Äúone standard error rule‚Äù",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#optimizing-over-many-models",
    "href": "w05/index.html#optimizing-over-many-models",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Optimizing over Many Models",
    "text": "Optimizing over Many Models\n\n Let \\(\\mathfrak{M} = (\\mathcal{M}_1, \\ldots, \\mathcal{M}_D)\\) be a set of \\(D\\) different models\n\nEx: \\(\\mathcal{M}_1\\) could be a linear model, \\(\\mathcal{M}_2\\) a quadratic model, \\(\\mathcal{M}_3\\) a cubic model, and so on‚Ä¶\n\n For each \\(\\mathcal{M}_i \\in \\mathfrak{M}\\) (and for given training data \\(\\mathbf{D}_{\\text{Tr}}\\)):\n\nUse [Insert Validation Approach] to derive \\(\\varepsilon_i\\)\n\n Model \\(\\mathcal{M}_i\\) with lowest \\(\\varepsilon_i\\) wins!\nWe are now cooking with gas! We can use CV to optimize over e.g.¬†HYPERPARAMETERS ü§Ø (that we had to just kind of‚Ä¶ guess before)!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#cooking-with-even-more-gas",
    "href": "w05/index.html#cooking-with-even-more-gas",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Cooking with Even More Gas!",
    "text": "Cooking with Even More Gas!\n(Âä† even more Ê≤π!)\n\nIt turns out that, for the purposes of estimating test error (aka, estimating how well the fitted model will generalize), Validation Set Approach is the worst approach (still good, just, least good!)\nTo see its limitations, consider: is there something special about the 20% of data we selected for the validation set?\nAnswer: No! It was literally randomly selected!\n\\(\\implies\\) Any other 20% ‚Äúchunk‚Äù (which we‚Äôll call a fold from now on) could work just as well",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#fold-cross-validation",
    "href": "w05/index.html#fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "5-Fold Cross-Validation",
    "text": "5-Fold Cross-Validation\n\nSecretly, by choosing a 20% fold to be the validation set earlier, I was priming you for 5-fold cross-validation!\n\n\n\n\nImage Source",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#k-fold-cross-validation",
    "href": "w05/index.html#k-fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "\\(K\\)-Fold Cross-Validation",
    "text": "\\(K\\)-Fold Cross-Validation\n\n5 was an arbitrary choice!\n\n(Though, one with important and desirable statistical properties, which we‚Äôll get to ASAP)\n\nIn general, \\(K\\)-Fold CV Estimator \\(\\varepsilon_{(K)} = \\widehat{\\text{Err}}_{\\text{Test}}\\) given by\n\n\\[\n\\varepsilon_{(K)} = \\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{\\text{Val}}_{i}\n\\]",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#leave-one-out-cross-validation-loocv",
    "href": "w05/index.html#leave-one-out-cross-validation-loocv",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\\(K\\)-Fold CV with \\(K := N\\)\nProduces the maximum possible number of terms in the summation that leads to \\(\\varepsilon_{(K)}\\)\n\ni.e., \\(\\varepsilon_{(N)}\\) is an average over the maximum possible number of models\n\nSo‚Ä¶ does this mean it‚Äôs the best choice?\nHint: How ‚Äúdifferent‚Äù are any two models used in the sum?\n\n\\[\n\\begin{align*}\n\\varepsilon_{(N)} =~ & \\text{Err}(\\mathbf{D}_1 \\mid \\mathbf{D}_{2:100}) + \\text{Err}(\\mathbf{D}_2 \\mid \\mathbf{D}_1, \\mathbf{D}_{3:100}) \\\\\n&+ \\text{Err}(\\mathbf{D}_3 \\mid \\mathbf{D}_{1:2}, \\mathbf{D}_{4:100}) + \\cdots + \\text{Err}(\\mathbf{D}_{100} \\mid \\mathbf{D}_{1:99})\n\\end{align*}\n\\]\n\nThe terms here are highly correlated!\nFun math results around how \\(K = 5\\) or \\(K = 10\\) can best strike a balance between overly-correlated terms (\\(K = N\\)) and not averaging enough models (\\(K = 2\\))!",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "References",
    "text": "References\n\n\nHume, David. 1760. An Enquiry Concerning Human Understanding. Simon and Schuster.\n\n\nPopper, Karl R. 1934. The Logic of Scientific Discovery. Psychology Press.",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/slides.html#reminder-w04-new-goal-generalizability",
    "href": "w05/slides.html#reminder-w04-new-goal-generalizability",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "[Reminder (W04)] New Goal: Generalizability",
    "text": "[Reminder (W04)] New Goal: Generalizability\n\n\n\n\n The Goal of Statistical Learning\n\n\nFind‚Ä¶\n\nA function \\(\\widehat{y} = f(x)\\) ‚úÖ\nThat best predicts \\(Y\\) for given values of \\(X\\) ‚úÖ\nFor data that has not yet been observed! üò≥‚ùì"
  },
  {
    "objectID": "w05/slides.html#our-working-dgp",
    "href": "w05/slides.html#our-working-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Our Working DGP",
    "text": "Our Working DGP\n\nEach country \\(i\\) has a certain \\(x_i = \\texttt{gdp\\_per\\_capita}_i\\)\nThey spend some portion of it on healthcare each year, which translates (based on the country‚Äôs healthcare system) into health outcomes \\(y_i\\)\nWe operationalize these health outcomes as \\(y_i = \\texttt{DALY}_i\\): Disability Adjusted Life Years, cross-nationally-standardized ‚Äúlost years of minimally-healthy life‚Äù\n\n\n\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(plotly) |&gt; suppressPackageStartupMessages()\ndaly_df &lt;- read_csv(\"assets/dalys_cleaned.csv\")\ndaly_df &lt;- daly_df |&gt; mutate(\n  gdp_pc_1k=gdp_pc_clean/1000\n)\nmodel_labels &lt;- labs(\n  x=\"GDP per capita ($1K PPP, 2021)\",\n  y=\"Log(DALYs/n)\",\n  title=\"Decrease in DALYs as GDP/n Increases\"\n)\ndaly_plot &lt;- daly_df |&gt; ggplot(aes(x=gdp_pc_1k, y=log_dalys_pc, label=name)) +\n  geom_point() +\n  # geom_smooth(method=\"loess\", formula=y ~ x) +\n  geom_smooth(method=\"lm\", formula=y ~ poly(x,5), se=FALSE) +\n  theme_dsan(base_size=14) +\n  model_labels\nggplotly(daly_plot)\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\leadsto Y = &10.58 - 0.2346 X + 0.01396 X^2 \\\\\n&- 0.0004 X^3 + 0.000005 X^4 \\\\\n&- 0.00000002 X^5 + \\varepsilon\n\\end{align*}\n\\]\n\n\nCode\neval_fitted_poly &lt;- function(x) {\n  coefs &lt;- c(\n    10.58,  -0.2346, 0.01396,\n    -0.0004156, 0.0000053527, -0.0000000244\n  )\n  x_terms &lt;- c(x^0, x^1, x^2, x^3, x^4, x^5)\n  dot_prod &lt;- sum(coefs * x_terms)\n  return(dot_prod)\n}\nN &lt;- 500\nx_vals &lt;- runif(N, min=0, max=90)\ny_vals &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\nsim_df &lt;- tibble(gdpc=x_vals, ldalys=y_vals)\nggplot() +\n  geom_line(data=sim_df, aes(x=gdpc, y=ldalys)) +\n  geom_point(data=daly_df, aes(x=gdp_pc_1k, y=log_dalys_pc)) +\n  theme_dsan() +\n  model_labels"
  },
  {
    "objectID": "w05/slides.html#the-true-model",
    "href": "w05/slides.html#the-true-model",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The ‚ÄúTrue‚Äù Model",
    "text": "The ‚ÄúTrue‚Äù Model\n\nFrom here onwards, we adopt this as our ‚Äútrue‚Äù model, for pedagogical purposes!\nMeaning: we use this model to get a sense for how‚Ä¶\n\nCV can ‚Äúforesee‚Äù test error \\(\\leadsto\\) confidence in CV\nRegularization can penalize overly-complex models \\(\\leadsto\\) confidence in LASSO\n\nIn the real world we don‚Äôt know the DGP!\n\n\\(\\implies\\) We build our confidence here, then take off the training wheels irl: use CV/Regularization in hopes they can help us ‚Äúuncover‚Äù the unknown DGP\n\n\n\n\nCode\nrun_dgp &lt;- function(world_label=\"Sim\", N=60, x_max=90) {\n  x_vals &lt;- runif(N, min=0, max=x_max)\n  y_raw &lt;- sapply(X=x_vals, FUN=eval_fitted_poly)\n  y_noise &lt;- rnorm(N, mean=0, sd=0.8)\n  y_vals &lt;- y_raw + y_noise\n  sim_df &lt;- tibble(\n    gdpc=x_vals,\n    ldalys=y_vals,\n    world=world_label\n  )\n  return(sim_df)\n}\ndf1 &lt;- run_dgp(\"World 1\")\ndf2 &lt;- run_dgp(\"World 2\")\ndf3 &lt;- run_dgp(\"World 3\")\ndgp_df &lt;- bind_rows(df1, df2, df3)\ndgp_df |&gt; ggplot(aes(x=gdpc, y=ldalys)) +\n  geom_point(aes(color=world)) +\n  facet_wrap(vars(world)) +\n  theme_dsan(base_size=22) +\n  remove_legend() +\n  model_labels +\n  labs(title=\"Three Possible Realizations of our DGP\")"
  },
  {
    "objectID": "w05/slides.html#training-vs.-test-data",
    "href": "w05/slides.html#training-vs.-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Training vs.¬†Test Data",
    "text": "Training vs.¬†Test Data\n\nWe introduced this as a first step towards tackling the scourge of overfitting!\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%\n\n\n\n\n\n\n\n\n\nTraining Set \\(\\leadsto\\) Training Error, Test Set \\(\\leadsto\\) Test Error\nSo‚Ä¶ what‚Äôs the issue? Why do we need to complicate this picture?"
  },
  {
    "objectID": "w05/slides.html#the-chilling-truth-behind-test-data",
    "href": "w05/slides.html#the-chilling-truth-behind-test-data",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Chilling Truth Behind Test Data ü´£",
    "text": "The Chilling Truth Behind Test Data ü´£\n\nScience-wise, technically, once you use the test set, you should stop working\nFull gory details in fancy books (Hume (1760) \\(\\rightarrow\\) Popper (1934)), but the essence is captured by visualizing scientific inference (and statistical learning!) like:\n\n\n\nSo, what do we do? Use \\(\\mathbf{D}_{\\text{Tr}}\\) along with knowledge of issues like overfitting to estimate test error!\nFulfills our goal: find model which best predicts \\(Y\\) from \\(X\\) for unobserved data"
  },
  {
    "objectID": "w05/slides.html#the-validation-set-approach",
    "href": "w05/slides.html#the-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "The Validation Set Approach",
    "text": "The Validation Set Approach\n\n‚ö†Ô∏è Remember: under our new goal, ‚Äúgood‚Äù models = models that generalize well!\nOptimizing over test set violates scientific inference axioms\nInstead, optimize over training data by ‚Äúholding out‚Äù some portion of it‚Ä¶ Enter the Validation Set:\n\n\n\nCode\ngraph grid\n{\n    graph [\n        overlap=true,\n        scale=0.2\n    ]\n    nodesep=0.0\n    ranksep=0.0\n    rankdir=\"LR\"\n    scale=0.2\n    node [\n        style=\"filled\",\n        color=black,\n        fillcolor=lightblue,\n        shape=box\n    ]\n\n    // uncomment to hide the grid\n    edge [style=invis]\n    \n    subgraph cluster_01 {\n        label=\"Training Set (80%)\"\n        subgraph cluster_02 {\n            label=\"Training Fold (80%)\"\n            A1[label=\"16%\"] A2[label=\"16%\"] A3[label=\"16%\"] A4[label=\"16%\"]\n        }\n        subgraph cluster_03 {\n            label=\"Validation Fold (20%)\"\n            B1[label=\"16%\",fillcolor=lightgreen]\n        }\n    }\n    subgraph cluster_04 {\n        label=\"Test Set (20%)\"\n    C1[label=\"20%\",fillcolor=orange]\n    }\n    A1 -- A2 -- A3 -- A4 -- B1 -- C1;\n}\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "w05/slides.html#evaluating-one-model-validation-set-approach",
    "href": "w05/slides.html#evaluating-one-model-validation-set-approach",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Evaluating One Model: Validation Set Approach",
    "text": "Evaluating One Model: Validation Set Approach\n\n Randomly pick 20% of \\(\\mathbf{D}_{\\text{train}}\\) as sub-training set \\(\\mathbf{D}_{\\text{SubTr}}\\)\n The other 80% becomes validation set \\(\\mathbf{D}_{\\text{Val}}\\)\n Train model on \\(\\mathbf{D}_{\\text{SubTr}}\\), then evaluate using \\(\\mathbf{D}_{\\text{Val}}\\), to produce validation error \\(\\boxed{\\varepsilon_{\\text{Val}} = \\widehat{\\text{Err}}_{\\text{Test}}}\\)\n\\(\\varepsilon_{\\text{Val}}\\) gives us an estimate of the test error\n\n\\(\\implies\\) this is what we want to optimize, in place of training error, for our new goal üòé!"
  },
  {
    "objectID": "w05/slides.html#how-does-it-do-for-our-dgp",
    "href": "w05/slides.html#how-does-it-do-for-our-dgp",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "How Does It Do for Our DGP?",
    "text": "How Does It Do for Our DGP?\n\nRecall that the ‚Äútrue‚Äù degree is 5, but that you‚Äôre not supposed to know that!\n\n\n\n\n\nCode\nlibrary(boot)\nset.seed(5300)\nsim200_df &lt;- run_dgp(\n  world_label=\"N=200\", N=200, x_max=100\n)\nsim1k_df &lt;- run_dgp(\n  world_label=\"N=1000\", N=1000, x_max=100\n)\ncompute_deltas &lt;- function(df, min_deg=1, max_deg=12) {\n  cv_deltas &lt;- c()\n  for (i in min_deg:max_deg) {\n    cur_poly &lt;- glm(ldalys ~ poly(gdpc, i), data=df)\n    cur_poly_cv_result &lt;- cv.glm(data=df, glmfit=cur_poly, K=5)\n    cur_cv_adj &lt;- cur_poly_cv_result$delta[1]\n    cv_deltas &lt;- c(cv_deltas, cur_cv_adj)\n  }\n  return(cv_deltas)\n}\nsim200_deltas &lt;- compute_deltas(sim200_df)\nsim200_delta_df &lt;- tibble(degree=1:12, delta=sim200_deltas)\nsim200_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 200\")\n\n\n\n\n\nHere, validation error fails to capture ‚Äòtrue‚Äô model (likely culprits: low \\(N\\), high noise‚Ä¶)\n\n\n\n\n\nPossible resolution: [See coming slides!]\n\n\n\n\nCode\nsim1k_deltas &lt;- compute_deltas(sim1k_df)\nsim1k_delta_df &lt;- tibble(degree=1:12, delta=sim1k_deltas)\nsim1k_delta_df |&gt; ggplot(aes(x=degree, y=delta)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept=5, linetype=\"dashed\") +\n  scale_x_continuous(\n    breaks=seq(from=1,to=12,by=1)\n  ) +\n  theme_dsan(base_size=22) +\n  labs(title=\"N = 1000\")\n\n\n\n\n\nHere, validation error fails to sharply distinguish \\(d \\in \\{5, 6, 7, 8\\}\\)\n\n\n\n\n\nPossible resolution: ‚Äúone standard error rule‚Äù"
  },
  {
    "objectID": "w05/slides.html#optimizing-over-many-models",
    "href": "w05/slides.html#optimizing-over-many-models",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Optimizing over Many Models",
    "text": "Optimizing over Many Models\n\n Let \\(\\mathfrak{M} = (\\mathcal{M}_1, \\ldots, \\mathcal{M}_D)\\) be a set of \\(D\\) different models\n\nEx: \\(\\mathcal{M}_1\\) could be a linear model, \\(\\mathcal{M}_2\\) a quadratic model, \\(\\mathcal{M}_3\\) a cubic model, and so on‚Ä¶\n\n For each \\(\\mathcal{M}_i \\in \\mathfrak{M}\\) (and for given training data \\(\\mathbf{D}_{\\text{Tr}}\\)):\n\nUse [Insert Validation Approach] to derive \\(\\varepsilon_i\\)\n\n Model \\(\\mathcal{M}_i\\) with lowest \\(\\varepsilon_i\\) wins!\nWe are now cooking with gas! We can use CV to optimize over e.g.¬†HYPERPARAMETERS ü§Ø (that we had to just kind of‚Ä¶ guess before)!"
  },
  {
    "objectID": "w05/slides.html#cooking-with-even-more-gas",
    "href": "w05/slides.html#cooking-with-even-more-gas",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Cooking with Even More Gas!",
    "text": "Cooking with Even More Gas!\n(Âä† even more Ê≤π!)\n\nIt turns out that, for the purposes of estimating test error (aka, estimating how well the fitted model will generalize), Validation Set Approach is the worst approach (still good, just, least good!)\nTo see its limitations, consider: is there something special about the 20% of data we selected for the validation set?\nAnswer: No! It was literally randomly selected!\n\\(\\implies\\) Any other 20% ‚Äúchunk‚Äù (which we‚Äôll call a fold from now on) could work just as well"
  },
  {
    "objectID": "w05/slides.html#fold-cross-validation",
    "href": "w05/slides.html#fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "5-Fold Cross-Validation",
    "text": "5-Fold Cross-Validation\n\nSecretly, by choosing a 20% fold to be the validation set earlier, I was priming you for 5-fold cross-validation!\n\n\nImage Source"
  },
  {
    "objectID": "w05/slides.html#k-fold-cross-validation",
    "href": "w05/slides.html#k-fold-cross-validation",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "\\(K\\)-Fold Cross-Validation",
    "text": "\\(K\\)-Fold Cross-Validation\n\n5 was an arbitrary choice!\n\n(Though, one with important and desirable statistical properties, which we‚Äôll get to ASAP)\n\nIn general, \\(K\\)-Fold CV Estimator \\(\\varepsilon_{(K)} = \\widehat{\\text{Err}}_{\\text{Test}}\\) given by\n\n\\[\n\\varepsilon_{(K)} = \\frac{1}{K}\\sum_{i=1}^{K}\\varepsilon^{\\text{Val}}_{i}\n\\]"
  },
  {
    "objectID": "w05/slides.html#leave-one-out-cross-validation-loocv",
    "href": "w05/slides.html#leave-one-out-cross-validation-loocv",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\\(K\\)-Fold CV with \\(K := N\\)\nProduces the maximum possible number of terms in the summation that leads to \\(\\varepsilon_{(K)}\\)\n\ni.e., \\(\\varepsilon_{(N)}\\) is an average over the maximum possible number of models\n\nSo‚Ä¶ does this mean it‚Äôs the best choice?\nHint: How ‚Äúdifferent‚Äù are any two models used in the sum?\n\n\\[\n\\begin{align*}\n\\varepsilon_{(N)} =~ & \\text{Err}(\\mathbf{D}_1 \\mid \\mathbf{D}_{2:100}) + \\text{Err}(\\mathbf{D}_2 \\mid \\mathbf{D}_1, \\mathbf{D}_{3:100}) \\\\\n&+ \\text{Err}(\\mathbf{D}_3 \\mid \\mathbf{D}_{1:2}, \\mathbf{D}_{4:100}) + \\cdots + \\text{Err}(\\mathbf{D}_{100} \\mid \\mathbf{D}_{1:99})\n\\end{align*}\n\\]\n\nThe terms here are highly correlated!\nFun math results around how \\(K = 5\\) or \\(K = 10\\) can best strike a balance between overly-correlated terms (\\(K = N\\)) and not averaging enough models (\\(K = 2\\))!"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Cross-Validation for Model Assessment",
    "section": "References",
    "text": "References\n\n\nHume, David. 1760. An Enquiry Concerning Human Understanding. Simon and Schuster.\n\n\nPopper, Karl R. 1934. The Logic of Scientific Discovery. Psychology Press."
  },
  {
    "objectID": "writeups/quiz-1/index.html",
    "href": "writeups/quiz-1/index.html",
    "title": "Quiz 1 Study Guide",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nPractice problems added 2 February 2025, 8:30pm\nFirst published 26 January 2025, 3:30pm"
  },
  {
    "objectID": "writeups/quiz-1/index.html#overview",
    "href": "writeups/quiz-1/index.html#overview",
    "title": "Quiz 1 Study Guide",
    "section": "Overview",
    "text": "Overview\nHello DSAN 5300 Section 01 friends! First, please take a look at the course-wide study guide that was sent out over email by Prof.¬†James! This guide just contains my additional summarization and some practice problems that I hope might link the concepts to stuff I‚Äôve mentioned specifically in Section 01.\nThe key topics that the Quiz will cover can be organized into the following four subheadings:\n\n(1) ‚ÄúParametric‚Äù Modeling\nWhat does it mean to have a ‚Äúparametric‚Äù model? For example, in a model such as regression, which we write as\n\\[\nY = \\beta_0 + \\beta1_ X + \\varepsilon,\n\\]\nwhich of the things in that equation are parameters of the model and which are not parameters (for example, which are just variables that we plug data into)?\n\n\n(2) Optimization in General\nOnce we‚Äôve identified the parameters in a model, how do we evaluate how ‚Äúgood‚Äù or ‚Äúbad‚Äù a certain setting for the parameters is? (The answer being, a loss function)\n\n\n(3) Gradient Descent\nOnce we have a loss function, how does the gradient allow us to choose a random value for the parameter and then ‚Äúmake our way‚Äù towards the optimal value? The answer to this question is the main content in this previous writeup\nAs a refresher, a gradient is just the vector equivalent of a derivative. For example, in calculus we learn how\n\\[\nf(x) = x^2\n\\]\nhas a derivative\n\\[\n\\frac{\\partial f}{\\partial x} = 2x.\n\\]\nSo in this class, if \\(\\mathbf{x}\\) is now a vector like \\(\\mathbf{x} = (x_1,x_2)\\) instead of just a single number, the gradient or vector-valued derivative of \\(f\\) with respect to \\(\\mathbf{x}\\), \\(\\nabla_{\\mathbf{x}} f\\), is\n\\[\n\\nabla_{\\mathbf{x}} f = \\frac{\\partial f}{\\partial \\mathbf{x}} = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right)\n\\]\nIf it helps, try to notice/keep in mind how the \\(\\mathbf{x}\\) in \\(\\frac{\\partial f}{\\partial \\mathbf{x}}\\) is a vector, whereas the \\(x_1\\) in \\(\\frac{\\partial f}{\\partial x_1}\\) is a scalar. In other words, although the first and second terms in this expression may look scary, each entry within the parentheses on the RHS of that equality (the third term) is just the ‚Äúregular‚Äù univariate derivative that you learn in calculus class!\n\n\n(4) More Efficient Optimization Methods\nHere the idea (or, the way I see these ‚Äúfancier‚Äù methods, at least) is, they use additional information about the loss function above and beyond just \\(L(x)\\) and its derivative \\(L'(x)\\).\nSo, Newton‚Äôs method for example uses the second derivative \\(L''(x)\\) as an additional piece of information about the curvature of the loss function, whereas the secant method is slower than Newton‚Äôs method but doesn‚Äôt require us to know this second derivative \\(L''(x)\\) (since it approximates it)."
  },
  {
    "objectID": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "href": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "title": "Quiz 1 Study Guide",
    "section": "A Full-On Lecture Replacement",
    "text": "A Full-On Lecture Replacement\nTo try and fully ‚Äúfill in‚Äù the missing week here, I can just give you all the resource that is literally a recording of the class I learned this stuff from, and that I later TAed. That way if you have additional questions I‚Äôll be able to refer specifically to the examples/materials that Prof.¬†Ng uses in the following video:"
  },
  {
    "objectID": "writeups/quiz-1/index.html#practice-problems",
    "href": "writeups/quiz-1/index.html#practice-problems",
    "title": "Quiz 1 Study Guide",
    "section": "Practice Problems",
    "text": "Practice Problems\n\nProblem 1: Handling Binary Features\nA team of data scientists working for the Los Angeles Lakers NBA team has developed a basic single linear regression model relating the number of minutes played by LeBron James \\(M\\) to their team‚Äôs total points \\(T\\) in a game:\n\\[\nT = \\beta_0 + \\beta_1 M\n\\]\nThey then estimated \\(\\beta_0\\) and \\(\\beta_1\\) from data on the past two seasons, arriving at estimates\n\\[\n\\widehat{\\beta}_0 = 80 \\text{ and }\\widehat{\\beta}_1 = 0.9\n\\]\nPlotting the resulting regression estimation function \\(\\widehat{t} = 80 + 0.9 m\\) on top of the data, they therefore obtain a figure that looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\ntotal_games &lt;- 164\nminute_vals &lt;- runif(total_games, min=0, max=48)\npts_raw &lt;- 80 + 0.9 * minute_vals\npts_noise &lt;- rnorm(total_games, mean=0, sd=5)\npts_obs &lt;- pts_raw + pts_noise\nlebron_df &lt;- tibble(minutes=minute_vals, points=pts_obs)\nlebron_df |&gt; ggplot(aes(x=minutes, y=points)) +\n  geom_point() +\n  geom_smooth(method='lm', formula='y ~ x', se=FALSE) +\n  labs(\n    title=\"Team Points vs. LeBron James Minutes Played\",\n    x = \"Minutes Played by LeBron (M)\",\n    y = \"Lakers Total Points (T)\"\n  ) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\nHowever, as of this morning, a big trade has been made sending Slovenian basketball phenom Luka Doncic to the Lakers(!)\nSo, the Lakers‚Äô data science team is frantically running simulations to model the potential impact Luka will have on their team‚Äôs performance, and especially how it will affect their model plotted above, of team performance vs.¬†LeBron‚Äôs minutes played. They therefore construct a dummy variable \\(D\\), for Doncic, which has the value 1 for games with Doncic on the Lakers and 0 for games without Doncic on the Lakers.\nIn the next two questions you will consider two hypotheses regarding Luka‚Äôs potential contribution, and the appropriate model for each of the two hypotheses!\n\n\nHypothesis 1: Additive Contribution\nA member of the data science team named Addison hypothesizes that Luka‚Äôs contribution will be ‚Äúadditive‚Äù with respect to LeBron‚Äôs performance, in the sense that:\n\nThe team‚Äôs total points will now shift upwards by some amount, regardless of the number of minutes played by LeBron, but\nThe team‚Äôs total points will still increase by about 0.9 points per additional minute that LeBron plays.\n\nFor example, Doncic scored about 34 points per game last season, while the player he was traded for, Anthony Davis, scored about 25 points per game in that same season, so (by this ‚Äúback of the envelope‚Äù calculation) Addison hypothesizes that the team‚Äôs total points with Doncic may be 9 higher than its total points without Doncic, for any amount of minutes that LeBron plays.\nUsing this information,\n\nWrite out a linear regression model which would allow this hypothesis to be tested (once data from games with Luka comes in),\nUse the ‚Äúsplit into cases‚Äù method from class to specify what the prediction equation will look like when \\(D = 0\\) and when \\(D = 1\\), and then\nWrite the hypothesis explicitly in terms of the parameters (\\(\\beta_0\\), \\(\\beta_1\\), etc.) of the regression model.\n\nSolution:\nSince Addison‚Äôs hypothesis implies that the slope from the above model would remain the same, but that the intercept would increase, we do not need an interaction term between \\(M\\) and \\(T\\) in our model. Therefore, we can model Addison‚Äôs hypothesis using a simple MLR model without any interaction terms:\n\\[\nT = \\beta_0 + \\beta_1 M + \\beta_2 D\n\\]\nNext, we can split this modeling equation into cases, to see what predictions it will generate when \\(D = 1\\) separately from the predictions it will generate when \\(D = 0\\):\n\\[\nT = \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n\\beta_0 + \\beta_1 M + \\beta_2 &\\text{if }D = 1\n\\end{cases}\n\\]\nLastly, we can see from these cases that the value of \\(\\beta_2\\) specifcally is what would tell us how much the Lakers‚Äô total points shift in games with Doncic (\\(D = 1\\)) compared to games without Doncic (\\(D = 0\\)). So, Addison‚Äôs hypothesis can be written as\n\\[\n\\mathcal{H}_{\\text{Addison}}: \\beta_2 = 9\n\\]\nWith the hypothesis written out in this way, we could now test Addison‚Äôs hypothesis by collecting data, estimating \\(\\beta_2\\) from this data, and using e.g.¬†the standard error, \\(T\\)-statistic, and \\(p\\)-value information provided in the regression output!\n\n\nHypothesis 2: Interactive Contribution\nA second member of the data science team named Interacthony hypothesizes that Luka‚Äôs contribution will in fact ‚Äúboost‚Äù LeBron‚Äôs performance, in the sense that:\n\nThe team‚Äôs total points will now shift upwards by some amount, even when LeBron doesn‚Äôt play at all, but also\nDoncic and LeBron will complement each other‚Äôs play style, such that LeBron will now ‚Äúconvert‚Äù each additional minute of play into a greater number of points for the team.\n\nFor example, whereas in the model plotted above each additional minute played by LeBron translated into an additional 0.9 points for the Lakers, Interacthony now hypothesizes (in addition to Addison‚Äôs hypothesis from the previous question) that in games with Doncic (games with \\(D = 1\\)) each additional minute played by LeBron will translate into an additional 1.1 points for the Lakers.\nSo, like in the previous part, your task is to:\n\nWrite out a linear regression model which would allow Interacthony‚Äôs two hypotheses to be tested (once data from games with Luka comes in),\nUse the ‚Äúsplit into cases‚Äù method from class to specify what the prediction equation will look like when \\(D = 0\\) and when \\(D = 1\\), and then\nWrite the two hypotheses explicitly in terms of the parameters (\\(\\beta_0\\), \\(\\beta_1\\), etc.) of the regression model.\n\nSolution:\nIn this case, we need a model which will allow both the intercept and the slope of the earlier model to change based on whether or not Doncic is on the Lakers. So, we now need an interaction term, to enable both intercept and slope to vary, as we discussed in class. The base model we should use here is therefore\n\\[\nT = \\beta_0 + \\beta_1 M + \\beta_2 D + \\beta_3 (M \\times D)\n\\]\nWriting out the cases for \\(D = 0\\) and \\(D = 1\\) reveals why this interaction term is necessary to allow both intercept and slope to vary:\n\\[\n\\begin{align*}\nT &= \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n\\beta_0 + \\beta_1 M + \\beta_2 + \\beta_3 M &\\text{if }D = 1\n\\end{cases} \\\\\n&= \\begin{cases}\n\\beta_0 + \\beta_1 M &\\text{if }D = 0 \\\\\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) M &\\text{if }D = 1\n\\end{cases}\n\\end{align*}\n\\]\nAnd so we see that \\(\\beta_2\\) represents the amount by which the intercept would change and \\(\\beta_3\\) the amount by which the slope would change for datapoints with \\(D = 1\\). Thus, we can write Interacthony‚Äôs two hypotheses as:\n\\[\n\\begin{align*}\n\\mathcal{H}_{\\text{Inter}}^{1}: \\beta_2 &= 9 \\\\\n\\mathcal{H}_{\\text{Inter}}^{2}: \\beta_3 &= (1.1 - 0.9) = 0.2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html",
    "href": "writeups/hw1-guide/index.html",
    "title": "Getting Started with HW 1",
    "section": "",
    "text": "Update Log\n\n\n\n\n\n\nSection on Least Squares Derivation added 1 Feb 2025, 1:00am\nOriginal version posted 31 Jan 2025, 2:30am\nAs with the Lab 1 getting-started guide, here we don‚Äôt give away the answers but do provide as much background as possible to nudge you up to the starting line!"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#sums-leftrightarrows-linear-algebraic-operations",
    "href": "writeups/hw1-guide/index.html#sums-leftrightarrows-linear-algebraic-operations",
    "title": "Getting Started with HW 1",
    "section": "Sums \\(\\leftrightarrows\\) Linear-Algebraic Operations",
    "text": "Sums \\(\\leftrightarrows\\) Linear-Algebraic Operations\nOne key motivation for this section is for you to see how all of the regression model‚Äôs mathematical details can be represented in two equally valid ways:\n\nFirst, using a ‚Äústandard‚Äù calculus approach, where you will end up with lots of sums (\\(\\Sigma\\) symbols) in your derivations because (for example) the loss function used by regression is itself a sum of squares (specifically, a sum of squared residuals). However, there is also‚Ä¶\nSecond, using a linear algebra-heavy approach, where in place of the sums you will now have lots of vector-vector, matrix-vector, and matrix-matrix products.\n\nTo see how the second approach ‚Äúreplaces‚Äù the sums with linear-algebraic operations, as a basic example you can use to remind yourself of these two different representations (and then think about which one feels more clear to you, and then use that one), consider a scenario where you have a set of datapoints (scalars for now):\n\\[\n\\mathbf{x} = (x_1, x_2, \\ldots, x_n),\n\\]\nand a set of weights, with one weight per datapoint (so that the subscripts ‚Äúmatch up‚Äù: \\(w_1\\) is the weight to be applied to \\(x_1\\), \\(w_2\\) is the weight to be applied to \\(x_2\\), and so on):\n\\[\n\\mathbf{w} = (w_1, w_2, \\ldots, w_n).\n\\]\nNow, if we wanted to write out the weighted sum \\(S\\) of these datapoints \\(\\mathbf{x}\\), with the weights given by \\(\\mathbf{w}\\), the ‚Äústraightforward‚Äù way (at least, in the sense that it‚Äôs probably the approach you learned earlier on in your math-class-taking career) would look like:\n\\[\nS = w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\sum_{i=1}^{n}w_ix_i,\n\\]\nwhich is why that \\(\\Sigma\\) notation will appear all over the place when you are working through the early parts of the homework.\nHowever, now consider the fact that you know about some fancier mathematical objects‚Äîvectors and matrices‚Äîand how they can help us in terms of providing an alternative representation of this same sum! Instead of ‚Äúzooming in‚Äù on the individual elements to write out the weighted sum, we could just as easily treat them as unitary objects‚Äîvectors‚Äîand apply the binary dot product operator from Linear Algebra to achieve the same weighted sum!\nTo see this, recall how the dot product of two vectors is defined, and then work out what (e.g.) \\(\\mathbf{w} \\cdot \\mathbf{x}^{\\top}\\) looks like:\n\\[\n\\begin{align*}\n\\mathbf{w} \\cdot \\mathbf{x} &= (w_1, w_2, \\ldots, w_n) \\cdot (x_1, x_2, \\ldots, x_n) \\\\\n&= w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\sum_{i=1}^{n}w_ix_i = S ~ ‚úÖ\n\\end{align*}\n\\tag{1}\\]\nAnd so we see that, indeed, we can obtain this same sum \\(S\\) by considering the datapoints and weights as vectors and then using the dot product as our key operator for combining these vectors (rather than thinking of \\(S\\) on the level of individual multiplications and additions)."
  },
  {
    "objectID": "writeups/hw1-guide/index.html#the-regression-models-linear-algebraic-representation",
    "href": "writeups/hw1-guide/index.html#the-regression-models-linear-algebraic-representation",
    "title": "Getting Started with HW 1",
    "section": "The Regression Model‚Äôs Linear Algebraic Representation",
    "text": "The Regression Model‚Äôs Linear Algebraic Representation\nThat all may be obvious to you, if you‚Äôve taken Linear Algebra for example, but now to foreshadow some of the later portions of the homework, let‚Äôs look at how this idea can help us when e.g.¬†we‚Äôre working with the math of regression models.\nConsider the non-Linear-Algebraic way we‚Äôve been writing out the basic pieces of the regression model thus far in the class. The Multiple Linear Regression model, for example, is typically written out in a form like this (where here I‚Äôm writing out the model we‚Äôd use to predict one particular label, \\(y_i\\), as a function of one observation‚Äôs features, \\(x_{i,1}\\) through \\(x_{i,m}\\)).\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\tag{2}\\]\nIf we look at this representation, we can see how it almost perfectly matches the type of weighted sum we re-wrote using the dot product in Equation¬†1 above. The only difference, in fact, is that the pesky \\(\\beta_0\\) coefficient has no corresponding \\(x_{i,0}\\) term‚Ä¶\n‚Ä¶But, what if we just defined an \\(x_{i,0}\\), to just always take on the numeric value \\(1\\)? This would provide the ‚Äúmissing piece‚Äù which would allow us to write the Multiple Linear Regression model as a dot product! That is: once we define \\(x_{i,0} \\triangleq 1\\), we can now write the data for this observation as as vector \\(\\mathbf{x}_i\\) like:\n\\[\n\\mathbf{x}_i = (x_{i,0}, x_{i,1}, x_{i,2}, \\ldots, x_{i,M}) = (1, x_{i,1}, x_{i,2}, \\ldots, x_{i,M}),\n\\]\nand we can write the coefficients as a vector \\(\\boldsymbol\\beta\\) like:\n\\[\n\\boldsymbol\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_M),\n\\]\nand we achieve the same type of result as in the previous section‚Äîthat the Multiple Linear Regression model itself (modeling the prediction for one observation, for now) can be written as a dot product:\n\\[\n\\begin{align*}\n\\widehat{y}_i &= \\boldsymbol\\beta \\cdot \\mathbf{x}_i = (\\beta_0, \\beta_1, \\ldots, \\beta_M) \\cdot (1, x_{i,1}, \\ldots, x_{i,M}) \\\\\n&= \\beta_0 + \\beta_1 x_{i,1} + \\cdots + \\beta_M x_{i,M} ~ ‚úÖ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#from-vectors-to-matrices",
    "href": "writeups/hw1-guide/index.html#from-vectors-to-matrices",
    "title": "Getting Started with HW 1",
    "section": "From Vectors to Matrices",
    "text": "From Vectors to Matrices\nThe final step, in terms of representing the full MLR model using objects from Linear Algebra (rather than just its prediction for one particular observation \\(i\\)), requires us to think about matrices rather than just the vectors we‚Äôve used thus far. However, just as we thought of:\n\nVectors as objects allowing us to group individual scalars together into a single object, here we can also retain our sanity by thinking of\nMatrices as objects allowing us to group individual vectors together into a single object!\n\nTo see how this way of thinking can help us here, notice how if we have \\(N\\) labels (\\(y_1\\) through \\(y_n\\)) for \\(N\\) datapoints (\\(\\mathbf{x}_1\\) through \\(\\mathbf{x}_n\\)), then our model is going to generate \\(N\\) predictions, using Equation¬†2 above \\(N\\) times:\n\\[\n\\begin{align*}\n\\widehat{y}_1 &= \\beta_0 + \\beta_1 x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\widehat{y}_2 &= \\beta_0 + \\beta_1 x_{2,1} + \\cdots + \\beta_m x_{2,m} \\\\\n\\phantom{y_3} &~~\\vdots \\\\\n\\widehat{y}_n &= \\beta_0 + \\beta_1 x_{n,1} + \\dots + \\beta_m x_{n,m}\n\\end{align*}\n\\]\nSo, if you squint your eyes while looking at this system of equations, and you keep in mind the above point about defining \\(x_{i,0}\\) to just be the value \\(1\\) for every observation \\(i\\), hopefully you can start to see how that whole thing could be re-written as a single matrix equation!\nTo start off, for example, we could gather all of the \\(\\widehat{y}_i\\) terms on the left-hand side of each equation into a single column vector:\n\\[\n\\widehat{\\mathbf{y}} = \\begin{bmatrix}\n\\widehat{y}_1 \\\\\n\\vdots \\\\\n\\widehat{y}_n\n\\end{bmatrix}\n\\]\nAnd, next, we already saw how weighted sums like the ones we see on the right-hand side of each equation can be re-written as vector-vector products (dot products). So, in the same way that we ‚Äústacked‚Äù the individual \\(\\widehat{y}_i\\) terms into a single column vector just now, we can also look at these right-hand side expressions as a ‚Äústack‚Äù of such products, like:\n\\[\n\\begin{bmatrix}\n\\beta_0 x_{1,0} + \\beta_1x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\vdots \\\\\n\\beta_0 x_{n,0} + \\beta_1x_{n,1} + \\cdots + \\beta_m x_{n,m}\n\\end{bmatrix} = \\begin{bmatrix}\n\\boldsymbol\\beta \\cdot \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\boldsymbol\\beta \\cdot \\mathbf{x}_n\n\\end{bmatrix}\n\\tag{3}\\]\nThe final leap, which I think is most helpful if you try to work it out yourself (like, as in, by trying different guesses and multiplying them out to see if you get the desired result), is to take this almost-there representation where we‚Äôve stacked the dot products (\\(\\boldsymbol\\beta \\cdot \\mathbf{x}_1\\), \\(\\boldsymbol\\beta \\cdot \\mathbf{x}_2\\), and so on) into a column vector, and turn it into a product of only ‚Äúbase‚Äù Linear Algebra objects: vectors and/or matrices.\nIn other words, right now we have a column-vector-of-dot-products, which is not exactly what we think of when we think of ‚Äúa vector‚Äù or ‚Äúa matrix‚Äù (it‚Äôs‚Ä¶ a hybrid of the two, in a sense). To make progress, take note of the fact that:\n\n\\(\\boldsymbol\\beta\\) appears in every row, whereas\nFor a given index \\(i\\), \\(\\mathbf{x}_i\\) only appears in one row.\n\nSo (here‚Äôs where you can pause and try to work it out on paper, before reading on!), the second bullet point provides us with a hint that it may be helpful to construct a data matrix \\(\\mathbf{X}\\), where each row \\(i\\) contains all of the terms in \\(\\mathbf{x}_i\\):\n\\[\n\\begin{align*}\n\\mathbf{X} &= \\begin{bmatrix}\n\\mathbf{x}_1 \\\\\n\\mathbf{x}_2 \\\\\n\\vdots \\\\\n\\mathbf{x}_n\n\\end{bmatrix} = \\begin{bmatrix}\nx_{1,0} & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\nx_{2,0} & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n,0} & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix}\n\\end{align*}\n\\]\nAnd finally, the first bullet point gives us a hint that we don‚Äôt need a full matrix to represent \\(\\boldsymbol\\beta\\), since the same set of parameters \\(\\boldsymbol\\beta\\) is used across all predictions from \\(\\widehat{y}_1\\) to \\(\\widehat{y}_n\\). Instead, to ensure that it ‚Äúcombines with‚Äù our data matrix \\(\\mathbf{X}\\) to produce the desired final system of equations, we can represent it as a column vector:\n\\[\n\\boldsymbol\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_m\n\\end{bmatrix}\n\\]\nThough the choice of a column vector rather than a row vector here might seem arbitrary at first, the point is exactly what I mentioned above, that you can try the two different representations and see which one is more helpful for what we‚Äôre trying to do. It turns out that, with this column vector representation, we can combine \\(\\mathbf{X}\\) with \\(\\boldsymbol\\beta\\) using a simple matrix-vector multiplication to obtain the final result we‚Äôve been looking for:\n\\[\n\\begin{align*}\n\\mathbf{X}\\boldsymbol\\beta &= \\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n\\end{bmatrix} \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_m\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\beta_0 x_{1,0} + \\beta_1x_{1,1} + \\cdots + \\beta_m x_{1,m} \\\\\n\\vdots \\\\\n\\beta_0 x_{n,0} + \\beta_1x_{n,1} + \\cdots + \\beta_m x_{n,m}\n\\end{bmatrix}\n\\end{align*}\n\\]\nWhich is precisely the stack-of-weighted-sums we were hoping to ‚Äúdecompose‚Äù into Linear Algebraic operations in Equation¬†3 above!\nSo, combining all this together, if we want a representation of our MLR model without any sums (since the sums are all performed implicitly via the vector-vector and matrix-vector operations), we can now just use the following ultra-shorthand Linear Algebraic form!\n\\[\n\\widehat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol\\beta\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#sanity-check-how-do-we-know-these-operations-will-be-well-defined",
    "href": "writeups/hw1-guide/index.html#sanity-check-how-do-we-know-these-operations-will-be-well-defined",
    "title": "Getting Started with HW 1",
    "section": "Sanity Check: How Do We Know These Operations Will Be Well-Defined?",
    "text": "Sanity Check: How Do We Know These Operations Will Be Well-Defined?\nThis is kind of‚Ä¶ shoved in here, since it‚Äôs generally super useful imo, across any situation where you‚Äôre using matrices and/or vectors, but I may as well introduce it here!\nThere is a ‚Äúsanity check‚Äù you can perform, whenever you‚Äôre doing math or writing code that involves matrices/vectors, that instantly gives you two pieces of information:\n\nA verification of whether or not the matrix-matrix or matrix-vector product is well-defined, but also\nThe specific dimensions that the result of the matrix-matrix or matrix-vector product will have!\n\nIn general, this sanity check involves just checking how many rows and columns are in the two objects you‚Äôre hoping to multiply, and writing these two dimensions (number of rows and number of columns) underneath each object. So, for example, if we‚Äôre thinking about right-multiplying a \\(3 \\times 2\\) matrix \\(A\\) by a \\(2 \\times 4\\) matrix \\(B\\), we can write the two matrices out like:\n\\[\nAB =\n\\underbrace{\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{bmatrix}}_{3 \\times 2} \\underbrace{\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24}\n\\end{bmatrix}}_{2 \\times 4}\n\\]\nAnd now, the first piece of information this gives us: how do we know whether or not this multiplication is well-defined?\n\nThe multiplication is well-defined if the two ‚Äúinner‚Äù numbers written under the matrices are equal: in this case, the operation is well-defined because the number of columns in the first matrix \\(A\\) (\\(2\\)) is equal to the number of rows in the second matrix \\(B\\) (\\(2\\))!\n\nI call these the ‚Äúinner‚Äù numbers because, if we simplified the above to just have the shapes written out, we‚Äôd get\n\\[\n[3 \\times 2][2 \\times 4],\n\\]\nin which the ‚Äúwell-defined test‚Äù can now be read off from the two numbers (\\(2]\\) and \\([2\\)) on the ‚Äúinside‚Äù of this 4-number representation.\nNext, we get the second piece of information: what dimensions will the resulting product have?\n\nIf the multiplication is well-defined, then the result will have a shape given by the two outer numbers written underneath the matrices above: In this case, the result have a number of rows equal to the number of rows in \\(A\\) (\\(3\\)) and a number of columns equal to the number of columns in \\(B\\) (\\(4\\)), i.e., the result will be a \\(3 \\times 4\\) matrix.\n\nWe can verify this, for example, if we know the procedure for matrix-matrix multiplication from Linear Algebra:\n\\[\n\\begin{align*}\nAB &= \\underbrace{\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\na_{31} & a_{32}\n\\end{bmatrix}}_{3 \\times 2} \\underbrace{\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24}\n\\end{bmatrix}}_{2 \\times 4} \\\\\n&= \\underbrace{\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} & a_{11}b_{13} + a_{12}b_{23} & a_{11}b_{14} + a_{12}b_{24} \\\\\na_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} & a_{21}b_{13} + a_{22}b_{23} & a_{21}b_{14} + a_{22}b_{24} \\\\\na_{31}b_{11} + a_{32}b_{21} & a_{31}b_{12} + a_{32}b_{22} & a_{31}b_{13} + a_{32}b_{23} & a_{31}b_{14} + a_{32}b_{24}\n\\end{bmatrix}}_{3 \\times 4}\n\\end{align*}\n\\]\nAnd we can see that the result is a \\(3 \\times 4\\) matrix, as expected given the pair of ‚Äúouter‚Äù numbers \\([3\\) and \\(4]\\) in the shape representation\n\\[\n[3 \\times 2][2 \\times 4].\n\\]\nIn my head, once I got used to it, this boiled down to something like the following ‚Äúrule‚Äù: given a \\(r_A \\times c_A\\) matrix \\(A\\) and a \\(r_B \\times c_B\\) matrix \\(B\\), their matrix-matrix product \\(AB\\) can be sanity-checked using a ‚Äúdiagram‚Äù like:\n\\[\n[\\underline{r_A} \\times \\overset{‚úÖ}{c_A}][\\overset{‚úÖ}{r_B} \\times \\underline{c_B}] = [r_A \\times c_B]\n\\]"
  },
  {
    "objectID": "writeups/hw1-guide/index.html#least-squares-derivation",
    "href": "writeups/hw1-guide/index.html#least-squares-derivation",
    "title": "Getting Started with HW 1",
    "section": "Least Squares Derivation",
    "text": "Least Squares Derivation\nFor this part, the truth is you‚Äôre mainly going to have to wade through a lot of algebra‚Äîthe idea is to get comfortable with the use/manipulation of the kinds of quantities that will probably come up again and again as you absorb fancier and fancier regression and ML models!\nSo, if your derivation has lots of terms that look like \\(\\sum_{i=1}^{N}x_iy_i\\), \\(\\sum_{i=1}^{N}x_i^2\\), and so on, that is a good sign (and relates to the points in the previous sections about turning sums into Linear Algebraic operations)!\nProbably the most concrete advice I can give, for those of you still wrestling with this part, is that at least for the way my brain works it helps to ‚Äúcompartmentalize‚Äù the derivation into two chunks:\nFirst (again, for me, though this division may not be helpful for everyone), I find it helpful to split the terms we might see into (a) terms which arise from treating it like a ‚Äúpure‚Äù calculus problem that we solve by taking derivatives and setting them equal to zero, and then (b) terms/definitions which we bring in from statistics (the definitions which are given in the problem) which can help us interpret what we find. So, for this derivation, that split would look like:\n\n\n\n\n\n\n\n(a) Calculus/Algebra Thing\n(b) Statistics Thing\n\n\n\n\n\\(\\sum_{i=1}^{N}x_i\\)\n\\(\\overline{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\n\\(\\sum_{i=1}^{N}y_i\\)\n\\(\\overline{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_i\\)\n\n\n\\(\\sum_{i=1}^{N}x_iy_i\\)\n\\(\\text{Cov}[X, Y] = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\overline{x})(y_i - \\overline{y})\\)\n\n\n\\(\\sum_{i=1}^{N}x_i^2\\)\n\\(\\text{Var}[X] = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\overline{x})^2\\)\n\n\n\nThen, I like to think of the first actual doing-math step as the step where I treat is as a ‚Äúpure‚Äù calculus problem. Mainly because my brain starts to hurt if I switch back-and-forth between Calculus/Algebra Things and Statistics Things, I find it more comfortable to forget about the right column in the above table and just solve:\n\\[\n\\begin{align*}\n\\min_{m,b}\\left[ L(m,b) \\right] &= \\min_{m,b}\\left[ \\sum_{i=1}^{N} (\\widehat{y}(m,b) - y)^2 \\right] \\\\\n&= \\min_{m,b}\\left[ \\sum_{i=1}^{N}((mx_i + b) - y)^2 \\right]\n\\end{align*}\n\\]\nThe way I learned to solve minimization problems in calculus: by taking the thing inside the square brackets, computing its derivative(s) with respect to the maximand(s) (\\(m\\) and \\(b\\) in this case), and then solving the system of equations which in some classes would be called the ‚ÄúFirst-Order Conditions‚Äù that are necessary (but not sufficient) for some chosen values \\(m^*\\) and \\(b^*\\) to minimize the function:\n\\[\n\\left. \\frac{\\partial L}{\\partial m}\\right|_{\\substack{m=m^* \\\\ b=b^*}} = 0 \\wedge \\left. \\frac{\\partial L}{\\partial b}\\right|_{\\substack{m=m^* \\\\ b=b^*}} = 0\n\\]\nThe goal, once you take these two derivatives and set them equal to zero, is to use algebraic manipulations to eventually arrive at closed-form solutions for \\(m\\) and \\(b\\). In this case, what would a closed-form solution look like?\n\nA closed-form solution for \\(m\\) would be an expression like\n\\[\nm = [\\text{stuff}],\n\\]\nwhere everything on the right-hand side is a function of only \\(x_i\\), \\(y_i\\), and \\(N\\). Meaning, if \\(b\\) or \\(m\\) itself appear on the right-hand side, you have not yet arrived at a closed-form solution for \\(m\\)!\nA closed-form solution for \\(b\\) would be an expression like\n\\[\nb = [\\text{stuff}],\n\\]\nwhere everything on the right-hand side here is a function of only \\(x_i\\), \\(y_i\\), and \\(N\\). So, if \\(m\\) or \\(b\\) itself appear on the right-hand side, you have not yet arrived at a closed-form solution for \\(b\\)!\n\nFinally, once I have these two closed-form solutions for \\(m\\) and \\(b\\), I then take the Statistics Things back out and try to rewrite the terms in this closed form solution in terms of \\(\\overline{x}\\), \\(\\overline{y}\\), \\(\\text{Var}[X]\\), \\(\\text{Var}[Y]\\), and \\(\\text{Cov}[X,Y]\\).\nThis may not help in terms of the particular point at which you may be stuck, in which case I‚Äôm sorry in advance! But, it‚Äôs the‚Ä¶ division-of-tasks that tended to help me when deriving closed-form solutions like this in e.g.¬†econometrics classes!"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nWeek\n\n\nAssignment\n\n\n\n\n\n\nGetting Started with HW 2\n\n\n5\n\n\nHW 2\n\n\n\n\nGetting Started with HW 1\n\n\n3\n\n\nHW 1\n\n\n\n\nUsing Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D\n\n\n3\n\n\nGeneral\n\n\n\n\nQuiz 1 Study Guide\n\n\n2\n\n\nQuiz 1\n\n\n\n\nGetting Started with Lab 1\n\n\n2\n\n\nLab 1\n\n\n\n\nExtra Slides: A Slightly Deeper Dive Into Machine Learning\n\n\n2\n\n\nGeneral\n\n\n\n\nRegression vs.¬†PCA\n\n\n2\n\n\nGeneral\n\n\n\n\nMathematical Optimization\n\n\n1\n\n\nGeneral\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Writeups"
    ]
  },
  {
    "objectID": "writeups/3d-plots/index.html",
    "href": "writeups/3d-plots/index.html",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "",
    "text": "The main reason why I figured a writeup on plotting with Python could be helpful for DSAN 5300 specifically is because, even though many of yall are in DSAN 5200 right now, there are visualization techniques that will be helpful to have early-on in 5300‚Äîprimarily the challenges of plotting in 3D rather than 2D‚Äîwhich may not be covered until later in 5200.\nSo, therefore, the motivating example in this writeup will be plotting a 3D surface within which we‚Äôll want to overlay the path taken by gradient descent to move from a randomly-selected point on the surface to a (possibly local) minimum of the surface."
  },
  {
    "objectID": "writeups/3d-plots/index.html#the-goal-3d-plots",
    "href": "writeups/3d-plots/index.html#the-goal-3d-plots",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "",
    "text": "The main reason why I figured a writeup on plotting with Python could be helpful for DSAN 5300 specifically is because, even though many of yall are in DSAN 5200 right now, there are visualization techniques that will be helpful to have early-on in 5300‚Äîprimarily the challenges of plotting in 3D rather than 2D‚Äîwhich may not be covered until later in 5200.\nSo, therefore, the motivating example in this writeup will be plotting a 3D surface within which we‚Äôll want to overlay the path taken by gradient descent to move from a randomly-selected point on the surface to a (possibly local) minimum of the surface."
  },
  {
    "objectID": "writeups/3d-plots/index.html#no-3d-plotting-in-seaborn-matplotlib-to-the-rescue",
    "href": "writeups/3d-plots/index.html#no-3d-plotting-in-seaborn-matplotlib-to-the-rescue",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "No 3D Plotting in Seaborn üò¢ Matplotlib to the Rescue!",
    "text": "No 3D Plotting in Seaborn üò¢ Matplotlib to the Rescue!\nSadly, even though Seaborn is typically my go-to for the types of visualizations I find myself needing to produce in Data Science contexts, it has little to no support for 3D plotting specifically. Which‚Ä¶ makes some sense, given that (as you‚Äôll see in 5200) in many visual-communication scenarios there is usually a way to use 2D plots to achieve a more easily-interpretable set of visualizations for your audience!\nBut, nonetheless, in this class we have run into one of the key cases (understanding gradient descent beyond single-variable loss functions) where I think we can truly benefit from having 3D plots. So, with that said, a link you can bookmark and explore to get a feel for 3D visualization in Python is Matplotlib‚Äôs plot-type: 3D tag, which gathers together all of the examples within Matplotlib‚Äôs documentation which involve generating 3D plots:\nhttps://matplotlib.org/stable/_tags/plot-type-3d.html"
  },
  {
    "objectID": "writeups/3d-plots/index.html#your-first-3d-plot-in-matplotlib",
    "href": "writeups/3d-plots/index.html#your-first-3d-plot-in-matplotlib",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Your First 3D Plot in Matplotlib",
    "text": "Your First 3D Plot in Matplotlib\nTo see your first 3D plot created with matplotlib, let‚Äôs consider how gradient descent might allow us to choose a random point on the (visible) surface of this plot (the green dot), and eventually make our way to the optimal loss-minimizing value (the light blue dot at \\((x,y) = (0,0)\\)):\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nrng = np.random.default_rng(seed=5302)\nfrom matplotlib import cm\n\ncb_palette = ['#E69F00', '#56B4E9', '#009E73']\n\ndef loss_fn(x, y):\n  return x**2 + y**2\n\ndef objective_fn(x, y):\n  return -loss_fn(x, y)\n\nrand_v = np.array([\n  rng.uniform(low=-4, high=0),\n  rng.uniform(low=0, high=4),\n])\n\ndef gen_surface_plot(\n  points,\n  title=None,\n  minimization=True,\n  label_points=False,\n  arrows=False,\n  elevation=42,\n  azimuth=-60\n):\n  if title is None:\n    title = \"Minimization of a Loss Function\"\n    if not minimization:\n      title = \"Maximization of an Objective Function\"\n  opt_fn = loss_fn if minimization else objective_fn\n  fig, ax = plt.subplots(\n    subplot_kw={\n      \"projection\": \"3d\",\n      \"title\": title,\n      \"computed_zorder\": False,\n    }\n  )\n  # Generate the surface representing the value of\n  # the loss function (the z coordinate) for any\n  # pair of (x,y) values\n  x_range = np.arange(-5, 5, 0.25)\n  y_range = np.arange(-5, 5, 0.25)\n  x_vals, y_vals = np.meshgrid(x_range, y_range)\n  z_vals = opt_fn(x_vals, y_vals)\n\n  # Plot the points given by the points argument!\n  points_x = [p[0] for p in points]\n  points_y = [p[1] for p in points]\n  points_z = [opt_fn(p[0],p[1]) for p in points]\n  ax.scatter(points_x, points_y, points_z, color=cb_palette[2], s=80, zorder=10)\n\n  # Add labels to points if label_points is True\n  if label_points:\n    for point_index, point in enumerate(points):\n      ax.text(point[0], point[1], loss_fn(point[0], point[1]), str(point_index), zorder=20)\n\n  # Compute and plot the optimal value\n  opt_z = np.min(z_vals) if minimization else np.max(z_vals)\n  ax.scatter([0], [0], [opt_z], color=cb_palette[1], s=80, zorder=10)\n  surf = ax.plot_surface(\n      x_vals, y_vals, z_vals, cmap='magma', zorder=0, alpha=0.8\n  )\n  ax.set_xlabel(\"x\")\n  ax.set_ylabel(\"y\")\n  \n  ax.view_init(elev=elevation, azim=azimuth)\n\n  # Add arrows if requested\n  if arrows:\n    for i in range(len(points) - 1):\n      cur_point = points[i]\n      cur_z = loss_fn(cur_point[0], cur_point[1])\n      next_point = points[i+1]\n      next_z = loss_fn(next_point[0], next_point[1])\n      x_diff = next_point[0] - cur_point[0]\n      y_diff = next_point[1] - cur_point[1]\n      z_diff = next_z - cur_z\n      ax.quiver(\n        cur_point[0], cur_point[1], cur_z,\n        x_diff, y_diff, z_diff,\n        color = 'white', alpha = .8, lw = 1,\n        length=1\n      )\n\n  # Add a color bar which maps values to colors.\n  fig.colorbar(surf, shrink=0.5, aspect=5)\n  plt.show()\ngen_surface_plot(points=[rand_v])\n\n\n\n\n\n\n\n\n\nAlso note how I made the plot-generation code into a function, here called gen_surface_plot(), which can be immensely helpful for your own plotting adventures, since you can utilize arguments to the functions to quickly change different aspects of the plot. For example, I set this function up to allow ‚Äúinstant‚Äù switching from visualization of minimization to visualization of a maximization problem, which means we can just pass minimization=False to the function to generate a plot of what finding the maximum value of a function (the same function, just ‚Äúflipped over‚Äù via \\(z \\mapsto -z\\)):\n\ngen_surface_plot(points=[rand_v], minimization=False)"
  },
  {
    "objectID": "writeups/3d-plots/index.html#plotting-the-gradient-descent-path",
    "href": "writeups/3d-plots/index.html#plotting-the-gradient-descent-path",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Plotting the Gradient Descent Path",
    "text": "Plotting the Gradient Descent Path\n\nThe Straightforward First Step: Plotting Points\nReturning to our minimization-in-3D case, how can we now plot the path/trajectory that we follow if we apply gradient descent starting from the randomly-chosen (green) point? One easy way is to just keep track of each new value in a list (here, we call it grad_path), and then plot each element in this list as a point along our gradient-descent pathway:\n\n\nCode\ndef grad_loss(x, y):\n  return np.array([2*x, 2*y])\n\ndef run_grad_descent(starting_v, num_steps, step_size=0.1):\n  cur_v = starting_v\n  path = [starting_v]\n  for i in range(num_steps):\n    grad_at_v = grad_loss(cur_v[0], cur_v[1])\n    new_v = cur_v - step_size * grad_at_v\n    path.append(new_v)\n    cur_v = new_v\n  return path\nnum_steps = 5\ngrad_path = run_grad_descent(rand_v, num_steps)\ngen_surface_plot(grad_path, title=f\"Gradient Descent, {num_steps} steps\")\n\n\n\n\n\n\n\n\n\nEven though it may be obvious in this case the order of the points (meaning, you can just ‚Äúeyeball‚Äù which was the starting point, the second point, the third, and so on), oftentimes it can help to label the order of the points. So, the function takes an optional label_points argument, which shows the order of the points along the gradient path if set to True (for more on how the text() function works in 3D world, see here):\n\n\nCode\ngen_surface_plot(grad_path, title=f\"Gradient Descent, {num_steps} steps\", label_points=True)\n\n\n\n\n\n\n\n\n\nAnd now we can get a sense for how many steps we‚Äôll need for convergence in this case (with a step size set to be \\(0.1\\)), by extending this plot to show the trajectory for 10 steps rather than 5:\n\n\nCode\nnum_steps = 10\ngrad_path_10 = run_grad_descent(rand_v, num_steps)\ngen_surface_plot(grad_path_10, title=f\"Gradient Descent, {num_steps} steps\", label_points=True)\n\n\n\n\n\n\n\n\n\n\n\nThe Less-Straightforward Second Step: Arrows\nI‚Äôm not gonna lie to you‚Ä¶ by the time you are trying to add arrows to your matplotlib plots, you have somewhat hit the limits of the (very basic!) functionality of matplotlib. It is possible, and supported by way of the quiver() function, but personally I‚Äôve never been able to get the arrows to actually look good, which is why at this point I might recommend switching over from matplotlib to something like Plotly for this task. But, since we‚Äôve already produced the above plot, we may as well see what it looks like to just add the arrows onto the plot using ax.quiver().\n\n\nCode\ngen_surface_plot(\n  grad_path_10,\n  title=\"Adding arrows with ax.quiver()\",\n  label_points=True,\n  arrows=True\n)"
  },
  {
    "objectID": "writeups/3d-plots/index.html#moving-the-camera",
    "href": "writeups/3d-plots/index.html#moving-the-camera",
    "title": "Using Python to Plot‚Ä¶ Whatever‚Äôs Going On in 3D",
    "section": "Moving the ‚ÄúCamera‚Äù",
    "text": "Moving the ‚ÄúCamera‚Äù\nLastly, finally, since this trajectory is somewhat difficult to see from the default angle, you can use the elevation and azimuth options to the ax.view_init() function to shift the ‚Äúcamera‚Äù around! Here we‚Äôll try to see the pathway a little better by lowering its elevation a bit and rotating it to look at this trajectory more ‚Äúfrom the side‚Äù than head-on:\n\n\nCode\ngen_surface_plot(\n  grad_path_10,\n  title=f\"Gradient Descent, {num_steps} steps\",\n  label_points=True,\n  arrows=True,\n  elevation=39,\n  azimuth=-98\n)\n\n\n\n\n\n\n\n\n\nSo, if you find yourself extremely stuck at some point where 3D visualization of what‚Äôs going on could help, I hope you are able to use this gen_surface_plot() function with the different options to maybe get a glimpse of e.g.¬†the trajectory that some numerical optimization algorithm is taking with respect to some loss surface!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html",
    "href": "writeups/regression-vs-pca/index.html",
    "title": "Regression vs.¬†PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "title": "Regression vs.¬†PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-goal",
    "href": "writeups/regression-vs-pca/index.html#the-goal",
    "title": "Regression vs.¬†PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "title": "Regression vs.¬†PCA",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "title": "Regression vs.¬†PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\ndist_to_line &lt;- function(x0, y0, a, c) {\n    numer &lt;- abs(a * x0 - y0 + c)\n    denom &lt;- sqrt(a * a + 1)\n    return(numer / denom)\n}\n# Finding PCA line for industrial vs. exports\nx &lt;- gdp_df$industrial\ny &lt;- gdp_df$exports\nlossFn &lt;- function(lineParams, x0, y0) {\n    a &lt;- lineParams[1]\n    c &lt;- lineParams[2]\n    return(sum(dist_to_line(x0, y0, a, c)))\n}\no &lt;- optim(c(0, 0), lossFn, x0 = x, y0 = y)\nggplot(gdp_df, aes(x = industrial, y = exports)) +\n    geom_point(size=g_pointsize/2) +\n    geom_abline(aes(slope = o$par[1], intercept = o$par[2], color=\"pca\"), linewidth=g_linewidth, show.legend = TRUE) +\n    geom_smooth(aes(color=\"lm\"), method = \"lm\", se = FALSE, linewidth=g_linewidth, key_glyph = \"blank\") +\n    scale_color_manual(element_blank(), values=c(\"pca\"=cbPalette[2],\"lm\"=cbPalette[1]), labels=c(\"Regression\",\"PCA\")) +\n    dsan_theme(\"half\") +\n    remove_legend_title() +\n    labs(\n      title = \"PCA Line vs. Regression Line\",\n        x = \"Industrial Production (% of GDP)\",\n        y = \"Exports (% of GDP)\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "title": "Regression vs.¬†PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!\n\n\nCode\nggplot(gdp_df, aes(pc1, .fittedPC2)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(aes(yintercept=0, color='PCA Line'), linetype='solid', size=g_linesize) +\n    geom_rug(sides = \"b\", linewidth=g_linewidth/1.2, length = unit(0.1, \"npc\"), color=cbPalette[3]) +\n    expand_limits(y=-1.6) +\n    scale_color_manual(element_blank(), values=c(\"PCA Line\"=cbPalette[2])) +\n    dsan_theme(\"full\") +\n    remove_legend_title() +\n    labs(\n      title = \"Exports vs. Industrial Production in Principal Component Space\",\n      x = \"First Principal Component (Dimension of Greatest Variance)\",\n      y = \"Second Principal Component\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "title": "Regression vs.¬†PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nplot_df &lt;- gdp_df %&gt;% select(c(country_code, pc1, agriculture, military))\nlong_df &lt;- plot_df %&gt;% pivot_longer(!c(country_code, pc1), names_to = \"var\", values_to = \"val\")\nlong_df &lt;- long_df |&gt; mutate(\n  var = case_match(\n    var,\n    \"agriculture\" ~ \"Agricultural Production\",\n    \"military\" ~ \"Military Spending\"\n  )\n)\nggplot(long_df, aes(x = pc1, y = val, facet = var)) +\n    geom_point() +\n    facet_wrap(vars(var), scales = \"free\") +\n    dsan_theme(\"full\") +\n    labs(\n        x = \"Industrial-Export Dimension\",\n        y = \"% of GDP\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "title": "Regression vs.¬†PCA",
    "section": "But in Our Case‚Ä¶",
    "text": "But in Our Case‚Ä¶\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "title": "Regression vs.¬†PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#regression-in-r",
    "href": "writeups/regression-vs-pca/index.html#regression-in-r",
    "title": "Regression vs.¬†PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#lm-syntax",
    "href": "writeups/regression-vs-pca/index.html#lm-syntax",
    "title": "Regression vs.¬†PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#interpreting-output",
    "href": "writeups/regression-vs-pca/index.html#interpreting-output",
    "title": "Regression vs.¬†PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth) +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"dashed\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"solid\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"dashed\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "title": "Regression vs.¬†PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(lin_model)\nggplot(gdp_resid_df, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Industrial ~ Military\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#q-q-plot",
    "href": "writeups/regression-vs-pca/index.html#q-q-plot",
    "title": "Regression vs.¬†PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "title": "Regression vs.¬†PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#references",
    "href": "writeups/regression-vs-pca/index.html#references",
    "title": "Regression vs.¬†PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "title": "Regression vs.¬†PCA",
    "section": "The Central Tool of Data Science",
    "text": "The Central Tool of Data Science\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-goal",
    "href": "writeups/regression-vs-pca/slides.html#the-goal",
    "title": "Regression vs.¬†PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "title": "Regression vs.¬†PCA",
    "section": "How Do We Define ‚ÄúBest‚Äù?",
    "text": "How Do We Define ‚ÄúBest‚Äù?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "title": "Regression vs.¬†PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "title": "Regression vs.¬†PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "title": "Regression vs.¬†PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "title": "Regression vs.¬†PCA",
    "section": "But in Our Case‚Ä¶",
    "text": "But in Our Case‚Ä¶\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "title": "Regression vs.¬†PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the ‚Äúbest‚Äù linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "href": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "title": "Regression vs.¬†PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "href": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "title": "Regression vs.¬†PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "href": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "title": "Regression vs.¬†PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "title": "Regression vs.¬†PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "title": "Regression vs.¬†PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: ‚Äúhomoskedasticity‚Äù\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "href": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "title": "Regression vs.¬†PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45¬∞ line:"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "title": "Regression vs.¬†PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#references",
    "href": "writeups/regression-vs-pca/slides.html#references",
    "title": "Regression vs.¬†PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Introduction to the Course",
    "section": "",
    "text": "Week 1 was held on Zoom as a combined session of the three sections of DSAN 5300\nThe recording can be found here",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  }
]