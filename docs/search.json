[
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Introduction to the Course",
    "section": "",
    "text": "Week 1 was held on Zoom as a combined session of the three sections of DSAN 5300\nThe recording can be found here",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/slides.html#the-central-tool-of-data-science",
    "title": "Regression vs. PCA",
    "section": "The Central Tool of Data Science",
    "text": "The Central Tool of Data Science\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-goal",
    "href": "writeups/regression-vs-pca/slides.html#the-goal",
    "title": "Regression vs. PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/slides.html#how-do-we-define-best",
    "title": "Regression vs. PCA",
    "section": "How Do We Define “Best”?",
    "text": "How Do We Define “Best”?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/slides.html#principal-component-analysis",
    "title": "Regression vs. PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/slides.html#create-your-own-dimension",
    "title": "Regression vs. PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/slides.html#and-use-it-for-eda",
    "title": "Regression vs. PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/slides.html#but-in-our-case",
    "title": "Regression vs. PCA",
    "section": "But in Our Case…",
    "text": "But in Our Case…\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/slides.html#key-features-of-regression-line",
    "title": "Regression vs. PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the “best” linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "href": "writeups/regression-vs-pca/slides.html#regression-in-r",
    "title": "Regression vs. PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "href": "writeups/regression-vs-pca/slides.html#lm-syntax",
    "title": "Regression vs. PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "href": "writeups/regression-vs-pca/slides.html#interpreting-output",
    "title": "Regression vs. PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-coefficients",
    "title": "Regression vs. PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/slides.html#zooming-in-significance",
    "title": "Regression vs. PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/slides.html#the-residual-plot",
    "title": "Regression vs. PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: “homoskedasticity”\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "href": "writeups/regression-vs-pca/slides.html#q-q-plot",
    "title": "Regression vs. PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45° line:"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/slides.html#multiple-linear-regression",
    "title": "Regression vs. PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/slides.html#references",
    "href": "writeups/regression-vs-pca/slides.html#references",
    "title": "Regression vs. PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\n\n\n\nDSAN 5300 Extra Slides: Regression vs. PCA"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html",
    "href": "writeups/regression-vs-pca/index.html",
    "title": "Regression vs. PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "href": "writeups/regression-vs-pca/index.html#the-central-tool-of-data-science",
    "title": "Regression vs. PCA",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nIf science is understanding relationships between variables, regression is the most basic but fundamental tool we have to start measuring these relationships\nOften exactly what humans do when we see data!\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nx_data &lt;- seq(from=0, to=1, by=0.02)\nnum_x &lt;- length(x_data)\ny_data &lt;- x_data + runif(num_x, 0, 0.2)\nreg_df &lt;- tibble(x=x_data, y=y_data)\nggplot(reg_df, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n psychology   psychology   trending_flat \n\n\n\n\n\nCode\nggplot(reg_df, aes(x=x, y=y)) + \n  geom_point(size=g_pointsize) +\n  geom_smooth(method = \"lm\", se = FALSE, color = cbPalette[1], formula = y ~ x, linewidth = g_linewidth*3) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-goal",
    "href": "writeups/regression-vs-pca/index.html#the-goal",
    "title": "Regression vs. PCA",
    "section": "The Goal",
    "text": "The Goal\n\nWhenever you carry out a regression, keep the goal in the front of your mind:\n\n\n\n\n\n\n\nThe Goal of Regression\n\n\n\nFind a line \\(\\widehat{y} = mx + b\\) that best predicts \\(Y\\) for given values of \\(X\\)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "href": "writeups/regression-vs-pca/index.html#how-do-we-define-best",
    "title": "Regression vs. PCA",
    "section": "How Do We Define “Best”?",
    "text": "How Do We Define “Best”?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "href": "writeups/regression-vs-pca/index.html#principal-component-analysis",
    "title": "Regression vs. PCA",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPrincipal Component Line can be used to project the data onto its dimension of highest variance\nMore simply: PCA can discover meaningful axes in data (unsupervised learning / exploratory data analysis settings)\n\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\ndist_to_line &lt;- function(x0, y0, a, c) {\n    numer &lt;- abs(a * x0 - y0 + c)\n    denom &lt;- sqrt(a * a + 1)\n    return(numer / denom)\n}\n# Finding PCA line for industrial vs. exports\nx &lt;- gdp_df$industrial\ny &lt;- gdp_df$exports\nlossFn &lt;- function(lineParams, x0, y0) {\n    a &lt;- lineParams[1]\n    c &lt;- lineParams[2]\n    return(sum(dist_to_line(x0, y0, a, c)))\n}\no &lt;- optim(c(0, 0), lossFn, x0 = x, y0 = y)\nggplot(gdp_df, aes(x = industrial, y = exports)) +\n    geom_point(size=g_pointsize/2) +\n    geom_abline(aes(slope = o$par[1], intercept = o$par[2], color=\"pca\"), linewidth=g_linewidth, show.legend = TRUE) +\n    geom_smooth(aes(color=\"lm\"), method = \"lm\", se = FALSE, linewidth=g_linewidth, key_glyph = \"blank\") +\n    scale_color_manual(element_blank(), values=c(\"pca\"=cbPalette[2],\"lm\"=cbPalette[1]), labels=c(\"Regression\",\"PCA\")) +\n    dsan_theme(\"half\") +\n    remove_legend_title() +\n    labs(\n      title = \"PCA Line vs. Regression Line\",\n        x = \"Industrial Production (% of GDP)\",\n        y = \"Exports (% of GDP)\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nSee https://juliasilge.com/blog/un-voting/ for an amazing blog post using PCA, with 2 dimensions, to explore UN voting patterns!"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "href": "writeups/regression-vs-pca/index.html#create-your-own-dimension",
    "title": "Regression vs. PCA",
    "section": "Create Your Own Dimension!",
    "text": "Create Your Own Dimension!\n\n\nCode\nggplot(gdp_df, aes(pc1, .fittedPC2)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(aes(yintercept=0, color='PCA Line'), linetype='solid', size=g_linesize) +\n    geom_rug(sides = \"b\", linewidth=g_linewidth/1.2, length = unit(0.1, \"npc\"), color=cbPalette[3]) +\n    expand_limits(y=-1.6) +\n    scale_color_manual(element_blank(), values=c(\"PCA Line\"=cbPalette[2])) +\n    dsan_theme(\"full\") +\n    remove_legend_title() +\n    labs(\n      title = \"Exports vs. Industrial Production in Principal Component Space\",\n      x = \"First Principal Component (Dimension of Greatest Variance)\",\n      y = \"Second Principal Component\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "href": "writeups/regression-vs-pca/index.html#and-use-it-for-eda",
    "title": "Regression vs. PCA",
    "section": "And Use It for EDA",
    "text": "And Use It for EDA\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nplot_df &lt;- gdp_df %&gt;% select(c(country_code, pc1, agriculture, military))\nlong_df &lt;- plot_df %&gt;% pivot_longer(!c(country_code, pc1), names_to = \"var\", values_to = \"val\")\nlong_df &lt;- long_df |&gt; mutate(\n  var = case_match(\n    var,\n    \"agriculture\" ~ \"Agricultural Production\",\n    \"military\" ~ \"Military Spending\"\n  )\n)\nggplot(long_df, aes(x = pc1, y = val, facet = var)) +\n    geom_point() +\n    facet_wrap(vars(var), scales = \"free\") +\n    dsan_theme(\"full\") +\n    labs(\n        x = \"Industrial-Export Dimension\",\n        y = \"% of GDP\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "href": "writeups/regression-vs-pca/index.html#but-in-our-case",
    "title": "Regression vs. PCA",
    "section": "But in Our Case…",
    "text": "But in Our Case…\n\n\\(x\\) and \\(y\\) dimensions already have meaning, and we have a hypothesis about \\(x \\rightarrow y\\)!\n\n\n\n\n\n\n\nThe Regression Hypothesis \\(\\mathcal{H}_{\\text{reg}}\\)\n\n\n\nGiven data \\((X, Y)\\), we estimate \\(\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1}x\\), hypothesizing that:\n\nStarting from \\(y = \\widehat{\\beta_0}\\) when \\(x = 0\\) (the intercept),\nAn increase of \\(x\\) by 1 unit is associated with an increase of \\(y\\) by \\(\\widehat{\\beta_1}\\) units (the coefficient)\n\n\n\n\nWe want to measure how well our line predicts \\(y\\) for any given \\(x\\) value \\(\\implies\\) vertical distance from regression line"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "href": "writeups/regression-vs-pca/index.html#key-features-of-regression-line",
    "title": "Regression vs. PCA",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the “best” linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-5mm] \\text{intercept}\\end{array}} + \\underbrace{\\widehat{\\beta_1}}_{\\small\\begin{array}{c}\\text{Predicted} \\\\[-4mm] \\text{slope}\\end{array}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\theta = \\left(\\widehat{\\beta_0}, \\widehat{\\beta_1}\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(\\overbrace{\\widehat{y}(x_i)}^{\\small\\text{Predicted }y} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^2 \\right]\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#regression-in-r",
    "href": "writeups/regression-vs-pca/index.html#regression-in-r",
    "title": "Regression vs. PCA",
    "section": "Regression in R",
    "text": "Regression in R\n\n\nCode\nlin_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#lm-syntax",
    "href": "writeups/regression-vs-pca/index.html#lm-syntax",
    "title": "Regression vs. PCA",
    "section": "lm Syntax",
    "text": "lm Syntax\nlm(\n  formula = dependent ~ independent + controls,\n  data = my_df\n)"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#interpreting-output",
    "href": "writeups/regression-vs-pca/index.html#interpreting-output",
    "title": "Regression vs. PCA",
    "section": "Interpreting Output",
    "text": "Interpreting Output\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-coefficients",
    "title": "Regression vs. PCA",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "href": "writeups/regression-vs-pca/index.html#zooming-in-significance",
    "title": "Regression vs. PCA",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest statistic\nHow extreme is test stat?\nStatistical significance\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth) +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"dashed\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"solid\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"dashed\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "href": "writeups/regression-vs-pca/index.html#the-residual-plot",
    "title": "Regression vs. PCA",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: “homoskedasticity”\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(lin_model)\nggplot(gdp_resid_df, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Industrial ~ Military\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#q-q-plot",
    "href": "writeups/regression-vs-pca/index.html#q-q-plot",
    "title": "Regression vs. PCA",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45° line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "href": "writeups/regression-vs-pca/index.html#multiple-linear-regression",
    "title": "Regression vs. PCA",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "writeups/regression-vs-pca/index.html#references",
    "href": "writeups/regression-vs-pca/index.html#references",
    "title": "Regression vs. PCA",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nWeek\n\n\nAssignment\n\n\n\n\n\n\nRegression vs. PCA\n\n\n \n\n\n \n\n\n\n\nMathematical Optimization\n\n\n13\n\n\nQuiz 4\n\n\n\n\nQuiz 1 Study Guide\n\n\n2\n\n\nQuiz 1\n\n\n\n\nGetting Started with Lab 1\n\n\n2\n\n\nLab 1\n\n\n\n\nExtra Slides: A Slightly Deeper Dive Into Machine Learning\n\n\n2\n\n\nGeneral\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Writeups"
    ]
  },
  {
    "objectID": "writeups/lab-1/index.html",
    "href": "writeups/lab-1/index.html",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let’s visualize the objective function one more time, then we’ll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we’re given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)—the differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here—while in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "href": "writeups/lab-1/index.html#visualizing-the-loss-function",
    "title": "Getting Started with Lab 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncb_palette &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n)\ncenter_title &lt;- function(orig_theme) {\n  theme_centered &lt;- orig_theme + theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n  return(theme_centered)\n}\n\n\n\nFirst, let’s visualize the objective function one more time, then we’ll see what the gradient vector (specifically, the gradient vector) tells us about how we should move/update our guess after each step.\nIn this case, we’re given the following loss function \\(L(w)\\): (but, see sidebar on loss functions below!)\n\\[\nL(w) = (w - 10)^2 + 5\n\\]\n\n\n\n\n\n\nWhere Does the Loss Function Come From?\n\n\n\n\n\nThe focus of this assignment is to help you see how numerical methods like gradient descent can use a loss function \\(L\\), along with its first and second derivates (whether exact or approximate), to optimize parameters of a model by finding the minimum of \\(L\\) with respect to these parameters.\nIn Section 01, for example, we looked at the contrived but (imo) useful-for-intuition model of regression without an intercept:\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\nAnd then we saw how, once we choose some particular value \\(b\\) for \\(\\beta_1\\), we can compute how well this model with this parameter setting fits a dataset \\((\\mathbf{x}, \\mathbf{y}) = ((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n))\\) by computing the residual sum of squares (RSS)—the differences between the predictions \\(\\widehat{y}_i = b x_i\\) generated by the model (again, using that choice \\(\\beta_1 = b\\)) and the actual observed values \\(y_i\\):\n\\[\nL(b) = RSS(b) = \\sum_{i=1}^{n}(\\widehat{y}_i(b) - y_i)^2\n\\]\nThis is why using a quadratic function as our starting example of a loss function is useful here—while in practice the loss function is a potentially-complex function of the data \\((\\mathbf{x}, \\mathbf{y})\\) and the model parameters, here we simplify the above RSS computation down to its essence of a quadratic function like \\(L(w) = (w - 10)^2 + 5\\), so that we can explore how a function like this can be optimized via numerical methods.\n\n\n\nThe given loss function \\(L(w)\\) on its own looks as follows:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nloss_fn &lt;- function(w) {\n    # Make predictions using w, then sum the\n    # squared residuals, how good/bad is a line\n    # with slope w\n    return((w - 10)^2 + 5)\n}\nggplot() +\n  stat_function(data=tibble(x=c(0, 10)), fun=loss_fn, linewidth=1) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title = \"Quadratic Loss Function\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "href": "writeups/lab-1/index.html#how-the-derivative-helps-us",
    "title": "Getting Started with Lab 1",
    "section": "How the Derivative Helps Us",
    "text": "How the Derivative Helps Us\nIn this case, our loss function actually has an easily-computed closed-form derivative (though, as mentioned in the info box above, this is usually not the case as we move to more complex models like neural networks). We can use the chain rule \\(\\frac{\\partial}{\\partial x}f(g(w)) = f'(g(w))g'(w)\\) to make our lives easier, letting \\(f(x) = x^2 + 5\\) and \\(g(x) = x - 10\\) and recalling that \\(\\frac{\\partial}{\\partial x}x^2 = 2x\\):\n\\[\nL'(w) = \\frac{\\partial L(w)}{\\partial w} = 2(w - 10)\n\\]\nThe reason this matters / the reason it helps us is as follows. Recall how, in calculus class, we were able to use the derivative as a tool for finding minima and maxima of functions since the minima and maxima of these functions are precisely the values at which the function’s derivative is zero!\nBut what happens if we’re not exactly at a minimum, in this case? Calculus classes usually gloss over this question, since the answer would usually be “Why do we care about points besides these optimal points? We can just compute the minimum and then we’re done! No need to worry about non-optimal points”\nHowever, when we work with more complicated models like neural networks, we don’t necessarily have an exact closed-form solution allowing us to take a derivative, set to zero, and solve. We therefore need to utilize numerical optimization approaches, which use the derivative as information telling us which direction we should move in and (approximately) how much we should move if we want to move from a non-optimal point towards the optimal point.\nLet’s pick three values of \\(w\\):\n\nA value below the minimizing value, \\(w_&lt; = 5\\),\nThe minimizing value itself, \\(w_0 = 10\\), and\nA value above the minimizing value, \\(w_&gt; = 19\\)\n\n\n\nCode\nlibrary(latex2exp)\nloss_deriv &lt;- function(w) {\n    return(2 * (w - 10))\n}\nw_vals &lt;- c(5, 10, 19)\nw_labels &lt;- factor(c(\"wlt\",\"w0\",\"wgt\"), levels=c(\"wlt\",\"w0\",\"wgt\"))\ndata_df &lt;- tibble(w=w_vals, label=w_labels)\ndata_df &lt;- data_df |&gt;\n  mutate(\n    loss = loss_fn(w),\n    deriv = loss_deriv(w),\n    second_deriv = 2\n  )\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=factor(label)),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_color_manual(\n    \"Our Three Values\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$w_&lt; = 5$\"),\"w0\"=TeX(\"$w_0 = 10$\"),\"wgt\"=TeX(\"$w_&gt; = 19$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\n\n\n\n\nAnd let’s evaluate both the loss function itself (loss) as well as the derivative of the loss function (deriv) at each point, looking closely at what these values tell us:\n\n\nCode\ndata_df\n\n\n\n\n\n\nw\nlabel\nloss\nderiv\nsecond_deriv\n\n\n\n\n5\nwlt\n30\n-10\n2\n\n\n10\nw0\n5\n0\n2\n\n\n19\nwgt\n86\n18\n2\n\n\n\n\n\n\nHere we can notice that, when we are at a value below the optimal value like \\(w_&lt;\\), the derivative has a negative sign, whereas at a value above the optimal value like \\(w_&gt;\\) the derivative has a positive sign. This relates to one of the natural interpretations of the derivative, one that your calculus class hopefully talked about, as the slope of the line tangent to the curve at that point. Adding to the previous plot of just the points, we can see this “slope interpretation” in action: while the loss value tells us how high or low we are on the \\(y\\)-axis here, the deriv value tells us how steep the loss function is at this point. If we adopt the convention of drawing the tangent lines (for nonzero slopes) as vectors, we get a picture that looks like:\n\n\nCode\ntangent_at_x0 &lt;- function(x,x0) loss_deriv(x0)*(x - x0) + loss_fn(x0)\ntan_wlt &lt;- function(x) tangent_at_x0(x, data_df$w[1])\ntan_w0 &lt;- function(x) tangent_at_x0(x, data_df$w[2])\ntan_wgt &lt;- function(x) tangent_at_x0(x, data_df$w[3])\nslopes &lt;- round(c(\n  data_df$deriv[1],\n  data_df$deriv[2],\n  data_df$deriv[3]\n), 3)\nggplot() +\n  stat_function(fun=loss_fn, linewidth=1) +\n  geom_function(\n    fun=tan_wlt, aes(color=data_df$label[1]), linewidth=1,\n    xlim=c(0,7.5), arrow = arrow(length=unit(0.30,\"cm\"))\n  ) +\n  geom_function(\n    fun=tan_w0, aes(color=data_df$label[2]), linewidth=1,\n    xlim=c(8,12)\n  ) +\n  geom_function(\n    fun=tan_wgt, aes(color=data_df$label[3]), linewidth=1,\n    xlim=c(14.5,20), arrow = arrow(length=unit(0.30,\"cm\"), ends=\"first\")\n  ) +\n  geom_point(\n    data=data_df,\n    aes(x=w, y=loss, color=label),\n    size=3\n  ) +\n  xlim(0, 20) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at w\",\n    values=c(\"wlt\"=cb_palette[1], \"w0\"=cb_palette[2], \"wgt\"=cb_palette[3]),\n    labels=c(\"wlt\"=TeX(\"$L'(w_&lt;) = -10$\"),\"w0\"=TeX(\"$L'(w_0) = 0$\"),\"wgt\"=TeX(\"$L'(w_&gt;) = 18$\"))\n  ) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  labs(\n    title=TeX(\"Points At, Below, and Above the Optimal $w^*$\"),\n    x = \"Parameter Value (w)\",\n    y = \"Loss L(w)\"\n  )\n\n\n\n\n\nThe derivatives “point” in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nAnd this shows us exactly why computing the derivative at a point away from the optimal value still helps us when it comes to numerical optimization, in two ways:\n\nFirst, the derivative at these points (literally) points us in the direction we should go in if we want to move towards the optimal value.\nThen (though I’ll stop after this and let you see the effect of this point by going through the assignment’s different parts, since it relates to the second derivative rather than the first), notice also how the monotonicity properties of the quadratic function \\(f(x) = x^2\\) also tells us “how wrong” we are, in a sense:\n\n\\(w_&gt; = 19\\) is further away from the optimal point than \\(w_&lt; = 5\\), therefore\nThe magnitude of the derivative at \\(w_&gt;\\) (18) is greater than the magnitude of the derivative at \\(w_&lt;\\) (10).\n\n\nLike I mentioned, I’m stopping here since the later portions of the assignment dive into the information that the second derivative at a point can provide for our numerical optimizer, but to test your understanding you can imagine writing a third section here titled “How The (Second) Derivative Helps Us”.\nFor example, the Huber Loss is often used as an alternative to both “pure” quadratic loss and “pure” absolute loss functions because it penalizes outliers less harshly than \\(f(x) = x^2\\) but more harshly than \\(f(x) = |x|\\). The following plot illustrates a “Huberized” version of Lab 1’s loss function, where values within \\(\\delta = 4\\) units of the optimal value \\(w^* = 10\\) are penalized quadratically, but values more than 4 units away from the optimal value are penalized linearly. Think through what is happening to the second derivative as we move from left to right here (relative to quadratic loss with \\(L''(w) = 2\\) and absolute loss with \\(L''(w) = 1\\)):\n\n\nCode\nabs_loss &lt;- function(w) {\n  return(abs(w - 10) + 5)\n}\ndelta &lt;- 4\nhuberized_loss &lt;- function(w) {\n  cases_result &lt;- ifelse(\n    abs(w - 10) &lt;= delta,\n    (1/2)*(w - 10)^2,\n    delta * (abs(w-10) - (1/2)*delta)\n  )\n  return(cases_result + 5)\n}\ntext_df &lt;- tibble::tribble(\n  ~x, ~y, ~label,\n  4, 100, \"← Linear\",\n  10, 100, \"Quadratic\",\n  16, 100, \"Linear →\"\n)\nggplot() +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=abs_loss,\n    aes(color='Absolute'),\n    linewidth=0.5\n  ) +\n  stat_function(\n    data=tibble(x=c(0, 10)),\n    fun=huberized_loss,\n    aes(color='Huber'),\n    linewidth=1\n  ) +\n  stat_function(\n    data=tibble(x=c(0,10)),\n    fun=loss_fn,\n    aes(color='Quadratic'),\n    linewidth=0.5\n  ) +\n  geom_vline(\n    xintercept=10 - delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_vline(\n    xintercept=10+delta,\n    linetype=\"dashed\",\n    linewidth=1,\n    color=cb_palette[2]\n  ) +\n  geom_text(\n    data=text_df,\n    aes(x=x, y=y, label=label),\n    color=cb_palette[2]\n  ) +\n  xlim(0, 20) +\n  theme_classic(base_size=14) |&gt; center_title() +\n  scale_color_manual(\n    \"Loss Functions\",\n    values=c('Absolute'=cb_palette[1], 'Huber'=cb_palette[2], 'Quadratic'=cb_palette[3])\n    #labels=c('Absolute', 'Huber', 'Quadratic')\n  ) +\n  labs(\n    title = \"Loss Functions: Huber vs. 'Pure' Squared or Absolute\",\n    x = \"Parameter (w)\",\n    y = \"Loss at w\"\n  )"
  },
  {
    "objectID": "w02/slides.html#how-do-we-define-best",
    "href": "w02/slides.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define “Best”?",
    "text": "How Do We Define “Best”?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!"
  },
  {
    "objectID": "w02/slides.html#predictive-example-advertising-effects",
    "href": "w02/slides.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#explanatory-example-industrialization-effects",
    "href": "w02/slides.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot"
  },
  {
    "objectID": "w02/slides.html#simple-linear-regression",
    "href": "w02/slides.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we’ll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe’ve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we’d like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the “least wrong” predictions: motivates focus on residuals…"
  },
  {
    "objectID": "w02/slides.html#least-squares-minimizing-residuals",
    "href": "w02/slides.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217"
  },
  {
    "objectID": "w02/slides.html#why-not-absolute-value",
    "href": "w02/slides.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we’re aiming to minimize these residuals…\nGhost of calculus past 😱: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )"
  },
  {
    "objectID": "w02/slides.html#outliers-penalized-quadratically",
    "href": "w02/slides.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#key-features-of-regression-line",
    "href": "w02/slides.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the “best” linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]"
  },
  {
    "objectID": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/slides.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\nImage Source"
  },
  {
    "objectID": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/slides.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But… What About All the Other Types of Vars?",
    "text": "But… What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs. continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like “I want to do [regression / classification], but my data is [not cardinal+continuous]”"
  },
  {
    "objectID": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/slides.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )"
  },
  {
    "objectID": "w02/slides.html#evaluating-with-residuals",
    "href": "w02/slides.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\nrc2_resid_plot"
  },
  {
    "objectID": "w02/slides.html#now-the-math",
    "href": "w02/slides.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]"
  },
  {
    "objectID": "w02/slides.html#regression-r-vs.-statsmodels",
    "href": "w02/slides.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs. statsmodels",
    "text": "Regression: R vs. statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)"
  },
  {
    "objectID": "w02/slides.html#interpreting-output",
    "href": "w02/slides.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106"
  },
  {
    "objectID": "w02/slides.html#zooming-in-coefficients",
    "href": "w02/slides.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]"
  },
  {
    "objectID": "w02/slides.html#zooming-in-significance",
    "href": "w02/slides.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))"
  },
  {
    "objectID": "w02/slides.html#the-residual-plot",
    "href": "w02/slides.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: “homoskedasticity”\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )"
  },
  {
    "objectID": "w02/slides.html#q-q-plot",
    "href": "w02/slides.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45° line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )"
  },
  {
    "objectID": "w02/slides.html#multiple-linear-regression",
    "href": "w02/slides.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant"
  },
  {
    "objectID": "w02/slides.html#visualizing-multiple-linear-regression",
    "href": "w02/slides.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided."
  },
  {
    "objectID": "w02/slides.html#interpreting-mlr",
    "href": "w02/slides.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant…\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant…\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units"
  },
  {
    "objectID": "w02/slides.html#but-wait",
    "href": "w02/slides.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait…",
    "text": "But Wait…\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It’s how we’re able to control for confounding variables!"
  },
  {
    "objectID": "w02/slides.html#correlations-among-features",
    "href": "w02/slides.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher…\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets…\nIn SLR which only examines sales vs. newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales…\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper “gets credit” for the association between radio and sales"
  },
  {
    "objectID": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/slides.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression 😎"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\n\n\n\n\nDSAN 5300-01 Week 2: Linear Regression"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Linear Regression",
    "section": "",
    "text": "Open slides in new tab →",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#how-do-we-define-best",
    "href": "w02/index.html#how-do-we-define-best",
    "title": "Week 2: Linear Regression",
    "section": "How Do We Define “Best”?",
    "text": "How Do We Define “Best”?\n\nIntuitively, two different ways to measure how well a line fits the data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(5321)\nN &lt;- 11\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.2)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\ndf &lt;- tibble(x = x, y = y, spread = spread)\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=(x+y)/2, yend=(x+y)/2, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Principal Component Line\"\n  )\nggplot(df, aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth*2) +\n  geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_point(size=g_pointsize) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the difference between these two lines, and why it matters, I cannot recommend Gelman and Hill (2007) enough!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#predictive-example-advertising-effects",
    "href": "w02/index.html#predictive-example-advertising-effects",
    "title": "Week 2: Linear Regression",
    "section": "Predictive Example: Advertising Effects",
    "text": "Predictive Example: Advertising Effects\n\nIndependent variable: $ put into advertisements\nDependent variable: Sales\nGoal: Figure out a good way to allocate an advertising budget\n\n\n\nCode\nlibrary(tidyverse)\nad_df &lt;- read_csv(\"assets/Advertising.csv\") |&gt; rename(id=`...1`)\n\n\nNew names:\nRows: 200 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nCode\nlong_df &lt;- ad_df |&gt; pivot_longer(-c(id, sales), names_to=\"medium\", values_to=\"allocation\")\nlong_df |&gt; ggplot(aes(x=allocation, y=sales)) +\n  geom_point() +\n  facet_wrap(vars(medium), scales=\"free_x\") +\n  geom_smooth(method='lm', formula=\"y ~ x\") +\n  theme_dsan() +\n  labs(\n    x = \"Allocation ($1K)\",\n    y = \"Sales (1K Units)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#explanatory-example-industrialization-effects",
    "href": "w02/index.html#explanatory-example-industrialization-effects",
    "title": "Week 2: Linear Regression",
    "section": "Explanatory Example: Industrialization Effects",
    "text": "Explanatory Example: Industrialization Effects\n\n\nCode\nlibrary(tidyverse)\ngdp_df &lt;- read_csv(\"assets/gdp_pca.csv\")\n\n\nRows: 89 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): country_code, country_name\ndbl (12): .rownames, services, agriculture, industrial, manufacturing, resou...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmil_plot &lt;- gdp_df |&gt; ggplot(aes(x=industrial, y=military)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Military Exports vs. Industrialization\",\n    x=\"Industrial Production (% of GDP)\",\n    y=\"Military Exports (% of All Exports)\"\n  )\nmil_plot\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#simple-linear-regression",
    "href": "w02/index.html#simple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nFor now, we treat Newspaper, Radio, TV advertising separately: how much do sales increase per $1 into [medium]? (Later we’ll consider them jointly: multiple regression)\nOur model:\n\\[\nY = \\underbrace{\\param{\\beta_0}}_{\\mathclap{\\text{Intercept}}} + \\underbrace{\\param{\\beta_1}}_{\\mathclap{\\text{Slope}}}X + \\varepsilon\n\\]\nThis model generates predictions via\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta_0}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta_1}}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nNote how these predictions will be wrong (unless the data is perfectly linear)\nWe’ve accounted for this in our model (by including \\(\\varepsilon\\) term)!\nBut, we’d like to find estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that produce the “least wrong” predictions: motivates focus on residuals…",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#least-squares-minimizing-residuals",
    "href": "w02/index.html#least-squares-minimizing-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Least Squares: Minimizing Residuals",
    "text": "Least Squares: Minimizing Residuals\nWhat can we optimize to ensure these residuals are as small as possible?\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.25)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_lg_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_lg_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nlarge_sum &lt;- sum(sim_lg_df$spread)\nwriteLines(fmt_decimal(large_sum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nlarge_sqsum &lt;- sum((sim_lg_df$spread)^2)\nwriteLines(fmt_decimal(large_sqsum))\n\n\n3.8405017200\n\n\n\n\n\n\n\n\nCode\nN &lt;- 21\nx &lt;- seq(from = 0, to = 1, by = 1 / (N - 1))\ny &lt;- x + rnorm(N, 0, 0.05)\nmean_y &lt;- mean(y)\nspread &lt;- y - mean_y\nsim_sm_df &lt;- tibble(x = x, y = y, spread = spread)\nsim_sm_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=cbPalette[1], linewidth=g_linewidth) +\n  # geom_segment(xend=x, yend=x, linewidth=g_linewidth*2, color=cbPalette[2]) +\n  geom_segment(aes(xend=x, yend=x, color=ifelse(y&gt;x,\"Positive\",\"Negative\")), linewidth=1.5*g_linewidth) +\n  geom_point(size=g_pointsize) +\n  # coord_equal() +\n  theme_dsan(\"half\") +\n  scale_color_manual(\"Spread\", values=c(\"Positive\"=cbPalette[3],\"Negative\"=cbPalette[6]), labels=c(\"Positive\"=\"Positive\",\"Negative\"=\"Negative\")) +\n  labs(\n    title = \"Regression Line\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSum?\n\n\n\nCode\nsmall_rsum &lt;- sum(sim_sm_df$spread)\nwriteLines(fmt_decimal(small_rsum))\n\n\n0.0000000000\n\n\n\nSum of Squares?\n\n\n\nCode\nsmall_sqrsum &lt;- sum((sim_sm_df$spread)^2)\nwriteLines(fmt_decimal(small_sqrsum))\n\n\n1.9748635217",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#why-not-absolute-value",
    "href": "w02/index.html#why-not-absolute-value",
    "title": "Week 2: Linear Regression",
    "section": "Why Not Absolute Value?",
    "text": "Why Not Absolute Value?\n\nTwo feasible ways to prevent positive and negative residuals cancelling out:\n\nAbsolute value \\(\\left|y - \\widehat{y}\\right|\\) or squaring \\(\\left( y - \\widehat{y} \\right)^2\\)\n\nBut remember that we’re aiming to minimize these residuals…\nGhost of calculus past 😱: which is differentiable everywhere?\n\n\n\n\n\nCode\nlibrary(latex2exp)\nx2_label &lt;- latex2exp(\"$f(x) = x^2$\")\n\n\nWarning in latex2exp(\"$f(x) = x^2$\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ .x^2, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=x2_label,\n    y=\"f(x)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Could use facet_grid() here, but it doesn't work too nicely with stat_function() :(\nggplot(data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=~ abs(.x), linewidth=g_linewidth) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title=\"f(x) = |x|\",\n    y=\"f(x)\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#outliers-penalized-quadratically",
    "href": "w02/index.html#outliers-penalized-quadratically",
    "title": "Week 2: Linear Regression",
    "section": "Outliers Penalized Quadratically",
    "text": "Outliers Penalized Quadratically\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#key-features-of-regression-line",
    "href": "w02/index.html#key-features-of-regression-line",
    "title": "Week 2: Linear Regression",
    "section": "Key Features of Regression Line",
    "text": "Key Features of Regression Line\n\nRegression line is BLUE: Best Linear Unbiased Estimator\nWhat exactly is it the “best” linear estimator of?\n\n\\[\n\\widehat{y} = \\underbrace{\\widehat{\\beta}_0}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-5mm] \\text{intercept}\\end{array}}} ~+~ \\underbrace{\\widehat{\\beta}_1}_{\\mathclap{\\small\\begin{array}{c}\\text{Estimated} \\\\[-4mm] \\text{slope}\\end{array}}}\\cdot x\n\\]\nis chosen so that\n\n\\[\n\\widehat{\\theta} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\right) = \\argmin_{\\beta_0, \\beta_1}\\left[ \\sum_{x_i \\in X} \\left(~~\\overbrace{\\widehat{y}(x_i)}^{\\mathclap{\\small\\text{Predicted }y}} - \\overbrace{\\expect{Y \\mid X = x_i}}^{\\small \\text{Avg. }y\\text{ when }x = x_i}\\right)^{2~} \\right]\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "href": "w02/index.html#where-did-that-mathbbey-mid-x-x_i-come-from",
    "title": "Week 2: Linear Regression",
    "section": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?",
    "text": "Where Did That \\(\\mathbb{E}[Y \\mid X = x_i]\\) Come From?\n\n\n\nImage Source",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "href": "w02/index.html#but-what-about-all-the-other-types-of-vars",
    "title": "Week 2: Linear Regression",
    "section": "But… What About All the Other Types of Vars?",
    "text": "But… What About All the Other Types of Vars?\n\n5000: you saw, e.g., nominal, ordinal, cardinal vars\n5100: you wrestled with discrete vs. continuous RVs\nGood News #1: Regression can handle all these types+more!\nGood News #2: Distinctions between classification and regression start to diminish as you learn fancier regression methods! (One key tool here: link functions)\nBy end of 5300 you should have something on your toolbelt for handling most cases like “I want to do [regression / classification], but my data is [not cardinal+continuous]”",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "href": "w02/index.html#a-sketch-hw-is-the-full-thing",
    "title": "Week 2: Linear Regression",
    "section": "A Sketch (HW is the Full Thing)",
    "text": "A Sketch (HW is the Full Thing)\n\nOLS for regression without intercept \\(\\param{\\beta_0}\\): Which line through origin best predicts \\(Y\\)?\n(Good practice + reminder of how restricted linear models are!)\n\n\\[\nY = \\beta_1 X + \\varepsilon\n\\]\n\n\nCode\nlibrary(latex2exp)\nset.seed(5300)\n# rand_slope &lt;- log(runif(80, min=0, max=1))\n# rand_slope[41:80] &lt;- -rand_slope[41:80]\n# rand_lines &lt;- tibble::tibble(\n#   id=1:80, slope=rand_slope, intercept=0\n# )\n# angles &lt;- runif(100, -pi/2, pi/2)\nangles &lt;- seq(from=-pi/2, to=pi/2, length.out=50)\npossible_lines &lt;- tibble::tibble(\n  slope=tan(angles), intercept=0\n)\nnum_points &lt;- 30\nx_vals &lt;- runif(num_points, 0, 1)\ny0_vals &lt;- 0.5 * x_vals + 0.25\ny_noise &lt;- rnorm(num_points, 0, 0.07)\ny_vals &lt;- y0_vals + y_noise\nrand_df &lt;- tibble::tibble(x=x_vals, y=y_vals)\ntitle_exp &lt;- latex2exp(\"Parameter Space ($\\\\beta_1$)\")\n\n\nWarning in latex2exp(\"Parameter Space ($\\\\beta_1$)\"): 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\n# Main plot object\ngen_lines_plot &lt;- function(point_size=2.5) {\n  lines_plot &lt;- rand_df |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=point_size) +\n    geom_hline(yintercept=0, linewidth=1.5) +\n    geom_vline(xintercept=0, linewidth=1.5) +\n    # Point at origin\n    geom_point(data=data.frame(x=0, y=0), aes(x=x, y=y), size=4) +\n    xlim(-1,1) +\n    ylim(-1,1) +\n    # coord_fixed() +\n    theme_dsan_min(base_size=28)\n  return(lines_plot)\n}\nmain_lines_plot &lt;- gen_lines_plot()\nmain_lines_plot +\n  # Parameter space of possible lines\n  geom_abline(\n    data=possible_lines,\n    aes(slope=slope, intercept=intercept, color='possible'),\n    # linetype=\"dotted\",\n    # linewidth=0.75,\n    alpha=0.25\n  ) +\n  # True DGP\n  geom_abline(\n    aes(\n      slope=0.5,\n      intercept=0.25,\n      color='true'\n    ), linewidth=1, alpha=0.8\n  ) + \n  scale_color_manual(\n    element_blank(),\n    values=c('possible'=\"black\", 'true'=cb_palette[2]),\n    labels=c('possible'=\"Possible Fits\", 'true'=\"True DGP\")\n  ) +\n  remove_legend_title() +\n  labs(\n    title=title_exp\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#evaluating-with-residuals",
    "href": "w02/index.html#evaluating-with-residuals",
    "title": "Week 2: Linear Regression",
    "section": "Evaluating with Residuals",
    "text": "Evaluating with Residuals\n\n\n\n\nCode\nrc1_df &lt;- possible_lines |&gt; slice(n() - 14)\n# Predictions for this choice\nrc1_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc1_df$slope * x,\n  resid = y - y_pred\n)\nrc1_label &lt;- latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \",round(rc1_df$slope, 3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 1: $\\\\beta_1 \\\\approx \", round(rc1_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc1_lines_plot +\n  geom_abline(\n    data=rc1_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[1]\n  ) +\n  geom_segment(\n    data=rc1_pred_df,\n    aes(x=x, y=y, xend=x, yend=y_pred),\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title = rc1_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngen_resid_plot &lt;- function(pred_df) {\n  rc_rss &lt;- sum((pred_df$resid)^2)\n  rc_resid_label &lt;- latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \",round(rc_rss,3)))\n  rc_resid_plot &lt;- pred_df |&gt; ggplot(aes(x=x, y=resid)) +\n    geom_point(size=5) +\n    geom_hline(\n      yintercept=0,\n      color=cb_palette[1],\n      linewidth=1.5\n    ) +\n    geom_segment(\n      aes(xend=x, yend=0)\n    ) +\n    theme_dsan(base_size=28) +\n    theme(axis.line.x = element_blank()) +\n    labs(\n      title=rc_resid_label\n    )\n  return(rc_resid_plot)\n}\nrc1_resid_plot &lt;- gen_resid_plot(rc1_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc1_resid_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_df &lt;- possible_lines |&gt; slice(n() - 9)\n# Predictions for this choice\nrc2_pred_df &lt;- rand_df |&gt; mutate(\n  y_pred = rc2_df$slope * x,\n  resid = y - y_pred\n)\nrc2_label &lt;- latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \",round(rc2_df$slope,3),\"$\"))\n\n\nWarning in latex2exp(paste0(\"Estimate 2: $\\\\beta_1 \\\\approx \", round(rc2_df$slope, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_lines_plot &lt;- gen_lines_plot(point_size=5)\nrc2_lines_plot +\n  geom_abline(\n    data=rc2_df,\n    aes(intercept=intercept, slope=slope),\n    linewidth=2,\n    color=cb_palette[3]\n  ) +\n  geom_segment(\n    data=rc2_pred_df,\n    aes(\n      x=x, y=y, xend=x,\n      yend=ifelse(y_pred &lt;= 1, y_pred, Inf)\n    )\n    # color=cb_palette[1]\n  ) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(\n    title=rc2_label\n  )\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrc2_resid_plot &lt;- gen_resid_plot(rc2_pred_df)\n\n\nWarning in latex2exp(paste0(\"Residuals: RSS $\\\\approx$ \", round(rc_rss, : 'latex2exp' is deprecated.\nUse 'TeX' instead.\nSee help(\"Deprecated\") and help(\"latex2exp-deprecated\").\n\n\nCode\nrc2_resid_plot",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#now-the-math",
    "href": "w02/index.html#now-the-math",
    "title": "Week 2: Linear Regression",
    "section": "Now the Math",
    "text": "Now the Math\n\\[\n\\begin{align*}\n\\beta_1^* = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\widehat{y}_i - y_i)^2 \\right] = \\argmin_{\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right]\n\\end{align*}\n\\]\nWe can compute this derivative to obtain:\n\\[\n\\frac{\\partial}{\\partial\\beta_1}\\left[ \\sum_{i=1}^{n}(\\beta_1x_i - y_i)^2 \\right] = \\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\beta_1}(\\beta_1x_i - y_i)^2 = \\sum_{i=1}^{n}2(\\beta_1x_i - y_i)x_i\n\\]\nAnd our first-order condition means that:\n\\[\n\\sum_{i=1}^{n}2(\\beta_1^*x_i - y_i)x_i = 0 \\iff \\beta_1^*\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i \\iff \\boxed{\\beta_1^* = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2}}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#regression-r-vs.-statsmodels",
    "href": "w02/index.html#regression-r-vs.-statsmodels",
    "title": "Week 2: Linear Regression",
    "section": "Regression: R vs. statsmodels",
    "text": "Regression: R vs. statsmodels\n\n\n\nIn (Base) R: lm()\n\n\n\nCode\nlin_model &lt;- lm(sales ~ TV, data=ad_df)\nsummary(lin_model)\n\n\n\nCall:\nlm(formula = sales ~ TV, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nGeneral syntax:\nlm(\n  dependent ~ independent + controls,\n  data = my_df\n)\n\n\nIn Python: smf.ols()\n\n\n\nCode\nimport statsmodels.formula.api as smf\nresults = smf.ols(\"sales ~ TV\", data=ad_df).fit()\nprint(results.summary(slim=True))\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nNo. Observations:                 200   F-statistic:                     312.1\nCovariance Type:            nonrobust   Prob (F-statistic):           1.47e-42\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nGeneral syntax:\nsmf.ols(\n  \"dependent ~ independent + controls\",\n  data = my_df\n)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-output",
    "href": "w02/index.html#interpreting-output",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting Output",
    "text": "Interpreting Output\n\n\n\n\nCode\nmil_plot + theme_dsan(\"quarter\")\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngdp_model &lt;- lm(military ~ industrial, data=gdp_df)\nsummary(gdp_model)\n\n\n\nCall:\nlm(formula = military ~ industrial, data = gdp_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3354 -1.0997 -0.3870  0.6081  6.7508 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.61969    0.59526   1.041   0.3010  \nindustrial   0.05253    0.02019   2.602   0.0111 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.671 on 79 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.07895,   Adjusted R-squared:  0.06729 \nF-statistic: 6.771 on 1 and 79 DF,  p-value: 0.01106",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-coefficients",
    "href": "w02/index.html#zooming-in-coefficients",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Coefficients",
    "text": "Zooming In: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\\[\n\\widehat{y} \\approx \\class{cb1}{\\overset{\\beta_0}{\\underset{\\small \\pm 0.595}{0.620}}} +  \\class{cb2}{\\overset{\\beta_1}{\\underset{\\small \\pm 0.020}{0.053}}} \\cdot x\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#zooming-in-significance",
    "href": "w02/index.html#zooming-in-significance",
    "title": "Week 2: Linear Regression",
    "section": "Zooming In: Significance",
    "text": "Zooming In: Significance\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n\n(Intercept)\n0.61969\n0.59526\n1.041\n0.3010\n\n\n\nindustrial\n0.05253\n0.02019\n2.602\n0.0111\n*\n\n\n\n\\(\\widehat{\\beta}\\)\nUncertainty\nTest stat \\(t\\)\nHow extreme is \\(t\\)?\nSignif. Level\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nint_tstat &lt;- 1.041\nint_tstat_str &lt;- sprintf(\"%.02f\", int_tstat)\nlabel_df_int &lt;- tribble(\n    ~x, ~y, ~label,\n    0.25, 0.05, paste0(\"P(t &gt; \",int_tstat_str,\")\\n= 0.3\")\n)\nlabel_df_signif_int &lt;- tribble(\n    ~x, ~y, ~label,\n    2.7, 0.075, \"95% Signif.\\nCutoff\"\n)\nfuncShaded &lt;- function(x, lower_bound, upper_bound){\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedIntercept &lt;- function(x) funcShaded(x, int_tstat, Inf)\nfuncShadedSignif &lt;- function(x) funcShaded(x, 1.96, Inf)\nggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=int_tstat), linewidth=g_linewidth, linetype=\"dashed\") +\n  geom_vline(aes(xintercept = 1.96), linewidth=g_linewidth, linetype=\"solid\") +\n  stat_function(fun = funcShadedIntercept, geom = \"area\", fill = cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  geom_text(label_df_int, mapping = aes(x = x, y = y, label = label), size = 10) +\n  geom_text(label_df_signif_int, mapping = aes(x = x, y = y, label = label), size = 8) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-2, 0, int_tstat, 2), labels=c(\"-2\",\"0\",int_tstat_str,\"2\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Intercept\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", cbPalette[1], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\nℹ Results may be unexpected or may change in future versions of ggplot2.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ncoef_tstat &lt;- 2.602\ncoef_tstat_str &lt;- sprintf(\"%.02f\", coef_tstat)\nlabel_df_coef &lt;- tribble(\n    ~x, ~y, ~label,\n    3.65, 0.06, paste0(\"P(t &gt; \",coef_tstat_str,\")\\n= 0.01\")\n)\nlabel_df_signif_coef &lt;- tribble(\n  ~x, ~y, ~label,\n  1.05, 0.03, \"95% Signif.\\nCutoff\"\n)\nfuncShadedCoef &lt;- function(x) funcShaded(x, coef_tstat, Inf)\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=dnorm, linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=coef_tstat), linetype=\"dashed\", linewidth=g_linewidth) +\n  geom_vline(aes(xintercept=1.96), linetype=\"solid\", linewidth=g_linewidth) +\n  stat_function(fun = funcShadedCoef, geom = \"area\", fill = cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedSignif, geom = \"area\", fill = \"grey\", alpha = 0.333) +\n  # Label shaded area\n  geom_text(label_df_coef, mapping = aes(x = x, y = y, label = label), size = 10) +\n  # Label significance cutoff\n  geom_text(label_df_signif_coef, mapping = aes(x = x, y = y, label = label), size = 8) +\n  coord_cartesian(clip = \"off\") +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(-4, -2, 0, 2, coef_tstat, 4), labels=c(\"-4\", \"-2\",\"0\", \"2\", coef_tstat_str,\"4\")) +\n  dsan_theme(\"quarter\") +\n  labs(\n    title = \"t Value for Coefficient\",\n    x = \"t\",\n    y = \"Density\"\n  ) +\n  theme(axis.text.x = element_text(colour = c(\"black\", \"black\", \"black\", \"black\", cbPalette[2], \"black\")))\n\n\nWarning: Vectorized input to `element_text()` is not officially supported.\nℹ Results may be unexpected or may change in future versions of ggplot2.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-residual-plot",
    "href": "w02/index.html#the-residual-plot",
    "title": "Week 2: Linear Regression",
    "section": "The Residual Plot",
    "text": "The Residual Plot\n\n\n\nA key assumption required for OLS: “homoskedasticity”\nGiven our model \\[\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\n\\] the errors \\(\\varepsilon_i\\) should not vary systematically with \\(i\\)\nFormally: \\(\\forall i \\left[ \\Var{\\varepsilon_i} = \\sigma^2 \\right]\\)\n\n\n\n\nCode\nlibrary(broom)\ngdp_resid_df &lt;- augment(gdp_model)\nggplot(gdp_resid_df, aes(x = industrial, y = .resid)) +\n    geom_point(size = g_pointsize/2) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n      title = \"Residual Plot for Military ~ Industrial\",\n      x = \"Fitted Value\",\n      y = \"Residual\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- 1:80\nerrors &lt;- rnorm(length(x), 0, x^2/1000)\ny &lt;- x + errors\nhet_model &lt;- lm(y ~ x)\ndf_het &lt;- augment(het_model)\nggplot(df_het, aes(x = .fitted, y = .resid)) +\n    geom_point(size = g_pointsize / 2) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    dsan_theme(\"quarter\") +\n    labs(\n        title = \"Residual Plot for Heteroskedastic Data\",\n        x = \"Fitted Value\",\n        y = \"Residual\"\n    )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#q-q-plot",
    "href": "w02/index.html#q-q-plot",
    "title": "Week 2: Linear Regression",
    "section": "Q-Q Plot",
    "text": "Q-Q Plot\n\nIf \\((\\widehat{y} - y) \\sim \\mathcal{N}(0, \\sigma^2)\\), points would lie on 45° line:\n\n\n\n\n\nCode\nggplot(df_het, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Heteroskedastic Data\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(gdp_resid_df, aes(sample=.resid)) +\n  stat_qq(size = g_pointsize/2) + stat_qq_line(linewidth = g_linewidth) +\n  dsan_theme(\"half\") +\n  labs(\n    title = \"Q-Q Plot for Industrial ~ Military Residuals\",\n    x = \"Normal Distribution Quantiles\",\n    y = \"Observed Data Quantiles\"\n  )",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#multiple-linear-regression",
    "href": "w02/index.html#multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nNotation: \\(x_{i,j}\\) = value of independent variable \\(j\\) for person/observation \\(i\\)\n\\(M\\) = total number of independent variables\n\n\\[\n\\widehat{y}_i = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\cdots + \\beta_M x_{i,M}\n\\]\n\n\\(\\beta_j\\) interpretation: a one-unit increase in \\(x_{i,j}\\) is associated with a \\(\\beta_j\\) unit increase in \\(y_i\\), holding all other independent variables constant",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-multiple-linear-regression",
    "href": "w02/index.html#visualizing-multiple-linear-regression",
    "title": "Week 2: Linear Regression",
    "section": "Visualizing Multiple Linear Regression",
    "text": "Visualizing Multiple Linear Regression\n\n\n\n(ISLR, Fig 3.5): A pronounced non-linear relationship. Positive residuals (visible above the surface) tend to lie along the 45-degree line, where budgets are split evenly. Negative residuals (most not visible) tend to be away from this line, where budgets are more lopsided.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#interpreting-mlr",
    "href": "w02/index.html#interpreting-mlr",
    "title": "Week 2: Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\n\n\n\n\nCode\nmlr_model &lt;- lm(sales ~ TV + radio + newspaper, data=ad_df)\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHolding radio and newspaper spending constant…\n\nAn increase of $1K in spending on TV advertising is associated with\nAn increase in sales of about 46 units\n\nHolding TV and newspaper spending constant…\n\nAn increase of $1K in spending on radio advertising is associated with\nAn increase in sales of about 189 units",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#but-wait",
    "href": "w02/index.html#but-wait",
    "title": "Week 2: Linear Regression",
    "section": "But Wait…",
    "text": "But Wait…\n\n\n\n\nCode\nprint(summary(mlr_model))\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = ad_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nlr_model &lt;- lm(sales ~ newspaper, data=ad_df)\nprint(summary(lr_model))\n\n\n\nCall:\nlm(formula = sales ~ newspaper, data = ad_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n\n\nMLR results can be drastically different from SLR results, because of correlations (next slide)\nThis is a good thing! It’s how we’re able to control for confounding variables!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#correlations-among-features",
    "href": "w02/index.html#correlations-among-features",
    "title": "Week 2: Linear Regression",
    "section": "Correlations Among Features",
    "text": "Correlations Among Features\n\n\nCode\ncor(ad_df |&gt; select(-id))\n\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\n\nObserve how \\(\\text{cor}(\\texttt{radio}, \\texttt{newspaper}) \\approx 0.35\\)\nIn markets where we spend more on radio our sales will tend to be higher…\nCorr matrix \\(\\implies\\) we spend more on newspaper in those same markets…\nIn SLR which only examines sales vs. newspaper, we (correctly!) observe that higher values of newspaper are associated with higher values of sales…\nIn essence, newspaper advertising is a surrogate for radio advertising \\(\\implies\\) in our SLR, newspaper “gets credit” for the association between radio and sales",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "href": "w02/index.html#another-mlr-superpower-incorporating-categorical-vars",
    "title": "Week 2: Linear Regression",
    "section": "Another MLR Superpower: Incorporating Categorical Vars",
    "text": "Another MLR Superpower: Incorporating Categorical Vars\n(Preview for next week)\n\n\n\\[\nY = \\beta_0 + \\beta_1 \\times \\texttt{income}\n\\]\n\n\nCode\ncredit_df &lt;- read_csv(\"assets/Credit.csv\")\n\n\nRows: 400 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Own, Student, Married, Region\ndbl (7): Income, Limit, Rating, Cards, Age, Education, Balance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncredit_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\ncredit_plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nY = &\\beta_0 + \\beta_1 \\times \\texttt{income} + \\beta_2 \\times \\texttt{Student} \\\\\n&+ \\beta_3 \\times (\\texttt{Student} \\times \\texttt{Income})\n\\end{align*}\n\\]\n\n\nCode\nstudent_plot &lt;- credit_df |&gt; ggplot(aes(x=Income, y=Balance, color=Student)) +\n  geom_point(size=0.5*g_pointsize) +\n  geom_smooth(method='lm', formula=\"y ~ x\", linewidth=1) +\n  theme_dsan() +\n  labs(\n    title=\"Credit Card Balance vs. Income Level\",\n    x=\"Income ($1K)\",\n    y=\"Credit Card Balance ($)\"\n  )\nstudent_plot\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the \\(\\texttt{Student} \\times \\texttt{Income}\\) term?\nUnderstanding this setup will open up a vast array of possibilities for regression 😎",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Linear Regression",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "writeups/quiz-1/index.html",
    "href": "writeups/quiz-1/index.html",
    "title": "Quiz 1 Study Guide",
    "section": "",
    "text": "Hello DSAN 5300 Section 01 friends!\nI apologize for the lack of info yall have going into the first Quiz for the course tomorrow. I should have sent out an announcement about it earlier, since we didn’t have class this week, but I ended up getting bogged down trying to put together a full-on comprehensive study guide, which is still not ready, so instead I thought it’d be good to at least send out this more summary-level study guide.\nThe key topics that the Quiz will cover are:"
  },
  {
    "objectID": "writeups/quiz-1/index.html#parametric-modeling",
    "href": "writeups/quiz-1/index.html#parametric-modeling",
    "title": "Quiz 1 Study Guide",
    "section": "“Parametric” Modeling",
    "text": "“Parametric” Modeling\nWhat does it mean to have a “parametric” model? For example, in a model such as regression, which we write as\n\\[\nY = \\beta_0 + \\beta1_ X + \\varepsilon,\n\\]\nwhich of the things in that equation are parameters of the model and which are not parameters (for example, which are just variables that we plug data into)?"
  },
  {
    "objectID": "writeups/quiz-1/index.html#optimization-in-general",
    "href": "writeups/quiz-1/index.html#optimization-in-general",
    "title": "Quiz 1 Study Guide",
    "section": "Optimization in General",
    "text": "Optimization in General\nOnce we’ve identified the parameters in a model, how do we evaluate how “good” or “bad” a certain setting for the parameters is? (The answer being, a loss function)"
  },
  {
    "objectID": "writeups/quiz-1/index.html#gradient-descent",
    "href": "writeups/quiz-1/index.html#gradient-descent",
    "title": "Quiz 1 Study Guide",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nOnce we have a loss function, how does the gradient allow us to choose a random value for the parameter and then “make our way” towards the optimal value? The answer to this question is the main content in this previous writeup\nAs a refresher, a gradient is just the vector equivalent of a derivative. For example, in calculus we learn how\n\\[\nf(x) = x^2\n\\]\nhas a derivative\n\\[\n\\frac{\\partial f}{\\partial x} = 2x.\n\\]\nSo in this class, if \\(\\mathbf{x}\\) is now a vector like \\(\\mathbf{x} = (x_1,x_2)\\) instead of just a single number, the gradient or vector-valued derivative of \\(f\\) with respect to \\(\\mathbf{x}\\), \\(\\nabla_{\\mathbf{x}} f\\), is\n\\[\n\\nabla_{\\mathbf{x}} f = \\frac{\\partial f}{\\partial \\mathbf{x}} = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right)\n\\]\nIf it helps, try to notice/keep in mind how the \\(\\mathbf{x}\\) in \\(\\frac{\\partial f}{\\partial \\mathbf{x}}\\) is a vector, whereas the \\(x_1\\) in \\(\\frac{\\partial f}{\\partial x_1}\\) is a scalar. In other words, although the first and second terms in this expression may look scary, each entry within the parentheses on the RHS of that equality (the third term) is just the “regular” univariate derivative that you learn in calculus class!"
  },
  {
    "objectID": "writeups/quiz-1/index.html#more-efficient-optimization-methods",
    "href": "writeups/quiz-1/index.html#more-efficient-optimization-methods",
    "title": "Quiz 1 Study Guide",
    "section": "More Efficient Optimization Methods",
    "text": "More Efficient Optimization Methods\nHere the idea (or, the way I see these “fancier” methods, at least) is, they use additional information about the loss function above and beyond just \\(L(x)\\) and its derivative \\(L'(x)\\).\nSo, Newton’s method for example uses the second derivative \\(L''(x)\\) as an additional piece of information about the curvature of the loss function, whereas the secant method is slower than Newton’s method but doesn’t require us to know this second derivative \\(L''(x)\\) (since it approximates it)."
  },
  {
    "objectID": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "href": "writeups/quiz-1/index.html#a-full-on-lecture-replacement",
    "title": "Quiz 1 Study Guide",
    "section": "A Full-On Lecture Replacement",
    "text": "A Full-On Lecture Replacement\nTo try and fully “fill in” the missing week here, I can just give you all the resource that is literally a recording of the class I learned this stuff from, and that I later TAed. That way if you have additional questions I’ll be able to refer specifically to the examples/materials that Prof. Ng uses in the following video:\n\nSorry again for the lateness here, and, I will do my best to add more details to this guide tonight (Sunday night) and tomorrow (Monday morning), before the lecture time at 6:30pm!"
  },
  {
    "objectID": "writeups/optimization/index.html",
    "href": "writeups/optimization/index.html",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of “shifted into high gear”.\nSo, although it won’t help in terms of the Quizzes and Labs you’ve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you’ll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the “next step” once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample “version” of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means “nothing”: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for “subject to”, or sometimes “such that”. This means that the full expression in general can be read like “\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____”"
  },
  {
    "objectID": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "href": "writeups/optimization/index.html#motivation-a-closer-look-at-parameter-estimation",
    "title": "Mathematical Optimization",
    "section": "",
    "text": "Code\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\newcommand{\\bigexp}[1]{\\exp\\mkern-4mu\\left[ #1 \\right]}\n\\newcommand{\\bigexpect}[1]{\\mathbb{E}\\mkern-4mu \\left[ #1 \\right]}\n\\newcommand{\\definedas}{\\overset{\\text{defn}}{=}}\n\\newcommand{\\definedalign}{\\overset{\\phantom{\\text{defn}}}{=}}\n\\newcommand{\\eqeventual}{\\overset{\\text{eventually}}{=}}\n\\newcommand{\\Err}{\\text{Err}}\n\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n\\newcommand{\\expectsq}[1]{\\mathbb{E}^2[#1]}\n\\newcommand{\\fw}[1]{\\texttt{#1}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\heads}{\\outcome{heads}}\n\\newcommand{\\iid}{\\overset{\\text{\\small{iid}}}{\\sim}}\n\\newcommand{\\lik}{\\mathcal{L}}\n\\newcommand{\\loglik}{\\ell}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\newcommand{\\mle}{\\textsf{ML}}\n\\newcommand{\\nimplies}{\\;\\not\\!\\!\\!\\!\\implies}\n\\newcommand{\\orange}[1]{\\color{orange}{#1}}\n\\newcommand{\\outcome}[1]{\\textsf{#1}}\n\\newcommand{\\param}[1]{{\\color{purple} #1}}\n\\newcommand{\\pgsamplespace}{\\{\\green{1},\\green{2},\\green{3},\\purp{4},\\purp{5},\\purp{6}\\}}\n\\newcommand{\\prob}[1]{P\\left( #1 \\right)}\n\\newcommand{\\purp}[1]{\\color{purple}{#1}}\n\\newcommand{\\sign}{\\text{Sign}}\n\\newcommand{\\spacecap}{\\; \\cap \\;}\n\\newcommand{\\spacewedge}{\\; \\wedge \\;}\n\\newcommand{\\tails}{\\outcome{tails}}\n\\newcommand{\\Var}[1]{\\text{Var}[#1]}\n\\newcommand{\\bigVar}[1]{\\text{Var}\\mkern-4mu \\left[ #1 \\right]}\n\\]\n\n\nAs we entered the unit on Parameter Estimation, encompassing both Maximum Likelihood Estimation (MLE) and Generalized Method of Moments (GMM) Estimation, a bunch of students helpfully pointed out how the mathematical content of the course suddenly kind of “shifted into high gear”.\nSo, although it won’t help in terms of the Quizzes and Labs you’ve already completed on these topics, I think it is helpful to return to the mathematical aspects of those units, since they will become fairly central to the problems you’ll encounter in DSAN 5300: Statistical Learning, in the Spring semester.\nMy goals here are:\n\nTo start specifically with the Maximum Likelihood Estimation approach, going more slowly through some example problems emphasizing how MLE is rooted in optimization of an objective function, and then\nTo introduce constrained optimization via the Lagrange Multiplier approach, as the “next step” once you feel comfortable with MLE, again going through example problems that can then also serve as preparation for DSAN 5300!\n\nThe second bullet point is why I also mentioned GMM Estimation above: it turns out that, whereas MLE is an optimization problem without constraints, GMM can essentially be written as an optimization problem with only constraints.\nAs a preview of what this means, first assume we have:\n\nA vector-valued Random Variable \\(\\mathfrak{X}\\),\nAn \\(N\\)-dimensional vector \\(\\mathbf{x} = (x_1, \\ldots, x_N)\\), our dataset (a realization of \\(\\mathfrak{X}\\)),\nA vector of \\(J\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\ldots, \\theta_J)\\),\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, it has \\(J = 2\\) parameters \\(\\param{\\boldsymbol\\theta} = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\)\n\nA likelihood function \\(\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta})\\).\n\nYou can compare MLE for this scenario, written in the form of an optimization problem:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\mathcal{L}(\\mathfrak{X} = \\mathbf{x} \\mid \\param{\\boldsymbol\\theta}) \\\\\n&& \\text{s.t.} \\quad &\\varnothing\n\\end{alignat}\n\\]\nwith GMM estimation for this scenario written in the same form:\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\boldsymbol\\theta}} \\quad &\\varnothing \\\\\n&& \\text{s.t.} \\quad & \\mu_1(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_1(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\mu_2(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_2(\\mathbf{x}, \\param{\\boldsymbol\\theta}) \\\\\n&& \\quad & \\phantom{\\mu_1(\\param{\\boldsymbol\\theta})} ~ \\vdots \\\\\n&& \\quad & \\mu_J(\\param{\\boldsymbol\\theta}) = \\widehat{\\mu}_J(\\mathbf{x}, \\param{\\boldsymbol\\theta}),\n\\end{alignat}\n\\]\nwhere:\n\n\\(\\mu_i(\\param{\\boldsymbol\\theta})\\) is the \\(i\\)th theoretical moment of \\(\\mathfrak{X}\\)\n\nFor example, if \\(\\mathfrak{X}\\) is distributed normally, \\(\\mu_1(\\theta_1, \\theta_2) = \\mathbb{E}[\\mathfrak{X}]\\)\n\n\\(\\widehat{\\mu}_i(\\mathbf{x}, \\param{\\boldsymbol\\theta})\\) is the \\(i\\)th empirical moment of \\(\\mathfrak{X}\\)\n\nFor example, since \\(\\mu_1 = \\mathbb{E}[\\mathfrak{X}]\\), \\(\\widehat{\\mu_1}\\) is the sample “version” of \\(\\mathbb{E}[\\mathfrak{X}]\\), namely, \\(\\widehat{\\mu}_1 = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nIn both cases, the symbol \\(\\varnothing\\) literally just means “nothing”: in the MLE case, we have no constraints, whereas in the GMM estimation case, we have no objective function.\nFor readability, \\(\\text{s.t.}\\) is just shorthand for “subject to”, or sometimes “such that”. This means that the full expression in general can be read like “\\(\\param{\\boldsymbol\\theta}^*\\) is the maximum of ____, subject to _____”"
  },
  {
    "objectID": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "href": "writeups/optimization/index.html#visualizing-unconstrained-and-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Visualizing Unconstrained and Constrained Optimization",
    "text": "Visualizing Unconstrained and Constrained Optimization\nThe unconstrained optimizations we will carry out here can be visualized as “hill climbing”: the optimization approach you probably learned in calculus class—computing the derivative and setting it equal to zero—works precisely because the top of the “hill” is the point at which the derivative becomes zero. For example, if we started climbing from the left side of the following plot, the derivative would get lower and lower as we moved right, hitting zero when we reach the top of the “hill”:\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nmy_hill &lt;- function(x) exp(-(1/2)*x^2)\nmy_slope &lt;- function(x) -x*exp(-(1/2)*x^2)\nx0_vals &lt;- c(-1.75, -0.333, 0)\ntangent_at_x0 &lt;- function(x,x0) my_slope(x0)*(x - x0) + my_hill(x0)\ntan_x1 &lt;- function(x) tangent_at_x0(x, x0_vals[1])\ntan_x2 &lt;- function(x) tangent_at_x0(x, x0_vals[2])\ntan_x3 &lt;- function(x) tangent_at_x0(x, x0_vals[3])\neval_df &lt;- tibble(x=x0_vals) |&gt; mutate(y=my_hill(x))\ntan_ext &lt;- 0.5\nslopes &lt;- round(c(\n  my_slope(x0_vals[1]),\n  my_slope(x0_vals[2]),\n  my_slope(x0_vals[3])\n), 3)\nopt_df &lt;- tibble(x=0, y=my_hill(0))\nx_df &lt;- tibble(x=c(-3, 3))\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_function(\n    fun=tan_x1, aes(color=\"x1\"), linewidth=1,\n    xlim=c(\n      x0_vals[1]-tan_ext,\n      x0_vals[1]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x2, aes(color=\"x2\"), linewidth=1,\n    xlim=c(\n      x0_vals[2]-tan_ext,\n      x0_vals[2]+tan_ext\n    )\n  ) +\n  geom_function(\n    fun=tan_x3, aes(color=\"x3\"), linewidth=1,\n    xlim=c(\n      -1,1\n    )\n  ) +\n  geom_point(\n    data=eval_df,\n    aes(x=x, y=y, color=c(\"x1\",\"x2\",\"x3\")),\n    size=2\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"mle\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"MLE Estimate\"\n  ) +\n  scale_color_manual(\n    \"Slope at x\",\n    values=c(cb_palette[1], cb_palette[2], cb_palette[3]),\n    labels=slopes\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.15))\n\n\n\n\n\nThe derivatives “point” in the direction you should go to get to the maximum value, at which it has value zero\n\n\n\n\nUsing this same metaphor of the top of a hill being the “best” point, constrained optimization is a bit harder to visualize, but you can think of it like:\n\nGetting (almost) crunched between two walls coming closer together, then\nGetting (almost) crunched between a floor and a ceiling coming closer together,\n\nwhere that coming-closer-together is set up so as to crunch the space, making it smaller and smaller until the only point(s) left are the point(s) representing the optimal solution:\n\n\nCode\nxl &lt;- -0.06\nxu &lt;- 0.06\nyl &lt;- 0.985\nyu &lt;- 1.015\nanno_x &lt;- -1.8\nanno_y &lt;- 0.2\nx_df &lt;- tibble(x=c(-3, 3))\nrib_df &lt;- x_df |&gt; mutate(\n  ymin = -0.1,\n  ymax = 0.1\n)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=my_hill, linewidth=1) +\n  geom_rect(\n    aes(fill=\"x\"), xmin=xl, xmax=xu, ymin=-Inf, ymax=Inf, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=xl, xend=xl, y=-Inf, yend=Inf) +\n  geom_segment(x=xu, xend=xu, y=-Inf, yend=Inf) +\n  geom_rect(\n    aes(fill=\"y\"), xmin=-Inf, xmax=Inf, ymin=yl, ymax=yu, alpha=0.5, color='black'\n  ) +\n  geom_segment(x=-Inf, xend=Inf, y=yl, yend=yl) +\n  geom_segment(x=-Inf, xend=Inf, y=yu, yend=yu) +\n  # Anno xmin\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xl-0.5, xend = xl-0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xl-1.2, label = \"x &gt; a\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno xmax\n  annotate(\n    \"segment\", y=anno_y, yend=anno_y, x = xu+0.5, xend = xu+0.04,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", y=anno_y, x=xu+0.6, label = \"x &lt; b\", color = \"black\",\n    angle = 0, hjust = 0, vjust=0.5, size = 5\n  ) +\n  # Anno ymin\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yl-0.2, yend = yl-0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yl-0.25, label = \"y &gt; c\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=0.5, size = 5\n  ) +\n  # Anno ymax\n  annotate(\n    \"segment\", x=anno_x, xend=anno_x, y = yu+0.2, yend = yu+0.015,\n    linewidth=1, linejoin = \"mitre\",\n    arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\n    \"text\", x=anno_x, y=yu+0.2, label = \"y &lt; d\", color = \"black\",\n    angle = 0, hjust = 0.5, vjust=-0.5, size = 5\n  ) +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(\"a &lt; x &lt; b\", \"c &lt; y &lt; d\")\n  ) +\n  geom_point(\n    data=opt_df,\n    aes(x=x, y=y, shape=\"gmm\"),\n    size=2\n  ) +\n  scale_shape_manual(\n    element_blank(),\n    values=19,\n    labels=\"GMM Estimate\"\n  ) +\n  theme_classic(base_size=18) +\n  ylim(c(0, 1.5))\n\n\n\n\n\nThe constraints “crunch” the admissible values of \\(x\\) until only one point (the Method of Moments estimate) is left. The metaphor is a bit more contrived in this case, however, since we usually don’t manually compute these constraints in exactly this form (they are implicit in the GMM’s system of equations), though numerically (i.e., if we use R to compute the GMM estimate), this is exactly how the constraints would be applied!\n\n\n\n\nLet’s look at how we can work through both MLE and GMM estimation (which we previously learned separately!) as optimization problems, and how that will give us a unified optimization framework for estimating parameters in the fancier ML models you’ll see in DSAN 5300!"
  },
  {
    "objectID": "writeups/optimization/index.html#general-unconstrained-optimization",
    "href": "writeups/optimization/index.html#general-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Unconstrained Optimization",
    "text": "General Unconstrained Optimization\nAs the name implies, Maximum likelihood estimation involves maximizing something:\n\nIn MLE, that thing is the Likelihood Function \\(\\mathcal{L}(X \\mid \\theta)\\).\nIn optimization, more generally, that thing is called the objective function \\(f(x, \\theta)\\).\n\nThis is the reasoning behind something I might have mentioned in class in earlier weeks, that “the objective function in MLE is the likelihood function”.\nIn other words, when you see the term “objective function”, just replace it in your brain with “thing I’m trying to optimize (minimize or maximize) here”.1\nBefore we look at MLE specifically, therefore, let’s look at a general unconstrained optimization problem, something you almost surely have already solved tons of times (even if you didn’t have the vocabulary of optimization, objective functions, constraints, and so on):\n\n\n\n\n\n\nExample 1: General Unconstrained Optimization\n\n\n\nFind \\(x^*\\), the solution to\n\\[\n\\begin{align}\n    \\min_{x} ~ & f(x) = 3x^2 - x \\\\\n    \\text{s.t. } ~ & \\varnothing\n\\end{align}\n\\]\n\n\nFrom calculus, we know that finding the optimum value for a function like this (whether minimum or maximum) boils down to:\n\nComputing the derivative \\(f'(x) = \\frac{\\partial}{\\partial x}f(x)\\),\nSetting it equal to zero: \\(f'(x) = 0\\), and\nSolving this equal for \\(x\\), i.e., finding values \\(x^*\\) which satisfy \\(f'(x^*) = 0\\)\n\nHere, without any constraints, we can follow this exact procedure to find the minimum value. We start by computing the derivative:\n\\[\nf'(x) = \\frac{\\partial}{\\partial x}f(x) = \\frac{\\partial}{\\partial x}\\left[3x^2 - x\\right] = 6x - 1,\n\\]\nthen solve for \\(x^*\\) as the value(s) satisfying \\(\\frac{\\partial}{\\partial x}f'(x^*) = 0\\) for the just-derived \\(f'(x)\\):\n\\[\nf'(x^*) = 0 \\iff 6x^* - 1 = 0 \\iff x^* = \\frac{1}{6}.\n\\]\n\n\n\n\n\n\nDerivative Cheatsheet (Click to Collapse / Expand)\n\n\n\n\n\n(These green boxes are where I get to pop off a tiny bit, but in a way that I hope is helpful! 😜)\nPersonally, I absolutely hate, despise memorizing things. So much of school growing up felt like memorizing a ton of things for no reason, since they were things we could just Google 90% of the time…\nSo, even though people usually associate math with memorization of formulas, for whatever reason I have the opposite association: math was the one class where I didn’t have to memorize things, because (unlike… “the mitochondria is the powerhouse of the cell”) I had really good teachers who always walked us through how to derive things from more basic principles.\nSo, I’m providing this here as a small set of “shortcuts”, but long story short each of these can be derived from even simpler procedures (I don’t want to clutter this writeup even more by writing those out, but I’m happy to walk you through how you could derive these, in office hours for example!)\n\n\n\n\n\n\n\n\nType of Thing\nThing\nChange in Thing when \\(x\\) Changes by Tiny Amount\n\n\n\n\nPolynomial\n\\(f(x) = x^n\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}f(x) = nx^{n-1}\\)\n\n\nFraction\n\\(f(x) = \\frac{1}{x}\\)\nUse Polynomial rule (since \\(\\frac{1}{x} = x^{-1}\\)) to get \\(f'(x) = -\\frac{1}{x^2}\\)\n\n\nLogarithm\n\\(f(x) = \\ln(x)\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x} = \\frac{1}{x}\\)\n\n\nExponential\n\\(f(x) = e^x\\)\n\\(f'(x) = \\frac{\\partial}{\\partial x}e^x = e^x\\) (🧐❗️)\n\n\nMultiplication\n\\(f(x) = g(x)h(x)\\)\n\\(f'(x) = g'(x)h(x) + g(x)h'(x)\\)\n\n\nDivision\n\\(f(x) = \\frac{g(x)}{h(x)}\\)\nToo hard to memorize… turn it into Multiplication, as \\(f(x) = g(x)(h(x))^{-1}\\)\n\n\nComposition (Chain Rule)\n\\(f(x) = g(h(x))\\)\n\\(f'(x) = g'(h(x))h'(x)\\)\n\n\nFancy Logarithm\n\\(f(x) = \\ln(g(x))\\)\n\\(f'(x) = \\frac{g'(x)}{g(x)}\\) by Chain Rule\n\n\nFancy Exponential\n\\(f(x) = e^{g(x)}\\)\n\\(f'(x) = g'(x)e^{g(x)}\\) by Chain Rule\n\n\n\n\n\n\nThe reason why derivatives are so important for optimization is that we’re trying to climb a hill (for maximization; if we’re minimizing then we’re trying to reach the bottom of a lake)"
  },
  {
    "objectID": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "href": "writeups/optimization/index.html#maximum-likelihood-estimation-as-unconstrained-optimization",
    "title": "Mathematical Optimization",
    "section": "Maximum Likelihood Estimation as Unconstrained Optimization",
    "text": "Maximum Likelihood Estimation as Unconstrained Optimization\nNow that we have this general procedure for non-constrained optimization in general, let’s use it to obtain a maximum likelihood estimate for some probabilistic model.\n\n“Standard” Example: Poisson Distribution\nWe’ll start with an example more similar to the ones you did in DSAN 5100: estimating the rate parameter \\(\\param{\\lambda}\\) for a random variable \\(X\\) with a Poisson distribution.\n\n\n\n\n\n\nExample 2: MLE for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Maximum Likelihood Estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nLike with other distributions we looked at in class, the way to approach problems like this is to write out the details step-by-step until you have enough information to start deriving the MLE. So, the first piece of information we have is:\n\nA random variable \\(X \\sim \\text{Pois}(\\param{\\lambda})\\)\n\nGiven how the Poisson distribution works, this means that2\n\\[\n\\Pr(X = k; \\param{\\lambda}) = \\frac{\\param{\\lambda}^ke^{-\\param{\\lambda}}}{k!},\n\\tag{1}\\]\nwhere we can use probability mass \\(\\Pr(X = k)\\) rather than probability density \\(f_X(k)\\) since the Poisson distribution is a discrete distribution (modeling integer counts rather than continuous values like the normal distribution).\nThe next piece of information we have, in a Maximum Likelihood setup, is an observed dataset containing \\(N\\) i.i.d. datapoints,\n\n\\(\\mathbf{x} = (x_1, \\ldots, x_n)\\),\n\nwhere each point is assumed to be drawn from this Poisson distribution with parameter \\(\\param{\\lambda}\\). This assumption means that we treat each of these observed points \\(x_i\\) as the realization of a Poisson-distributed Random Variable \\(X_i\\), so that (by Equation 1 above):\n\\[\n\\Pr(X_i = x_i; \\param{\\lambda}) =  \\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!},\n\\]\nOur goal in Maximum Likelihood Estimation is to take this observed dataset and figure out what value of the parameter \\(\\param{\\lambda}\\) is most likely to have produced \\(\\mathbf{x}\\).\nIn other words, we are trying to find the value of \\(\\param{\\lambda}\\) which maximizes the joint probability that \\(X_1\\) is realized as \\(x_1\\), \\(X_2\\), is realized as \\(x_2\\), and so on up to \\(X_n\\) being realized as \\(x_n\\), which we call the likelihood \\(\\mathcal{L}(\\mathbf{x}; \\param{\\lambda})\\) of the dataset:\n\\[\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) = \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}).\n\\]\nThe key for being able to compute this giant joint probability is the independence assumption: since the values in \\(\\mathbf{x}\\) are assumed to be independent and identically distributed (i.i.d.), by the definition of independence, we can factor this full joint probability into the product of individual-variable probabilities:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n; \\param{\\lambda}) \\\\\n&= \\Pr(X_1 = x_1; \\param{\\lambda}) \\times \\cdots \\times \\Pr(X_n = x_n; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda})\n\\end{align*}\n\\]\nThis factoring into individual terms is the key to solving this problem! Now that we’ve done this, the rest of the problem boils down to a calculus problem. Let’s write out this product (which will look messy at first, but we’ll make it simpler using \\(\\log()\\) below!):\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{x}; \\param{\\lambda}) &= \\prod_{i=1}^{N}\\Pr(X_i = x_i; \\param{\\lambda}) \\\\\n&= \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!}\n\\end{align*}\n\\]\nBecause \\(\\log()\\) is a monotonic function, the value of \\(x\\) which maximizes \\(\\log(f(x))\\) will be the same as the value which maximizes \\(f(x)\\). So, to make our lives easier since \\(\\log()\\) turns multiplications into additions, we compute the log-likelihood (the log of the likelihood function above), which we denote \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\):\n\\[\n\\ell(\\mathbf{x}; \\param{\\lambda}) = \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right]\n\\]\nI recommend working through this application of \\(\\log()\\) yourself, if you can, then you can click the following button to show the worked-out solution:\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{\\param{\\lambda}^{x_i}e^{-\\param{\\lambda}}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{\\param{\\lambda}^{x_i}e^{-\\lambda}}{x_i!} \\right] \\\\\n&= \\sum_{i=1}^{N}\\log (\\param{\\lambda}^{x_i}e^{-\\lambda}) - \\sum_{i=1}^N\\log(x_i!) \\\\\n&= \\sum_{i=1}^{N}x_i\\log(\\param{\\lambda}) + \\sum_{i=1}^{N}\\log(e^{-\\param{\\lambda}}) - \\sum_{i=1}^{N}\\log(x_i!) \\\\\n&= \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!).\n\\end{align*}\n\\]\n\nThis might look scary at first, for example, because of the term with the \\(x_i!\\). However, keep in mind that we won’t need to worry about this term, since it does not involve \\(\\param{\\lambda}\\), the parameter we are maximizing over!\nSo, following the same procedure as our previous example, we maximize this log-likelihood function with respect to \\(\\param{\\lambda}\\). We start by computing the derivative of \\(\\ell(\\mathbf{x}; \\param{\\lambda})\\) with respect to \\(\\param{\\lambda}\\):\n\\[\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) = \\frac{\\partial}{\\partial \\param{\\lambda}}\\left[ \\log(\\param{\\lambda})\\sum_{i=1}^{N}x_i - N\\param{\\lambda} - \\sum_{i=1}^{N}\\log(x_i!) \\right]\n\\]\nLike before, I recommend working through this yourself on paper, and then you can click the following to show the worked-out solution:\n\n\nClick to Show Solution\n\nSince both \\(\\sum_{i=1}^{N}x_i\\) and \\(\\sum_{i=1}^{N}\\log(x_i!)\\) are constants with respect to \\(\\param{\\lambda}\\), and since the derivative operator \\(\\frac{\\partial}{\\partial\\param{\\lambda}}\\) is linear, this reduces to:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\param{\\lambda}}\\ell(\\mathbf{x}; \\param{\\lambda}) &= \\left( \\sum_{i=1}^{N}x_i\\right) \\frac{\\partial}{\\partial \\param{\\lambda}}[\\log(\\param{\\lambda})] - N\\frac{\\partial}{\\partial\\param{\\lambda}}[\\param{\\lambda}] \\\\\n&= \\frac{\\sum_{i=1}^{N}x_i}{\\param{\\lambda}} - N.\n\\end{align*}\n\\]\nAnd now we can set this equal to zero and solve to obtain the maximum-likelihood estimator \\(\\lambda^*\\):\n\\[\n\\begin{align*}\n&\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} - N = 0 \\\\\n\\iff &\\frac{\\sum_{i=1}^{N}x_i}{\\lambda^*} = N \\\\\n\\iff &\\sum_{i=1}^{N}x_i = N\\lambda^* \\\\\n\\iff &\\lambda^* = \\frac{1}{N}\\sum_{i=1}^{N}x_i,\n\\end{align*}\n\\]\n\nmeaning that, after all this work, the maximum likelihood estimator for a dataset containing realizations of i.i.d. Poisson RVs is the sample mean of those points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\nIn other words, if you are given a dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\), and you think that the entries in this dataset were generated via the Poisson distribution, then the “best guess” (if we define “best guess” as “guess with maximum likelihood”) for the parameter \\(\\param{\\lambda}\\) of this Poisson distribution is the sample mean of the observed points, \\(\\frac{1}{N}\\sum_{i=1}^{N}x_i\\).\n\n\nTrickier Example: Linear Regression\nSince I think linear regression is a really important model to have in the back of your mind as you move towards fancier Machine Learning models, but is a bit more complex than the Poisson case we just looked at, a good starting point is an over-simplified version of linear regression, where we don’t even have an intercept.\n\n\n\n\n\n\nExample 3: Zero-Intercept Linear Regression\n\n\n\nAssume we have a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_N,y_N))\\) containing noisy observations from some underlying linear relationship \\(y = \\param{\\beta} x\\), so that we model what we observe in \\(\\mathbf{d}\\) as realizations of random variables \\(X\\), \\(Y\\), and \\(\\varepsilon_i\\):\n\\[\nY_i = \\param{\\beta} X_i + \\varepsilon_i,\n\\]\nwhere the variables \\(\\varepsilon_i\\) are i.i.d. normally-distributed variables with mean zero and a given variance \\(\\sigma^2\\): \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).\nFind the Maximum Likelihood Estimator for the single parameter in this case, \\(\\param{\\beta}\\).\n\n\nNote that in general, if a random variable \\(X\\) is distributed normally, then adding things to \\(X\\) just shifts the mean parameter \\(\\mu\\) (meaning, for example, if \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\), then \\(X + 3 \\sim \\mathcal{N}(\\mu + 3, \\sigma)\\)).\nHere, since \\(\\varepsilon_i\\) is a normally-distributed random variable with \\(\\mu = 0\\), the left-hand side of the above equation means that \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nJust as we used the Poisson PMF in the previous example, here you will use the Normal pdf, the probability density function for \\(Y_i\\) in this case, which is typically denoted using the Greek letter \\(\\varphi\\) (“Phi”):\n\\[\n\\varphi(v; \\param{\\mu}, \\param{\\sigma}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{v - \\mu}{\\sigma}\\right)^2\\right].\n\\]\nAlthough in this “standard form” for the pdf of the normal distribution the two parameters are \\(\\mu\\) and \\(\\sigma\\), in our case note that we are not estimating the parameters \\(\\mu\\) and \\(\\sigma\\) itself. Instead, we are estimating a single parameter \\(\\param{\\beta}\\), which is not the mean or standard deviation itself, though it ends up affecting the mean since \\(Y_i \\sim \\mathcal{N}(\\param{\\beta} X_i, \\sigma)\\).\nSo, the way we obtain the likelihood which we can then use to estimate \\(\\param{\\beta}\\) is by plugging \\(\\param{\\beta} X_i\\) into the pdf above, to obtain:\n\\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{d}; \\param{\\beta}) &= \\prod_{i=1}^{N}\\varphi(y_i; \\param{\\beta}x_i, \\sigma) \\\\\n&= \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right]\n\\end{align*}\n\\]\nLike in the Poisson case, this looks scary until you transform it into the log-likelihood function, at which point lots of things simplify and you can compute a closed-form solution!\n\n\nClick to Show Solution\n\n\\[\n\\begin{align*}\n\\ell(\\mathbf{d}; \\param{\\beta}) &= \\log\\left[ \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}\\log\\left[ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{1}{2}\\left(\\frac{y_i - \\param{\\beta}x_i}{\\sigma}\\right)^2 \\right] \\right] \\\\\n&= \\sum_{i=1}^{N}-\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2}\\sum_{i=1}^{N}\\left(\\frac{y_i - \\param{\\beta} x_i}{\\sigma}\\right)^2 \\\\\n&= -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2.\n\\end{align*}\n\\]\nAnd we now have the log-likelihood in a form where we can compute a derivative straightforwardly (using the derivative “rules” in the table presented earlier):\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\param{\\beta}}\\ell(\\mathbf{d}; \\param{\\beta}) &= \\frac{\\partial}{\\partial\\param{\\beta}}\\left[ -N\\log\\left(\\sqrt{2\\pi}\\sigma \\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left(y_i - \\param{\\beta} x_i\\right)^2 \\right] \\\\\n&= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\param{\\beta}}\\left[ (y_i - \\param{\\beta}x_i)^2 \\right] \\\\\n&= -\\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(y_i-\\param{\\beta}x_i)x_i \\\\\n&= -\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right].\n\\end{align*}\n\\]\nBy setting this equal to zero, we can now solve for the MLE estimator \\(\\beta^*\\):\n\\[\n\\begin{align*}\n&-\\frac{1}{\\sigma^2}\\left[\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2\\right] = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i - \\param{\\beta}\\sum_{i=1}^{N}x_i^2 = 0 \\\\\n\\iff &\\sum_{i=1}^{N}x_iy_i = \\param{\\beta}\\sum_{i=1}^{N}x_i^2,\n\\end{align*}\n\\]\nAnd thus the MLE for \\(\\param{\\beta}\\) in this case is\n\\[\n\\widehat{\\beta}_{MLE} = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}.\n\\]\n\nThis means that, if we want the line of “best” fit for a dataset \\(\\mathbf{d} = ((x_1,y_1),\\ldots,(x_n,y_n))\\), where “best fit” is defined to be “slope with maximum likelihood, with intercept 0”, this line is\n\\[\ny = \\widehat{\\beta}_{MLE}x = \\frac{\\sum_{i=1}^{N}x_iy_i}{\\sum_{i=1}^{N}x_i^2}x.\n\\]"
  },
  {
    "objectID": "writeups/optimization/index.html#general-constraints-only-optimization",
    "href": "writeups/optimization/index.html#general-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constraints-Only Optimization",
    "text": "General Constraints-Only Optimization\nSeeing an optimization problem written out with only constraints looks a bit weird at first, but will be helpful to consider before we look at full-on constrained optimization.\nLike how we looked at a “basic” calculus problem before applying the same methodology to MLE before, here let’s look at a “basic” algebra problem with inequalities:\n\n\n\n\n\n\nExample 4: Inequality Constraints\n\n\n\nFind the value \\(x^*\\) which satisfies the system of inequalities\n\\[\n\\begin{alignat}{2}\nx^* = &&\\max_{x} \\quad &f(x,y) = 0 \\\\\n&&\\text{s.t.} \\quad & y \\geq x^2 + 1 \\\\\n&& \\quad & y \\leq \\sqrt{1 - x^2}\n\\end{alignat}\n\\]\n\n\nIt looks weird at first because, the \\(\\max_x\\) portion doesn’t give us anything to work with: we’re “maximizing” \\(f(x)\\) which is always just the number \\(0\\)! So, instead, we focus on just the constraints:\n\n\\(y \\geq x^2 + 1\\)\n\\(y \\leq \\sqrt{1 - x^2}\\)\n\nPlotting these two functions, we can see that they meet at exactly one point:\n\n\nCode\nlibrary(latex2exp)\nx_df &lt;- tibble(x=c(-2,2))\nf1 &lt;- function(x) x^2 + 1\nf1_lab &lt;- TeX(\"$y \\\\geq x^2 + 1$\")\nf2 &lt;- function(x) sqrt(1 - x^2)\nf2_lab &lt;- TeX(\"$y \\\\leq \\\\sqrt{1 - x^2}$\")\nsoln_df &lt;- tibble(x=0, y=1)\nx_df |&gt; ggplot(aes(x=x)) +\n  stat_function(fun=f1, color='black') +\n  stat_function(fun=f1, geom=\"ribbon\", mapping=aes(ymin=after_stat(y),ymax=4, fill=\"f1\"), alpha=0.5) +\n  stat_function(fun=f2, color='black') +\n  stat_function(fun=f2, geom=\"area\", aes(fill=\"f2\"), alpha=0.5) +\n  geom_point(data=soln_df, aes(x=x, y=y)) +\n  theme_classic() +\n  scale_fill_manual(\n    \"Constraints:\",\n    values=c(cb_palette[1], cb_palette[2]),\n    labels=c(f1_lab, f2_lab)\n  ) +\n  ylim(0,4)\n\n\n\n\n\n\n\n\n\nwhich means that we can solve for where they’re equal to derive the unique optimal solution \\(x^*\\)!\n\\[\n\\begin{align*}\n&x^2 + 1 = \\sqrt{1 - x^2} \\\\\n\\iff &(x^2 + 1)^2 = 1 - x^2 \\\\\n\\iff & x^4 + 2x^2 + 1 = 1 - x^2 \\\\\n\\iff & x^4 + 3x^2 = 0 \\\\\n\\iff & x^2(x^2 + 3) = 0,\n\\end{align*}\n\\]\nwhich means that the only possible solutions are the solutions to \\(x^2 = 0\\) and \\(x^2 + 3 = 0\\). The only \\(x\\) which satisfies \\(x^2 = 0\\) is \\(x^* = 0\\). The only \\(x\\) which satisfies \\(x^2 + 3 = 0\\) is \\(x^* = \\sqrt{-3}= \\pm 3i\\), meaning that our only real solution is \\(x^* = 0\\), forming the unique (real) solution to our optimization problem."
  },
  {
    "objectID": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "href": "writeups/optimization/index.html#gmm-estimation-as-constraints-only-optimization",
    "title": "Mathematical Optimization",
    "section": "GMM Estimation as Constraints-Only Optimization",
    "text": "GMM Estimation as Constraints-Only Optimization\nThe Generalized Method of Moments approach basically takes advantage of the type of “crunching” we saw in the previous example, setting up a system of equations which “crunches” the set of possible values for the desired parameter \\(\\param{\\theta}\\) down into just a single value.\nIt is able to accomplish this, basically, by saying:\nIf the entries in our dataset \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) are realizations of RVs \\(X_1, \\ldots, X_n\\) drawn i.i.d. from some distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) with parameters \\(\\param{\\boldsymbol\\theta}\\), then the moments of the theoretical distribution \\(\\mathcal{D}(\\param{\\boldsymbol\\theta})\\) should match their observed counterparts.”\nSpecifically, \\(\\param{\\boldsymbol\\theta}\\) should be the value which makes the:\n\n\n\n\n\n\n\n\nExpected mean \\(\\mu = \\mathbb{E}[X_i]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved mean of the dataset, \\(\\widehat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\)\n\n\nExpected variance \\(\\sigma^2 = \\mathbb{E}[(X_i - \\mu)^2]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved variance of the dataset, \\(\\widehat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^2\\)\n\n\nExpected skewness \\(\\gamma = \\mathbb{E}[(X_i - \\mu)^3]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\gamma} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^3\\)\n\n\nExpected kurtosis \\(\\kappa = \\mathbb{E}[(X_i - \\mu)^4]\\) of \\(X_i \\sim \\mathcal{D}(\\param{\\boldsymbol\\theta})\\)\nequal to the\nobserved skewness of the dataset, \\(\\widehat{\\kappa} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\widehat{\\mu})^4\\)\n\n\n\nand so on—as many equations as we need to estimate the parameters \\(\\param{\\boldsymbol\\theta}\\) of the distribution \\(\\mathcal{D}\\)!\nSo, let’s repeat the earlier problem with the Poisson distribution, using GMM instead of MLE, to compute an estimator for the rate parameter \\(\\param{\\lambda}\\).\n\n\n\n\n\n\nExample 5: GMM Estimate for Poisson-Distributed RV\n\n\n\nGiven a dataset consisting of \\(N\\) i.i.d. realizations \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) of Poisson-distributed random variables\n\\[\nX_1, \\ldots, X_n \\sim \\text{Pois}(\\param{\\lambda}),\n\\]\nfind the Generalized Method of Moments estimate for the parameter \\(\\param{\\lambda}\\).\n\n\nHere, since our distribution only has one parameter \\(\\param{\\lambda}\\), we only need one equation in our system of equations, which will “match” the expected value of a Poisson-distributed RV with the mean of our dataset \\(\\mathbf{x}\\):\n\\[\n\\begin{align*}\n&\\mu = \\widehat{\\mu} \\\\\n\\iff &\\mathbb{E}[X_i] = \\frac{1}{N}\\sum_{i=1}^{N}x_i \\\\\n\\iff &\\param{\\lambda} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n\\end{align*}\n\\]\nAnd… yup, that’s it! Since the expected value of a poisson-distributed Random Variable \\(X_i\\) is just exactly the rate parameter \\(\\param{\\lambda}\\)3, we’ve obtained the GMM estimate of \\(\\param{\\lambda}\\) as desired here, and we see that in fact the GMM estimator is the same as the MLE estimator in this case."
  },
  {
    "objectID": "writeups/optimization/index.html#general-constrained-optimization",
    "href": "writeups/optimization/index.html#general-constrained-optimization",
    "title": "Mathematical Optimization",
    "section": "General Constrained Optimization",
    "text": "General Constrained Optimization\nThough we introduced objective functions and constraints separately here, in reality most problems will require you to optimize with respect to both of these to find the solution to your problem.\nTo use the example I used throughout the semester one more time: if you are trying to estimate the population mean height from a sample of heights, you may want to have:\n\nAn objective function quantifying how “good” a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) is in terms of fitting the data (this would be precisely the likelihood function), but also\nA constraint on \\(\\mu\\) to ensure that you don’t get negative values (since heights in reality can’t be negative), and perhaps another constraint to ensure that you don’t get absurdly high numbers in the millions and billions as well.\n\nWhile you’ve seen likelihood functions, constraints in optimization world are written out as:\n\nEqualities like \\(\\sum_{i=1}^{N}\\theta_i = 1\\) or\nInequalities like \\(\\theta_i &lt; 0.5 ~ \\forall i\\).\n\nSo, for the normal distribution example just mentioned, you could introduce inequality constraints to re-write the problem in the following form (where we just include the non-negative constraint, for simplicity):\n\\[\n\\begin{alignat}{2}\n\\boldsymbol\\theta^* = &&\\max_{\\param{\\mu}, \\sigma} \\quad &\\mathcal{L}(X = v_X \\mid \\param{\\mu}, \\sigma) \\\\\n&& \\text{s.t.} \\quad & \\param{\\mu} &gt; 0\n\\end{alignat}\n\\]\nThese two working together (the objective function and the constraints), it turns out, will “supercharge” your estimation! Now, instead of just guessing a random number from \\(-\\infty\\) to \\(\\infty\\) as your initial guess for \\(\\widehat{\\mu}\\), you have the power to “guide” the optimization by (say) starting at the lower bound on \\(\\widehat{\\mu}\\), in this case, 0.\nFor solving by hand, though, we’ll need to use a technique called the Lagrange Multiplier approach, to turn this constrained optimization problem back into an unconstrained optimization, since this is the case where we know how to use calculus to solve!\n\n\n\n\n\n\nExample 6: Constrained Optimization via Lagrange Multipliers\n\n\n\nFind the optimal value \\(x^*\\) for the following optimization problem:\n\\[\n\\begin{alignat}{2}\nx^* = &&\\min_{x} \\quad &f(x, y) = 2 - x^2 - 2y^2 \\\\\n&& \\text{s.t.} \\quad & x^2 + y^2 = 1\n\\end{alignat}\n\\]\n\n\nWe can visualize the two “pieces” of this optimization, to see (finally!) how the objective function and the constraints come together:\n\n\nCode\nlibrary(ggforce)\nmy_f &lt;- function(x,y) 2 - x^2 - 2*y^2\nx_vals &lt;- seq(from=-2, to=2, by=0.1)\ny_vals &lt;- seq(from=-2, to=2, by=0.1)\ndata_df &lt;- expand_grid(x=x_vals, y=y_vals)\ndata_df &lt;- data_df |&gt; mutate(\n  z = my_f(x, y)\n)\ndata_df |&gt; ggplot(aes(x=x, y=y, z=z)) +\n  # geom_rect(xmin=0, xmax=1, ymin=-Inf, ymax=Inf, alpha=0.5, fill=cb_palette[1]) +\n  # geom_vline(xintercept=0, linewidth=0.75) +\n  geom_contour_filled(alpha=0.9, binwidth = 0.5, color='black', linewidth=0.2) +\n  geom_point(aes(x=0, y=0)) +\n  geom_circle(aes(x0=0, y0=0, r=1)) +\n  geom_segment(aes(x=0, y=0, xend=1, yend=0), linetype=\"dashed\") +\n  scale_fill_viridis_d(option=\"C\") +\n  theme_classic() +\n  # theme(legend.position=\"none\") +\n  coord_equal()\n\n\n\n\n\n\n\n\nFigure 1: The goal of our constrained optimization problem is to find the optimal value(s) \\((x^*,y^*)\\) which maximize \\(f(x,y)\\) (higher values are brighter yellow here), subject to the constraint that the point \\((x^*, y^*)\\) lies on the unit circle.\n\n\n\n\n\nSadly, our earlier approach of finding the derivative of the objective function, setting it equal to zero, and solving, won’t work here. Let’s see why. First compute the partial derivatives:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = -2x \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = -4y\n\\end{align*}\n\\]\nThen set them equal to zero and solve the system of equations:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x,y) = 0 \\iff -2x^* = 0 \\iff &\\boxed{x^* = 0} \\\\\n\\frac{\\partial}{\\partial y}f(x,y) = 0 \\iff -4y^* = 0 \\iff &\\boxed{y^* = 0}.\n\\end{align*}\n\\]\nWe can see now that our computed optimal point, \\((0, 0)\\), violates the desired constraint, since it does not lie on the unit circle \\(x^2 + y^2 = 1\\):\n\\[\n(x^*)^2 + (y^*)^2 = 0^2 + 0^2 = 0 \\neq 1.\n\\]\nThe Lagrange Multiplier approach comes to the rescue here, because it allows us to use this same derivative-based method by incorporating the constraints into the function we take the derivative of and set equal to zero to solve.\nThe new approach you can use to solve for \\(x^*\\) in situations like this is based on constructing a new function \\(\\mathscr{L}(x, y, \\lambda)\\), called the Lagrangian, where the new parameter \\(\\lambda\\) is just the coefficient on a constraint function \\(g(x, y)\\).\nThis \\(g(x, y)\\) may seem complicated at first, but I think of it like a “helper function” which you derive from the constraint(s), finding a \\(g(x, y)\\) such that:\n\n\\(g(x) = 0\\) when the constraint is satisfied, and\n\\(g(x) \\neq 0\\) when the constraint is violated.\n\nIn our case, therefore, we can rewrite the constraint like:\n\\[\nx^2 + y^2 = 1 \\iff x^2 + y^2 - 1 = 0,\n\\]\nand choose our \\(g(x,y)\\) to be the left side of this equation: \\(g(x,y) = x^2 + y^2 - 1\\).\nWith our constraint function now chosen, we construct the Lagrangian:\n\\[\n\\begin{align*}\n\\mathscr{L}(x, \\lambda) &= f(x) + \\lambda g(x) = 2 - x^2 - 2y^2 + \\lambda(x^2 + y^2 - 1) \\\\\n&= 2 - x^2 - 2y^2 + \\lambda x^2 + \\lambda y^2 - \\lambda\n\\end{align*}\n\\]\nand optimize in the same way we’ve always optimized via calculus, making sure to compute all three of the partial derivatives:\n\nWith respect to \\(x\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial x} = -2x + 2\\lambda x\n\\]\nWith respect to \\(y\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial y} = -4y + 2\\lambda y\n\\]\nAnd with respect to \\(\\lambda\\):\n\\[\n\\frac{\\partial \\mathscr{L}}{\\partial \\lambda} = x^2 + y^2 - 1\n\\]\n\nwe can now set all of these derivatives equal to zero, to obtain a system of equations:\n\\[\n\\begin{align*}\n-2x + 2\\lambda x &= 0 \\\\\n-4y + 2\\lambda y &= 0 \\\\\nx^2 + y^2 - 1 &= 0\n\\end{align*}\n\\]\nThere are a few ways to solve this system (including using matrices!), but here’s how I solved it, by solving for \\(\\lambda^*\\) first:\n\\[\n\\begin{align*}\n-2x + 2\\lambda^* x = 0 \\iff 2\\lambda^* x = 2x \\iff \\boxed{\\lambda^* = 1},\n\\end{align*}\n\\]\nthen deriving the value of \\(y\\):\n\\[\n\\begin{align*}\n&-4y^* + 2\\lambda^* y^* = 0 \\iff -4y^* + 2(1)y^* = 0 \\\\\n&\\iff -4y^* + 2y^* = 0 \\iff \\boxed{y^* = 0},\n\\end{align*}\n\\]\nand \\(x\\):\n\\[\n\\begin{align*}\n&(x^*)^2 + (y^*)^2 - 1 = 0 \\iff (x^*)^2 + (0)^2 - 1 = 0 \\\\\n&\\iff (x^*)^2 = 1 \\iff \\boxed{x^* = \\pm 1}\n\\end{align*}\n\\]\nAnd indeed, by looking at the plot in Figure 1 above, we see that \\((-1,0)\\) and \\((1,0)\\) are the two optimal values here!"
  },
  {
    "objectID": "writeups/optimization/index.html#references",
    "href": "writeups/optimization/index.html#references",
    "title": "Mathematical Optimization",
    "section": "References",
    "text": "References\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf."
  },
  {
    "objectID": "writeups/optimization/index.html#footnotes",
    "href": "writeups/optimization/index.html#footnotes",
    "title": "Mathematical Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA somewhat important point, but I thought I’d make it a footnote in case it’s already clear to some folks: when you enter the “world” of optimization problems, you stop caring so much about whether the problem is a maximization or minimization problem. Instead, you literally just use whichever is easier to compute: the max of \\(f(x)\\) or the min of \\(-f(x)\\). To be a little more specific: in convex optimization, the most general type of optimization we have efficient algorithms for, you always minimize, and then just transform \\(f(x)\\) into \\(-f(x)\\) if your original intent was to maximize (this is… by convention essentially. See Boyd and Vandenberghe (2004) for more!).↩︎\nNote how we use “;” to separate random variables like \\(X\\) in this case from non-random variables like \\(\\lambda\\) in this case: sometimes people use the conditional operator “|” for this, but that can sometimes lead to confusion since conditioning on the value of a Random Variable is different from having the value of a non-random variable as a parameter to the function.↩︎\nYou can find a proof of this fact here.↩︎"
  },
  {
    "objectID": "writeups/machine-learning/index.html",
    "href": "writeups/machine-learning/index.html",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/index.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\nsource(\"../../dsan-globals/_globals.r\")\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don’t like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to “find a pattern” in this data, by clustering similar cars together"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures",
    "href": "writeups/machine-learning/index.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (“features”) and response variables (“labels”)\n\n\nsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms, ~rating,\n  0, 1000, 1, \"Disliked\",\n  1, 2000, 2, \"Liked\",\n  2, 2500, 1, \"Liked\",\n  3, 1500, 2, \"Disliked\",\n  4, 2200, 1, \"Liked\"\n)\nsup_data\n\n# A tibble: 5 × 4\n  home_id  sqft bedrooms rating  \n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1       0  1000        1 Disliked\n2       1  2000        2 Liked   \n3       2  2500        1 Liked   \n4       3  1500        2 Disliked\n5       4  2200        1 Liked   \n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (“features”)\n\n\nunsup_data &lt;- tibble::tribble(\n  ~home_id, ~sqft, ~bedrooms,\n  0, 1000, 1,\n  1, 2000, 2,\n  2, 2500, 1,\n  3, 1500, 2,\n  4, 2200, 1\n)\nunsup_data\n\n# A tibble: 5 × 3\n  home_id  sqft bedrooms\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1       0  1000        1\n2       1  2000        2\n3       2  2500        1\n4       3  1500        2\n5       4  2200        1"
  },
  {
    "objectID": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/index.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  dsan_theme(\"half\")\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# To force a legend\nunsup_grouped &lt;- unsup_data |&gt; mutate(big=bedrooms &gt; 1)\nunsup_grouped[['big']] &lt;- factor(unsup_grouped[['big']], labels=c(\"?1\",\"?2\"))\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms, fill=big)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  theme(legend.background = element_rect(fill=\"white\", color=\"white\"), legend.box.background = element_rect(fill=\"white\"), legend.text = element_text(color=\"white\"), legend.title = element_text(color=\"white\"), legend.position = \"right\") +\n  scale_fill_discrete(labels=c(\"?\",\"?\")) +\n  #scale_color_discrete(values=c(\"white\",\"white\"))\n  scale_color_manual(name=NULL, values=c(\"white\",\"white\")) +\n  #scale_color_manual(values=c(\"?1\"=\"white\",\"?2\"=\"white\"))\n  guides(fill = guide_legend(override.aes = list(shape = NA)))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#different-goals",
    "href": "writeups/machine-learning/index.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals\n\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size = g_pointsize * 2) +\n  labs(\n    title = \"Supervised Data: House Listings\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_vline(xintercept = 1750, linetype=\"dashed\", color = \"black\", size=1) +\n  annotate('rect', xmin=-Inf, xmax=1750, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[1]) +\n  annotate('rect', xmin=1750, xmax=Inf, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[2])\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n  #geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf, alpha=.2, fill='red'))\n\n\n\nlibrary(ggforce)\nggplot(unsup_grouped, aes(x=sqft, y=bedrooms)) +\n  #scale_color_brewer(palette = \"PuOr\") +\n  geom_mark_ellipse(expand=0.1, aes(fill=big), size = 1) +\n  geom_point(size=g_pointsize * 2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    fill = \"?\"\n  ) +\n  dsan_theme(\"half\") +\n  ggtitle(\"Unsupervised Data: House Listings\") +\n  #theme(legend.position = \"none\") +\n  #theme(legend.title = text_element(\"?\"))\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(cbPalette[3],cbPalette[4]), labels=c(\"?\",\"?\"))\n\n\n\n\n\n\n\n  #scale_fill_manual(labels=c(\"?\",\"?\"))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/index.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The “Learning” in Machine Learning",
    "text": "The “Learning” in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNaïve idea: Try random lines (each forming a decision boundary), pick “best” one\n\n\nx_min &lt;- 0\nx_max &lt;- 3000\ny_min &lt;- -1\ny_max &lt;- 3\nrand_y0 &lt;- runif(50, min=y_min, max=y_max)\nrand_y1 &lt;- runif(50, min=y_min, max=y_max)\nrand_slope &lt;- (rand_y1 - rand_y0)/(x_max - x_min)\nrand_intercept &lt;- rand_y0\nrand_lines &lt;- tibble::tibble(id=1:50, slope=rand_slope, intercept=rand_intercept)\n#ggplot() +\n#  geom_abline(data=rand_lines, aes(slope=slope, #intercept=intercept)) +\n#  xlim(0,3000) +\n#  ylim(0,2) +\n#  dsan_theme()\n\n\nggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + \n  geom_point(size=g_pointsize) +\n  labs(\n    title = \"The Like vs. Dislike Boundary: 50 Guesses\",\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"Outcome\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +\n  geom_abline(data=rand_lines, aes(slope=slope, intercept=intercept), linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/index.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a “Good”/“Best” Guess?",
    "text": "What Makes a “Good”/“Best” Guess?\n\nWhat’s your intuition? How about accuracy… 🤔\n\n\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3100)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nlibrary(patchwork)\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(710),f2(data_range),f2(Inf)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 60% Accuracy\") +\n  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot\n\n\n\n\n\n\n\n\nSo… what’s wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/index.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What’s Wrong with Accuracy?",
    "text": "What’s Wrong with Accuracy?\n\ngen_homes &lt;- function(n) {\n  rand_sqft &lt;- runif(n, min=2000, max=3000)\n  rand_bedrooms &lt;- sample(c(1,2), size=n, prob=c(0.5,0.5), replace=TRUE)\n  rand_ids &lt;- 1:n\n  rand_rating &lt;- \"Liked\"\n  rand_tibble &lt;- tibble::tibble(home_id=rand_ids, sqft=rand_sqft, bedrooms=rand_bedrooms, rating=rand_rating)\n  return(rand_tibble)\n}\nfake_homes &lt;- gen_homes(18)\nfake_sup_data &lt;- dplyr::bind_rows(sup_data, fake_homes)\nline_data &lt;- tibble::tribble(\n  ~id, ~slope, ~intercept,\n  0, 0, 0.75,\n  1, 0.00065, 0.5\n)\nf1 &lt;- function(x) { return(0*x + 0.75) }\nf2 &lt;- function(x) { return(0.00065*x + 0.5) }\n# And check accuracy\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary1=f1(sqft)) %&gt;% mutate(guessDislike1 = bedrooms &lt; boundary1) %&gt;% mutate(correct1 = ((rating==\"Disliked\") & (guessDislike1)) | (rating==\"Liked\") & (!guessDislike1))\nfake_sup_data &lt;- fake_sup_data %&gt;% mutate(boundary2=f2(sqft)) %&gt;% mutate(guessDislike2 = bedrooms &gt; boundary2) %&gt;% mutate(correct2 = ((rating==\"Disliked\") & (guessDislike2)) | (rating==\"Liked\") & (!guessDislike2))\n\ndata_range &lt;- 800:2700\nribbon_range &lt;- c(-Inf,data_range,Inf)\n\nf1_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3200)))\ng1_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==0))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 1: 91.3% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct1, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Disliked\"), alpha=0.2) +\n  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Liked\"), alpha=0.2) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\n\nf2_data &lt;- tibble::tibble(line_x=ribbon_range,line_y=c(f2(700),f2(data_range),f2(3100)))\ng2_plot &lt;- ggplot(data=(line_data %&gt;% filter(id==1))) +\n  geom_abline(aes(slope=slope, intercept=intercept), linetype=\"dashed\", size=1) +\n  ggtitle(\"Guess 2: 73.9% Accuracy\") +\n  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct2, levels=c(TRUE,FALSE))), size=g_pointsize) +\n  scale_shape_manual(values=c(24, 25)) +\n  labs(\n    x = \"Square Footage\",\n    y = \"Number of Bedrooms\",\n    color = \"True Label\",\n    shape = \"Correct Guess\"\n  ) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill=\"Liked\"), alpha=0.2) +\n  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill=\"Disliked\"), alpha=0.2) +\n  dsan_theme() +\n  expand_limits(x=data_range, y=c(0.8,2.2)) +\n  scale_fill_manual(values=c(\"Liked\"=cbPalette[2],\"Disliked\"=cbPalette[1]), name=\"Guess\")\n\ng1_plot + g2_plot"
  },
  {
    "objectID": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/index.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?2 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)3\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\n\"Science\"\n\n\ncluster_01\n\n\"Nature\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/index.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/index.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just “true prediction good, false prediction bad”\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you’ve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/index.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\nbase &lt;-\n  ggplot() +\n  xlim(-5, 5) +\n  ylim(0, 25) +\n  labs(\n    x = \"Y[obs] - Y[pred]\",\n    y = \"Prediction Badness (Loss)\"\n  ) +\n  dsan_theme()\n\nmy_fn &lt;- function(x) { return(x^2) }\nmy_deriv2 &lt;- function(x) { return(4*x - 4) }\nmy_derivN4 &lt;- function(x) { return(-8*x - 16) }\nbase + geom_function(fun = my_fn, color=cbPalette[1], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_deriv2, color=cbPalette[2], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-4,y=16)), aes(x=x,y=y), color=cbPalette[3], size=g_pointsize/2) + \n  geom_function(fun = my_derivN4, color=cbPalette[3], linewidth=1)\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 70 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\nmy_fake_deriv &lt;- function(x) { return(-x) }\nmy_fake_deriv2 &lt;- function(x) { return(-(1/2)*x + 1/2) }\nmy_fake_deriv3 &lt;- function(x) { return(-(1/4)*x + 3/4) }\n\nd=data.frame(x=c(-2,-1,0,1,2), y=c(1,0,0,1,1))\nbase &lt;- ggplot() +\n  xlim(-5,5) +\n  ylim(0,2) +\n  labs(\n    x=\"Y[obs] - Y[pred]\",\n    y=\"Prediction Badness (Loss)\"\n  ) +\n  geom_step(data=d, mapping=aes(x=x, y=y), linewidth=1) +\n  dsan_theme()\n\nbase + geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_fake_deriv, color=cbPalette[2], linewidth=1) +\n  geom_function(fun = my_fake_deriv2, color=cbPalette[3], linewidth=1) +\n  geom_function(fun = my_fake_deriv3, color=cbPalette[4], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-1,y=1)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) +\n  annotate(\"text\", x = -0.7, y = 1.1, label = \"?\", size=6)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 80 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_function()`).\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots 💀\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/index.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]\n\ny_pred &lt;- seq(from = 0, to = 1, by = 0.001)\ncompute_ce &lt;- function(y_p, y_o) { return(-(y_o * log(y_p) + (1-y_o)*log(1-y_p))) }\nce0 &lt;- compute_ce(y_pred, 0)\nce1 &lt;- compute_ce(y_pred, 1)\nce0_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=0, ce=ce0)\nce1_data &lt;- tibble::tibble(y_pred=y_pred, y_obs=1, ce=ce1)\nce_data &lt;- dplyr::bind_rows(ce0_data, ce1_data)\nggplot(ce_data, aes(x=y_pred, y=ce, color=factor(y_obs))) +\n  geom_line(linewidth=1) +\n  labs(\n    title=\"Binary Cross-Entropy Loss\",\n    x = \"Predicted Value\",\n    y = \"Loss\",\n    color = \"Actual Value\"\n  ) +\n  dsan_theme() +\n  ylim(0,6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/index.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we’ve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model’s parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#calculus-strikes-again",
    "href": "writeups/machine-learning/index.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = “direction of sharpest decrease”\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the “loss crater”\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#good-and-bad-news",
    "href": "writeups/machine-learning/index.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\nFigure from @schmidinger_exploring_2019\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) − p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#so-whats-the-issue",
    "href": "writeups/machine-learning/index.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What’s the Issue?",
    "text": "So What’s the Issue?\n\n\n\nx &lt;- seq(from = 0, to = 1, by = 0.1)\nn &lt;- length(x)\neps &lt;- rnorm(n, 0, 0.04)\ny &lt;- x + eps\n# But make one big outlier\nmidpoint &lt;- ceiling((3/4)*n)\ny[midpoint] &lt;- 0\nof_data &lt;- tibble::tibble(x=x, y=y)\n# Linear model\nlin_model &lt;- lm(y ~ x)\n# But now polynomial regression\npoly_model &lt;- lm(y ~ poly(x, degree = 10, raw=TRUE))\n#summary(model)\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  labs(\n    title = \"Training Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  geom_point(size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw=TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  labs(\n    title = \"A Perfect Model?\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\nsummary(lin_model)$r.squared\n\n[1] 0.57189\n\nget_rss(lin_model)\n\n[1] 0.5356712\n\n\n\nPolynomial Model:\n\n\nsummary(poly_model)$r.squared\n\n[1] 1\n\nget_rss(poly_model)\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/index.html#generalization",
    "href": "writeups/machine-learning/index.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n# Data setup\nx_test &lt;- seq(from = 0, to = 1, by = 0.1)\nn_test &lt;- length(x_test)\neps_test &lt;- rnorm(n_test, 0, 0.04)\ny_test &lt;- x_test + eps_test\nof_data_test &lt;- tibble::tibble(x=x_test, y=y_test)\nlin_y_pred_test &lt;- predict(lin_model, as.data.frame(x_test))\n#lin_y_pred_test\nlin_resids_test &lt;- y_test - lin_y_pred_test\n#lin_resids_test\nlin_rss_test &lt;- sum(lin_resids_test^2)\n#lin_rss_test\n# Lin R2 = 1 - RSS/TSS\ntss_test &lt;- sum((y_test - mean(y_test))^2)\nlin_r2_test &lt;- 1 - (lin_rss_test / tss_test)\n#lin_r2_test\n# Now the poly model\npoly_y_pred_test &lt;- predict(poly_model, as.data.frame(x_test))\npoly_resids_test &lt;- y_test - poly_y_pred_test\npoly_rss_test &lt;- sum(poly_resids_test^2)\n#poly_rss_test\n# RSS\npoly_r2_test &lt;- 1 - (poly_rss_test / tss_test)\n#poly_r2_test\n\n\nggplot(of_data, aes(x=x, y=y)) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ poly(x, 10, raw = TRUE),\n              se = FALSE, aes(color=\"Polynomial\")) +\n  dsan_theme() +\n  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +\n  geom_abline(aes(intercept=0, slope=1, color=\"Linear\"), linewidth=1, show.legend = FALSE) +\n  labs(\n    title = \"Performance on Unseen Test Data\",\n    color = \"Model\"\n  ) +\n  dsan_theme()\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\nlin_r2_test\n\n[1] 0.8837171\n\nlin_rss_test\n\n[1] 0.1223117\n\n\n\nPolynomial Model:\n\n\npoly_r2_test\n\n[1] 0.4190901\n\npoly_rss_test\n\n[1] 0.6110277"
  },
  {
    "objectID": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/index.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, “complexity”?"
  },
  {
    "objectID": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/index.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to “just figure it out”\nThe gist, in the general case, is thus: try to “amplify” the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/index.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO4 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))"
  },
  {
    "objectID": "writeups/machine-learning/index.html#training-vs.-test-data",
    "href": "writeups/machine-learning/index.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs. Test Data",
    "text": "Training vs. Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTest Set (20%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#cross-validation",
    "href": "writeups/machine-learning/index.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_04\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameters",
    "href": "writeups/machine-learning/index.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) “settings” for our learning procedure (that we haven’t optimized via gradient descent)\nThere are several we’ve already seen – can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/index.html#hyperparameter-selection",
    "href": "writeups/machine-learning/index.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/index.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater “gaps” between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)"
  },
  {
    "objectID": "writeups/machine-learning/index.html#footnotes",
    "href": "writeups/machine-learning/index.html#footnotes",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)↩︎\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of name↩︎\nThanks to Bayes’ Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)↩︎\nLeast Absolute Shrinkage and Selection Operator↩︎"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "href": "writeups/machine-learning/slides.html#supervised-vs.-unsupervised-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\n\n\n\nSupervised Learning: You want the computer to learn the existing pattern of how you are classifying1 observations\n\nDiscovering the relationship between properties of data and outcomes\nExample (Binary Classification): I look at homes on Zillow, saving those I like to folder A and don’t like to folder B\nExample (Regression): I assign a rating of 0-100 to each home\nIn both cases: I ask the computer to learn my schema (how I classify)\n\n\nUnsupervised Learning: You want the computer to find patterns in a dataset, without any prior classification info\n\nTypically: grouping or clustering observations based on shared properties\nExample (Clustering): I save all the used car listings I can find, and ask the computer to “find a pattern” in this data, by clustering similar cars together\n\n\nWhether standard classification (sorting observations into bins) or regression (assigning a real number to each observation)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures",
    "href": "writeups/machine-learning/slides.html#dataset-structures",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures",
    "text": "Dataset Structures\n\n\nSupervised Learning: Dataset has both explanatory variables (“features”) and response variables (“labels”)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\nrating\n\n\n\n\n0\n1000\n1\nDisliked\n\n\n1\n2000\n2\nLiked\n\n\n2\n2500\n1\nLiked\n\n\n3\n1500\n2\nDisliked\n\n\n4\n2200\n1\nLiked\n\n\n\n\n\n\n\n\nUnsupervised Learning: Dataset has only explanatory variables (“features”)\n\n\n\n\n\n\n\nhome_id\nsqft\nbedrooms\n\n\n\n\n0\n1000\n1\n\n\n1\n2000\n2\n\n\n2\n2500\n1\n\n\n3\n1500\n2\n\n\n4\n2200\n1"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "href": "writeups/machine-learning/slides.html#dataset-structures-visualized",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Dataset Structures: Visualized",
    "text": "Dataset Structures: Visualized"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#different-goals",
    "href": "writeups/machine-learning/slides.html#different-goals",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Different Goals",
    "text": "Different Goals"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "href": "writeups/machine-learning/slides.html#the-learning-in-machine-learning",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The “Learning” in Machine Learning",
    "text": "The “Learning” in Machine Learning\n\nGiven these datasets, how do we learn the patterns?\nNaïve idea: Try random lines (each forming a decision boundary), pick “best” one\n\n\n\nWhat parameters are we choosing when we draw a random line? Random curve?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "href": "writeups/machine-learning/slides.html#what-makes-a-goodbest-guess",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What Makes a “Good”/“Best” Guess?",
    "text": "What Makes a “Good”/“Best” Guess?\n\nWhat’s your intuition? How about accuracy… 🤔\n\n\nSo… what’s wrong here?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "href": "writeups/machine-learning/slides.html#whats-wrong-with-accuracy",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "What’s Wrong with Accuracy?",
    "text": "What’s Wrong with Accuracy?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "href": "writeups/machine-learning/slides.html#the-oversimplified-big-picture",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "The (Oversimplified) Big Picture",
    "text": "The (Oversimplified) Big Picture\n\nA model: some representation of something in the world\n\n\n\n\n\nHow well does our model represent the world?1 \\(\\mathsf{Correspondence}(y_{obs}, \\theta)\\)\n\\(P\\left(y_{obs}, \\theta\\right)\\), \\(P\\left(\\theta \\; | \\; y_{obs}\\right)\\), \\(P\\left(y_{obs} \\; | \\; \\theta\\right)\\)2\nMaximum Likelihood Estimation?\n\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\n\"Nature\"\n\n\ncluster_02\n\n\"Science\"\n\n\n\nObs\n\nThing(s) we can see\n\n\n\nUnd\n\nUnderlying process\n\n\n\nUnd-&gt;Obs\n\n\n\n\n\nModel\n\nModel\n\n\n\nUnd-&gt;Model\n\n\n\n\n\nModel-&gt;Obs\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathsf{Correspondence}(y_{obs}, \\theta) &\\equiv P(y = y_{obs}, \\theta) \\\\\nP(y = y_{obs}, \\theta) &= P(y=y_{obs} \\; | \\; \\theta)P(\\theta) \\\\\n&\\propto P\\left(y = y_{obs} \\; | \\; \\theta\\right)\\ldots \\implies \\text{(maximize this!)}  \\\\\n\\end{align*}\n\\]\nComputer scientists implicitly assume a Correspondence Theory of Truth, hence the choice of nameThanks to Bayes’ Rule, mathematically we can always convert between the two: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "href": "writeups/machine-learning/slides.html#measuring-errors-f1-score",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: F1 Score",
    "text": "Measuring Errors: F1 Score\n\nHow can we reward guesses which best discriminate between classes?\n\n\\[\n\\begin{align*}\n\\mathsf{Precision} &= \\frac{\\# \\text{true positives}}{\\# \\text{predicted positive}} = \\frac{tp}{tp+fp} \\\\[1.5em]\n\\mathsf{Recall} &= \\frac{\\# \\text{true positives}}{\\# \\text{positives in data}} = \\frac{tp}{tp+fn} \\\\[1.5em]\nF_1 &= 2\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision} + \\mathsf{Recall}} = \\mathsf{HMean}(\\mathsf{Precision}, \\mathsf{Recall})\n\\end{align*}\n\\]\n\nThink about: How does this address/fix issue with accuracy?\n\n\n\nHere \\(\\mathsf{HMean}\\) is the Harmonic Mean function: see appendix slide or Wikipedia."
  },
  {
    "objectID": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "href": "writeups/machine-learning/slides.html#measuring-errors-the-loss-function",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Measuring Errors: The Loss Function",
    "text": "Measuring Errors: The Loss Function\n\nWhat about regression?\n\nNo longer just “true prediction good, false prediction bad”\n\nWe have to quantify how bad the guess is! Then we can scale the penalty accordingly: \\(\\text{penalty} \\propto \\text{badness}\\)\nEnter Loss Functions! Just distances (using distance metrics you’ve already seen) between the true value and our guess:\n\nSquared Error \\(L^2(y_{obs}, y_{pred}) = (y_{obs} - y_{pred})^2\\)\nKullback-Leibler Divergence if guessing distributions"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "href": "writeups/machine-learning/slides.html#calculus-rears-its-ugly-head",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Rears its Ugly Head",
    "text": "Calculus Rears its Ugly Head\n\nNeural networks use derivatives/gradients to improve their predictions given a particular loss function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan we just use the \\(F_1\\) score?\n\n\\[\n\\frac{\\partial F_1(weights)}{\\partial weights} = \\ldots \\; ? \\; \\ldots 💀\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "href": "writeups/machine-learning/slides.html#quantifying-discrete-loss",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Quantifying Discrete Loss",
    "text": "Quantifying Discrete Loss\n\nWe can quantify a differentiable discrete loss by asking the algorithm how confident it is\n\nCloser to 0 \\(\\implies\\) more confident that the true label is 0\nCloser to 1 \\(\\implies\\) more confident that the true label is 1\n\n\n\\[\n\\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\\log(y_{pred}) + (1-y_{obs})\\log(1-y_{pred}))\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "href": "writeups/machine-learning/slides.html#loss-function-implies-ready-to-learn",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Loss Function \\(\\implies\\) Ready to Learn!",
    "text": "Loss Function \\(\\implies\\) Ready to Learn!\n\nOnce we’ve chosen a loss function, the learning algorithm has what it needs to proceed with the actual learning\nNotation: Bundle all the model’s parameters together into \\(\\theta\\)\nThe goal: \\[\n\\min_{\\theta} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nWhat would this look like for the random-lines approach?\nIs there a more efficient way?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "href": "writeups/machine-learning/slides.html#calculus-strikes-again",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Calculus Strikes Again",
    "text": "Calculus Strikes Again\n\ntldr: The slope of a function tells us how to get to a minimum (why a minimum rather than the minimum?)\nDerivative (gradient) = “direction of sharpest decrease”\nThink of hill climbing! Let \\(\\ell_t \\in L\\) be your location at time \\(t\\), and \\(Alt(\\ell)\\) be the altitude at a location \\(\\ell\\)\nGradient descent for \\(\\ell^* = \\max_{\\ell \\in L} Alt(\\ell)\\): \\[\n\\ell_{t+1} = \\ell_t + \\gamma\\nabla Alt(\\ell_t),\\ t\\geq 0.\n\\]\nWhile top of mountain = good, Loss is bad: we want to find the bottom of the “loss crater”\n\n\\(\\implies\\) we do the opposite: subtract \\(\\gamma\\nabla Alt(\\ell_t)\\)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#good-and-bad-news",
    "href": "writeups/machine-learning/slides.html#good-and-bad-news",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Good and Bad News",
    "text": "Good and Bad News\n\n\n\nUniversal Approximation Theorem\nNeural networks can represent any function mapping one Euclidean space to another\n(Neural Turing Machines:)\n\n\n\n\n\n\n\nWeierstrass Approximation Theorem\n(Polynomials could already represent any function)\n\n\\[\nf \\in C([a,b],[a,b])\n\\] \\[\n\\implies \\forall \\epsilon &gt; 0, \\exists p \\in \\mathbb{R}[x] :\n\\] \\[\n\\forall x \\in [a, b] \\; \\left|f(x) − p(x)\\right| &lt; \\epsilon\n\\]\n\nImplications for machine learning?\n\n\n\nFigure from @schmidinger_exploring_2019"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "href": "writeups/machine-learning/slides.html#so-whats-the-issue",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "So What’s the Issue?",
    "text": "So What’s the Issue?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher \\(R^2\\) = Better Model? Lower \\(RSS\\)?\nLinear Model:\n\n\n\nCode\nsummary(lin_model)$r.squared\n\n\n[1] 0.5468355\n\n\nCode\nget_rss(lin_model)\n\n\n[1] 0.5653339\n\n\n\nPolynomial Model:\n\n\n\nCode\nsummary(poly_model)$r.squared\n\n\n[1] 1\n\n\nCode\nget_rss(poly_model)\n\n\n[1] 0"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#generalization",
    "href": "writeups/machine-learning/slides.html#generalization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Generalization",
    "text": "Generalization\n\nTraining Accuracy: How well does it fit the data we can see?\nTest Accuracy: How well does it generalize to future data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Model:\n\n\n\nCode\nlin_r2_test\n\n\n[1] 0.8939268\n\n\nCode\nlin_rss_test\n\n\n[1] 0.1193267\n\n\n\nPolynomial Model:\n\n\n\nCode\npoly_r2_test\n\n\n[1] 0.4036601\n\n\nCode\npoly_rss_test\n\n\n[1] 0.6708505"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "href": "writeups/machine-learning/slides.html#how-to-avoid-overfitting",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "How to Avoid Overfitting?",
    "text": "How to Avoid Overfitting?\n\nThe gist: penalize model complexity\n\nOriginal optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(y_{obs}, y_{pred}(\\theta))\n\\]\nNew optimization: \\[\n\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\mathcal{L}(y_{obs}, y_{pred}(\\theta)) + \\mathsf{Complexity}(\\theta) \\right]\n\\]\n\nSo how do we measure, and penalize, “complexity”?"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "href": "writeups/machine-learning/slides.html#regularization-measuring-and-penalizing-complexity",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Regularization: Measuring and Penalizing Complexity",
    "text": "Regularization: Measuring and Penalizing Complexity\n\nIn the case of polynomials, fairly simple complexity measure: degree of polynomial\n\n\\[\n\\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3) &gt; \\mathsf{Complexity}(y_{pred} = \\beta_0 + \\beta_1 x)\n\\]\n\nIn general machine learning, however, we might not be working with polynomials\nIn neural networks, for example, we sometimes toss in millions of features and ask the algorithm to “just figure it out”\nThe gist, in the general case, is thus: try to “amplify” the most important features and shrink the rest, so that\n\n\\[\n\\mathsf{Complexity} \\propto \\frac{|\\text{AmplifiedFeatures}|}{|\\text{ShrunkFeatures}|}\n\\]"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "href": "writeups/machine-learning/slides.html#lasso-and-elastic-net-regularization",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "LASSO and Elastic Net Regularization",
    "text": "LASSO and Elastic Net Regularization\n\nMany ways to translate this intuition into math!\nIn several fields, however (econ, biostatistics), LASSO1 [@tibshirani_regression_1996] is standard:\n\n\\[\n\\beta^*_{LASSO} = {\\underset{\\beta}{\\operatorname{argmin}}}\\left\\{{\\frac {1}{N}}\\left\\|y-X\\beta \\right\\|_{2}^{2}+\\lambda \\|\\beta \\|_{1}\\right\\}\n\\]\n\nWhy does this work to penalize complexity? What does the parameter \\(\\lambda\\) do?\nSome known issues with LASSO fixed in extension of the same intuitions: Elastic Net\n\n\\[\n\\beta^*_{EN} = {\\underset {\\beta }{\\operatorname {argmin} }}\\left\\{ \\|y-X\\beta \\|^{2}_2+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1} \\right\\}\n\\]\n\n(Ensures a unique global minimum! Note that \\(\\lambda_2 = 0, \\lambda_1 = 1 \\implies \\beta^*_{LASSO} = \\beta^*_{EN}\\))\n\nLeast Absolute Shrinkage and Selection Operator"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "href": "writeups/machine-learning/slides.html#training-vs.-test-data",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Training vs. Test Data",
    "text": "Training vs. Test Data\n\n\n\n\n\n\n\ngrid\n\n\ncluster_02\n\nTest Set (20%)\n\n\ncluster_01\n\nTraining Set (80%)\n\n\n\nN1\n\n20%\n\n\n\nN2\n\n20%\n\n\n\nN3\n\n20%\n\n\n\nN4\n\n20%\n\n\n\nN5\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#cross-validation",
    "href": "writeups/machine-learning/slides.html#cross-validation",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nThe idea that good models generalize well is crucial!\n\nWhat if we could leverage this insight to optimize over our training data?\nThe key: Validation Sets\n\n\n\n\n\n\n\n\n\ngrid\n\n\ncluster_01\n\nTraining Set (80%)\n\n\ncluster_02\n\nTraining Fold (80%)\n\n\ncluster_03\n\nValidation Fold (20%)\n\n\ncluster_04\n\nTest Set (20%)\n\n\n\nA1\n\n16%\n\n\n\nA2\n\n16%\n\n\n\n\nA3\n\n16%\n\n\n\n\nA4\n\n16%\n\n\n\n\nB1\n\n16%\n\n\n\n\nC1\n\n20%"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameters",
    "href": "writeups/machine-learning/slides.html#hyperparameters",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe unspoken (but highly consequential!) “settings” for our learning procedure (that we haven’t optimized via gradient descent)\nThere are several we’ve already seen – can you name them?\n\n\n\nUnsupervised Clustering: The number of clusters we want (\\(K\\))\nGradient Descent: The step size \\(\\gamma\\)\nLASSO/Elastic Net: \\(\\lambda\\)\nThe train/validation/test split!"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "href": "writeups/machine-learning/slides.html#hyperparameter-selection",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Hyperparameter Selection",
    "text": "Hyperparameter Selection\n\nEvery model comes with its own hyperparameters:\n\nNeural Networks: Number of layers, number of nodes per layer\nDecision Trees: Maximum tree depth, max number of features to include\nTopic Models: Number of topics, document/topic priors\n\nSo, how do we choose?\n\nOften more art than science\nPrincipled, universally applicable, but slow: grid search\nSpecific methods for specific algorithms: ADAM [@kingma_adam_2017] for Neural Network learning rates)"
  },
  {
    "objectID": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "href": "writeups/machine-learning/slides.html#appendix-harmonic-mean",
    "title": "Extra Slides: A Slightly Deeper Dive Into Machine Learning",
    "section": "Appendix: Harmonic Mean",
    "text": "Appendix: Harmonic Mean\n\n\\(\\mathsf{HMean}\\) is the harmonic mean, an alternative to the standard (arithmetic) mean\nPenalizes greater “gaps” between precision and recall: if precision is 0 and recall is 1, for example, their arithmetic mean is 0.5 while their harmonic mean is 0.\nFor the curious: given numbers \\(X = \\{x_1, \\ldots, x_n\\}\\), \\(\\mathsf{HMean}(X) = \\frac{n}{\\sum_{i=1}^nx_i^{-1}}\\)\n\n\n\n\nDSAN5000 Extra Slides: Machine Learning"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5300-01: Statistical Learning",
    "section": "",
    "text": "Welcome to the homepage for Section 01 (Mondays 6:30-9pm in Car Barn 203) of DSAN 5300: Statistical Learning at Georgetown University, for the Spring 2025 semester!\nUse the following links to view notes and lecture slides for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Introduction to the Course\n\n\nJanuary 8\n\n\n\n\nWeek 2: Linear Regression\n\n\nJanuary 13\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  }
]