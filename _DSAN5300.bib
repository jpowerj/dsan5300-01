@book{boehmke2019handson,
  title = {Hands-{{On Machine Learning}} with {{R}}},
  author = {Boehmke, Brad and Greenwell, Brandon M.},
  year = {2019},
  month = nov,
  publisher = {CRC Press},
  abstract = {Hands-on Machine Learning with R provides a practical and applied approach to learning and developing intuition into today's most popular machine learning methods. This book serves as a practitioner's guide to the machine learning process and is meant to help the reader learn to apply the machine learning stack within R, which includes using various R packages such as glmnet, h2o, ranger, xgboost, keras, and others to effectively model and gain insight from their data. The book favors a hands-on approach, providing an intuitive understanding of machine learning concepts through concrete examples and just a little bit of theory. Throughout this book, the reader will be exposed to the entire machine learning process including feature engineering, resampling, hyperparameter tuning, model evaluation, and interpretation. The reader will be exposed to powerful algorithms such as regularized regression, random forests, gradient boosting machines, deep learning, generalized low rank models, and more! By favoring a hands-on approach and using real word data, the reader will gain an intuitive understanding of the architectures and engines that drive these algorithms and packages, understand when and how to tune the various hyperparameters, and be able to interpret model results. By the end of this book, the reader should have a firm grasp of R's machine learning stack and be able to implement a systematic approach for producing high quality modeling results.Features:{$\cdot$} Offers a practical and applied introduction to the most popular machine learning methods.{$\cdot$} Topics covered include feature engineering, resampling, deep learning and more.{$\cdot$} Uses a hands-on approach and real world data.},
  googlebooks = {aXC9DwAAQBAJ},
  isbn = {978-1-000-73019-7},
  langid = {english}
}

@book{boyd_convex_2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  month = mar,
  publisher = {Cambridge University Press},
  url = {https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf},
  abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
  googlebooks = {mYm0bLd3fcoC},
  isbn = {978-0-521-83378-3},
  langid = {english}
}

@book{gelman_data_2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  publisher = {Cambridge University Press},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\textasciitilde}gelman/arm/},
  googlebooks = {lV3DIdV0F9AC},
  isbn = {978-0-521-68689-1},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General,Political Science / General,Psychology / Assessment Testing & Measurement,Social Science / Research}
}

@book{hume_enquiry_1760,
  title = {An {{Enquiry Concerning Human Understanding}}},
  author = {Hume, David},
  year = {1760},
  publisher = {{Simon and Schuster}},
  abstract = {David Hume, an empiricist philosopher, takes on perhaps one of the most challenging of conceivable problems in An Enquiry Concerning Human Understanding. Moving beyond Descartes classic statement, I think, therefore I am, Hume addresses issues of knowing that fall outside the realms of active thought or incremental learning. While innumerable philosophers discuss various aspects of experience, Hume stands alone in his successful treatise on the nature of experience itself.},
  googlebooks = {BWDrAgAAQBAJ},
  isbn = {978-1-62558-316-1},
  langid = {english}
}

@book{hume_treatise_1739,
  title = {A {{Treatise}} of {{Human Nature}}: {{Being}} an {{Attempt}} to {{Introduce}} the {{Experimental Method}} of {{Reasoning Into Moral Subjects}}; and {{Dialogues Concerning Natural Religion}}},
  shorttitle = {A {{Treatise}} of {{Human Nature}}},
  author = {Hume, David},
  year = {1739},
  publisher = {Longmans, Green},
  langid = {english}
}

@misc{kingma_adam_2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2023-04-10},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jpj/Zotero/storage/UP3UQYS7/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/jpj/Zotero/storage/TLHMS66C/1412.html}
}

@book{lyall_divided_2020,
  title = {Divided {{Armies}}: {{Inequality}} and {{Battlefield Performance}} in {{Modern War}}},
  shorttitle = {Divided {{Armies}}},
  author = {Lyall, Jason},
  year = {2020},
  month = feb,
  publisher = {Princeton University Press},
  abstract = {How do armies fight and what makes them victorious on the modern battlefield? In Divided Armies, Jason Lyall challenges long-standing answers to this classic question by linking the fate of armies to their levels of inequality. Introducing the concept of military inequality, Lyall demonstrates how a state's prewar choices about the citizenship status of ethnic groups within its population determine subsequent battlefield performance. Treating certain ethnic groups as second-class citizens, either by subjecting them to state-sanctioned discrimination or, worse, violence, undermines interethnic trust, fuels grievances, and leads victimized soldiers to subvert military authorities once war begins. The higher an army's inequality, Lyall finds, the greater its rates of desertion, side-switching, casualties, and use of coercion to force soldiers to fight.In a sweeping historical investigation, Lyall draws on Project Mars, a new dataset of 250 conventional wars fought since 1800, to test this argument. Project Mars breaks with prior efforts by including overlooked non-Western wars while cataloguing new patterns of inequality and wartime conduct across hundreds of belligerents. Combining historical comparisons and statistical analysis, Lyall also marshals evidence from nine wars, ranging from the Eastern Fronts of World Wars I and II to less familiar wars in Africa and Central Asia, to illustrate inequality's effects.Sounding the alarm on the dangers of inequality for battlefield performance, Divided Armies offers important lessons about warfare over the past two centuries---and for wars still to come.},
  googlebooks = {A32pDwAAQBAJ},
  isbn = {978-0-691-19415-8},
  langid = {english}
}

@book{popper_logic_1934,
  title = {The {{Logic}} of {{Scientific Discovery}}},
  author = {Popper, Karl R.},
  year = {1934},
  publisher = {Psychology Press},
  abstract = {Described by the philosopher A.J. Ayer as a work of 'great originality and power', this book revolutionized contemporary thinking on science and knowledge. Ideas such as the now legendary doctrine of 'falsificationism' electrified the scientific community, influencing even working scientists, as well as post-war philosophy. This astonishing work ranks alongside The Open Society and Its Enemies as one of Popper's most enduring books and contains insights and arguments that demand to be read to this day.},
  googlebooks = {Yq6xeupNStMC},
  isbn = {978-0-415-27844-7},
  langid = {english}
}

@article{tibshirani_regression_1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  number = {1},
  eprint = {2346178},
  eprinttype = {jstor},
  pages = {267--288},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9246},
  url = {https://www.jstor.org/stable/2346178},
  urldate = {2023-04-10},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.}
}
